import os
import uuid
import hashlib
import datetime
import fnmatch
from typing import List, Dict, Optional, Generator, Tuple, Any, Union

from tree_sitter import Parser, Node
from tree_sitter_languages import get_language

from ..models import FileRecord, ChunkNode, ChunkContent, ParsingResult, CodeRelation
from ..providers.metadata import MetadataProvider, GitMetadataProvider, LocalMetadataProvider
from .parsing_filters import (
    GLOBAL_IGNORE_DIRS, 
    SEMANTIC_NOISE_DIRS, 
    LANGUAGE_SPECIFIC_FILTERS, 
    MAX_FILE_SIZE_BYTES,
    MAX_LINE_LENGTH
)

class TreeSitterRepoParser:
    """
    Parser semantico ottimizzato per High-Performance Indexing.
    Utilizza MemoryView e Bytearray per ridurre allocazioni e copie (Zero-Copy Slicing).
    """

    EXT_TO_LANG_CONFIG = {
        ".py": "python",
        ".js": "javascript", ".jsx": "javascript", ".ts": "javascript", ".tsx": "javascript",
        ".java": "java",
        ".go": "go",
        ".html": "web", ".css": "web", ".json": "web"
    }
    
    LANGUAGE_MAP = {
        ".py": "python", ".js": "javascript", ".jsx": "javascript", 
        ".ts": "typescript", ".tsx": "typescript", ".go": "go", 
        ".java": "java", ".c": "c", ".cpp": "cpp", ".rs": "rust",
        ".html": "html", ".css": "css", ".php": "php"
    }
    

    MAX_CHUNK_SIZE = 800 
    CHUNK_TOLERANCE = 400

    CONTAINER_TYPES = {
        "class_definition", "class_declaration", "function_definition", "method_definition", 
        "function_declaration", "arrow_function", "interface_declaration", "impl_item", "mod_item",
        "async_function_definition", "decorated_definition", "export_statement"
    }

    GLUE_TYPES = {"comment", "decorator", "line_comment", "block_comment", "string_literal"}

    def __init__(self, repo_path: str, metadata_provider: Optional[MetadataProvider] = None):
        self.repo_path = os.path.abspath(repo_path)
        if not os.path.isdir(self.repo_path): raise FileNotFoundError(f"Invalid path: {repo_path}")

        is_git_repo = os.path.exists(os.path.join(self.repo_path, ".git"))
        self.metadata_provider = metadata_provider or (
            GitMetadataProvider(self.repo_path) if is_git_repo else LocalMetadataProvider(self.repo_path)
        )
        self.repo_info = self.metadata_provider.get_repo_info()
        
        self.snapshot_id: Optional[str] = None 
        
        # Caricamento Lingue (Cache)
        self.languages: Dict[str, Any] = {}
        lang_cache: Dict[str, Any] = {}

        for ext, lang_name in self.LANGUAGE_MAP.items():
            if lang_name in lang_cache:
                self.languages[ext] = lang_cache[lang_name]
                continue
            try:
                lang_obj = get_language(lang_name)
                lang_cache[lang_name] = lang_obj
                self.languages[ext] = lang_obj
            except Exception as e:
                print(f"[ERROR] Failed to load language {lang_name}: {e}")
                continue

        self.parser = Parser()
        
        # Il parser ignora SIA il rumore tecnico (node_modules) SIA quello semantico (se deciso così)
        # Ma nel nostro design, il Parser ACCETTA semantic noise (test) ma SCIP no.
        # Quindi qui ignoriamo SOLO GLOBAL_IGNORE_DIRS (rumore tecnico).
        # Se vuoi che il parser ignori anche i test, aggiungi SEMANTIC_NOISE_DIRS qui.
        self.all_ignore_dirs = GLOBAL_IGNORE_DIRS

    def _set_parser_language(self, lang_object):
        if hasattr(self.parser, 'set_language'): self.parser.set_language(lang_object)
        else: self.parser.language = lang_object

    # ==============================================================================
    #  RESILIENCE & I/O
    # ==============================================================================

    def _is_binary(self, content_sample: bytes) -> bool:
        return b'\0' in content_sample

    def _is_minified_or_generated(self, content_sample: bytes, file_path: str) -> bool:
        """
        Euristica basata sul contenuto per rilevare file minificati/generati.
        """
        try:
            # Check 1: Linea troppo lunga (Minified JS/CSS)
            # Analizziamo solo le prime righe per velocità
            first_lines = content_sample[:2048].split(b'\n')[:5]
            for line in first_lines:
                if len(line) > MAX_LINE_LENGTH: 
                    return True
            
            # Check 2: Header "Auto-generated" comune
            header = content_sample[:500].lower()
            if b"generated by" in header or b"auto-generated" in header or b"do not edit" in header:
                return True
                
        except Exception: 
            pass
        return False

    def _safe_read_file(self, full_path: str) -> Tuple[Optional[bytes], Optional[str]]:
        try:
            size = os.path.getsize(full_path)
            if size > MAX_FILE_SIZE_BYTES:
                return None, f"File too large ({size / 1024 / 1024:.2f} MB)"

            with open(full_path, 'rb') as f:
                head = f.read(1024)
                if self._is_binary(head):
                    return None, "Binary file detected"
                
                f.seek(0)
                content = f.read()
                return content, None
        except Exception as e:
            return None, f"Read Error: {str(e)}"

    # ==============================================================================
    #  SEMANTIC QUERY ENGINE
    # ==============================================================================

    def _load_query_for_language(self, language_name: str) -> Optional[str]:
        try:
            base_path = os.path.dirname(__file__)
            query_path = os.path.join(base_path, "queries", f"{language_name}.scm")
            if os.path.exists(query_path):
                with open(query_path, "r", encoding="utf-8") as f:
                    return f.read()
        except Exception as e:
            # Silenzioso se manca il file query per lingue meno comuni
            pass
        return None

    def _generate_label(self, category: str, value: str) -> str:
        labels = {
            ("role", "entry_point"): "Application Entry Point",
            ("role", "test_suite"): "Test Suite Class",
            ("role", "test_case"): "Unit/Integration Test Case",
            ("role", "api_endpoint"): "API Route Handler",
            ("role", "data_schema"): "Data Model / Schema",
            ("type", "class"): "Class Definition",
            ("type", "function"): "Function Definition",
        }
        return labels.get((category, value), f"{value.replace('_', ' ').title()}")

    def _get_semantic_captures(self, tree, language_name: str) -> List[Dict[str, Any]]:
        query_scm = self._load_query_for_language(language_name)
        if not query_scm: return []
        
        target_ext = next((ext for ext, lang in self.LANGUAGE_MAP.items() if lang == language_name), None)
        if not target_ext or target_ext not in self.languages: return []
        lang_obj = self.languages[target_ext]
        
        try:
            query = lang_obj.query(query_scm)
            captures = query.captures(tree.root_node)
            results = []
            for node, capture_name in captures:
                parts = capture_name.split('.')
                if len(parts) < 2: continue
                
                category, value = parts[0], parts[1]
                results.append({
                    "start": node.start_byte,
                    "end": node.end_byte,
                    "metadata": {
                        "category": category,
                        "value": value,
                        "label": self._generate_label(category, value)
                    }
                })
            return results
        except Exception as e:
            # Warning leggero, non blocca il parsing
            return []

    # ==============================================================================
    #  MAIN PIPELINE (OPTIMIZED)
    # ==============================================================================

    def stream_semantic_chunks(self, file_list: Optional[List[str]] = None) -> Generator[Tuple[FileRecord, List[ChunkNode], List[ChunkContent], List[CodeRelation]], None, None]:
        if not self.snapshot_id:
            raise ValueError("Parser: snapshot_id not set. Cannot process files without a version context.")

        files_to_process = set(file_list) if file_list else None
        commit_hash = self.repo_info.get('commit_hash', 'HEAD')
        
        for root, dirs, files in os.walk(self.repo_path, topdown=True):
            # Pruning delle directory ignorate "in-place" per non scenderci dentro
            # Ignoriamo GLOBAL_IGNORE_DIRS e le directory nascoste
            dirs[:] = [d for d in dirs if d not in self.all_ignore_dirs and not d.startswith('.')]

            
            for file_name in files:
                _, ext = os.path.splitext(file_name)
                lang_object = self.languages.get(ext)
                if not lang_object: continue
                
                full_path = os.path.join(root, file_name)
                rel_path = os.path.relpath(full_path, self.repo_path)

                if not self._should_process_file(rel_path): continue
                if files_to_process and rel_path not in files_to_process: continue

                try:
                    content, error_msg = self._safe_read_file(full_path)
                    
                    if error_msg:
                        # Log o skip silenzioso per non sporcare il DB con errori noti
                        continue

                    file_rec = FileRecord(
                        id=str(uuid.uuid4()), 
                        snapshot_id=self.snapshot_id, 
                        commit_hash=commit_hash,
                        file_hash="", path=rel_path,
                        language=self.LANGUAGE_MAP[ext], size_bytes=0,
                        category=self.metadata_provider.get_file_category(rel_path),
                        indexed_at=datetime.datetime.utcnow().isoformat() + "Z",
                        parsing_status="success", parsing_error=None
                    )

                    if self._is_minified_or_generated(content, rel_path):
                        file_rec.parsing_status = "skipped"
                        file_rec.parsing_error = "Minified/Generated"
                        # Yieldiamo comunque il file skipped per tracciabilità nel DB?
                        # Enterprise: Sì, meglio sapere perché manca un file.
                        yield (file_rec, [], [], [])
                        continue

                    if error_msg:
                        file_rec.parsing_status = "skipped"
                        file_rec.parsing_error = error_msg
                        yield (file_rec, [], [], [])
                        continue

                    file_rec.size_bytes = len(content)
                    file_rec.file_hash = self.metadata_provider.get_file_hash(rel_path, content)
                    
                    if self._is_minified_or_generated(content, rel_path):
                        file_rec.parsing_status = "skipped"
                        file_rec.parsing_error = "Minified/Generated"
                        yield (file_rec, [], [], [])
                        continue

                    self._set_parser_language(lang_object)
                    tree = self.parser.parse(content)
                    
                    lang_name = self.LANGUAGE_MAP[ext]
                    semantic_captures = self._get_semantic_captures(tree, lang_name)
                    
                    nodes = []; contents = {}; relations = []
                    
                    # [OPTIMIZATION] Uso memoryview per zero-copy slicing
                    mv_content = memoryview(content)
                    
                    self._process_scope(
                        tree.root_node, mv_content, content, rel_path, file_rec.id, None, 
                        nodes, contents, relations,
                        semantic_captures=semantic_captures
                    )

                    if nodes:
                        nodes.sort(key=lambda c: c.byte_range[0])
                        yield (file_rec, nodes, list(contents.values()), relations)
                    else:
                        yield (file_rec, [], [], [])

                except Exception as e: 
                    # Logghiamo ma non fermiamo il processo
                    print(f"[ERROR] Processing {rel_path}: {e}")
                    try:
                        err_rec = FileRecord(
                            id=str(uuid.uuid4()), 
                            snapshot_id=self.snapshot_id,
                            commit_hash=commit_hash,
                            file_hash="error", path=rel_path, language=self.LANGUAGE_MAP.get(ext, "unknown"),
                            size_bytes=0, category="unknown",
                            indexed_at=datetime.datetime.utcnow().isoformat() + "Z",
                            parsing_status="failed", parsing_error=str(e)
                        )
                        yield (err_rec, [], [], [])
                    except: pass

    def extract_semantic_chunks(self) -> ParsingResult:
        files, nodes, contents, all_rels = [], [], {}, []
        for f, n, c, r in self.stream_semantic_chunks():
            files.append(f); nodes.extend(n); all_rels.extend(r)
            for item in c: contents[item.chunk_hash] = item
        return ParsingResult(files, nodes, contents, all_rels)
    

    def _should_process_file(self, rel_path: str) -> bool:
        """
        Motore decisionale Enterprise per filtrare i file.
        Ritorna True se il file è valido per l'indicizzazione.
        """
        parts = rel_path.split(os.sep)
        filename = parts[-1]
        _, ext = os.path.splitext(filename)
        
        # 1. Fast Directory Check (O(1) lookup)
        # Controlla se una qualsiasi directory genitore è nella blacklist
        for part in parts[:-1]:
            if part in self.all_ignore_dirs or part.startswith('.'):
                return False

        # 2. Configurazione Specifica Linguaggio
        lang_key = self.EXT_TO_LANG_CONFIG.get(ext)
        if lang_key:
            config = LANGUAGE_SPECIFIC_FILTERS[lang_key]
            
            # Check Estensioni proibite (es. .pyc, .min.js)
            if ext in config.get("exclude_extensions", set()):
                return False
                
            # Check Patterns (Glob matching, es. *_test.py)
            for pattern in config.get("exclude_patterns", []):
                if fnmatch.fnmatch(filename, pattern) or fnmatch.fnmatch(rel_path, pattern):
                    # Se è un pattern escluso (es. test), decidiamo se il parser lo vuole o no.
                    # Nel design discusso: Parser VUOLE i test (Semantic Context), SCIP no.
                    # Quindi qui ritorniamo True (o meglio, non ritorniamo False).
                    # SE invece vuoi che il parser ignori i test, scommenta:
                    # return False 
                    pass 
        
        # 3. Check Generici (Lockfiles, Dotfiles nascosti)
        if filename.startswith('.') or filename.endswith('.lock'):
            return False
            
        return True

    # ==============================================================================
    #  LOGICA CHUNKING (OPTIMIZED - ZERO COPY & BYTEARRAY)
    # ==============================================================================
    
    def _process_scope(self, parent_node: Node, content_mv: memoryview, full_content_bytes: bytes, 
                       file_path: str, file_id: str, parent_chunk_id: Optional[str],
                       nodes: List, contents: Dict, relations: List, 
                       initial_glue: Union[bytes, bytearray] = b"", initial_glue_start: Optional[int] = None,
                       is_breakdown_mode: bool = False,
                       semantic_captures: List[Dict] = None):
        
        semantic_captures = semantic_captures or []
        body_node = parent_node.child_by_field_name("body") or parent_node.child_by_field_name("block") or parent_node.child_by_field_name("consequence")
        iterator_node = body_node if body_node else parent_node
        
        cursor = iterator_node.start_byte
        
        # [OPTIMIZATION] bytearray è mutabile: append veloce senza reallocazione stringa
        glue_buffer = bytearray(initial_glue)
        glue_start_byte = initial_glue_start 
        if initial_glue and glue_start_byte is None:
            glue_start_byte = iterator_node.start_byte 

        group_buffer = bytearray()
        group_start_byte = None; group_end_byte = None
        current_active_parent = parent_chunk_id
        first_chunk_created_in_scope = False

        def register_chunk_creation(new_chunk_id: str):
            nonlocal current_active_parent, first_chunk_created_in_scope
            if is_breakdown_mode and not first_chunk_created_in_scope:
                current_active_parent = new_chunk_id
                first_chunk_created_in_scope = True

        def flush_group():
            nonlocal group_buffer, group_start_byte, group_end_byte
            if group_buffer:
                cid = self._create_chunk(
                    group_buffer, group_start_byte, group_end_byte, full_content_bytes,
                    file_path, file_id, current_active_parent,
                    nodes, contents, relations, semantic_captures=semantic_captures
                )
                register_chunk_creation(cid)
                # Reset buffer veloce
                group_buffer = bytearray(); group_start_byte = None; group_end_byte = None

        for child in iterator_node.children:
            if child.start_byte > cursor:
                # [OPTIMIZATION] Zero-copy slice
                gap = content_mv[cursor : child.start_byte]
                glue_buffer.extend(gap)
                if glue_start_byte is None: glue_start_byte = cursor

            is_glue = (child.type in self.GLUE_TYPES or child.type.startswith('comment'))
            is_barrier = (child.type in self.CONTAINER_TYPES)
            
            # [OPTIMIZATION] Zero-copy slice
            child_mv = content_mv[child.start_byte : child.end_byte]
            
            if is_glue:
                glue_buffer.extend(child_mv)
                if glue_start_byte is None: glue_start_byte = child.start_byte
            
            elif is_barrier:
                flush_group()
                
                barrier_start = glue_start_byte if glue_start_byte is not None else child.start_byte
                barrier_end = child.end_byte
                
                # Check dimensione approssimativa (somma lunghezze)
                full_len = len(glue_buffer) + len(child_mv)
                
                if full_len > self.MAX_CHUNK_SIZE:
                    # Necessario convertire in bytes immutabili solo per breakdown ricorsivo
                    prefix_bytes = bytes(glue_buffer)
                    self._handle_large_node(
                        child, content_mv, full_content_bytes, prefix_bytes, barrier_start, 
                        file_path, file_id, current_active_parent,
                        nodes, contents, relations, semantic_captures=semantic_captures
                    )
                    if is_breakdown_mode and not first_chunk_created_in_scope:
                         first_chunk_created_in_scope = True
                else:
                    # Fast path: uniamo nel bytearray
                    combined = bytearray(glue_buffer)
                    combined.extend(child_mv)
                    
                    cid = self._create_chunk(
                        combined, barrier_start, barrier_end, full_content_bytes,
                        file_path, file_id, current_active_parent,
                        nodes, contents, relations, tags=self._extract_tags(child),
                        semantic_captures=semantic_captures
                    )
                    register_chunk_creation(cid)
                
                glue_buffer = bytearray(); glue_start_byte = None

            else:
                if not group_buffer:
                    group_start_byte = glue_start_byte if glue_start_byte is not None else child.start_byte
                
                if glue_buffer:
                    group_buffer.extend(glue_buffer)
                    glue_buffer = bytearray(); glue_start_byte = None
                
                group_buffer.extend(child_mv)
                group_end_byte = child.end_byte
                
                if len(group_buffer) > self.MAX_CHUNK_SIZE:
                    remaining = iterator_node.end_byte - child.end_byte
                    if remaining > self.CHUNK_TOLERANCE: flush_group()

            cursor = child.end_byte

        flush_group()
        
        if cursor < iterator_node.end_byte:
            glue_buffer.extend(content_mv[cursor : iterator_node.end_byte])
            
        if glue_buffer:
             # Check veloce su bytearray
             if not bytes(glue_buffer).strip(): return

             start = glue_start_byte if glue_start_byte is not None else cursor
             self._create_chunk(
                glue_buffer, start, iterator_node.end_byte, full_content_bytes,
                file_path, file_id, current_active_parent,
                nodes, contents, relations, tags=[], semantic_captures=semantic_captures
            )

    def _handle_large_node(self, node: Node, content_mv: memoryview, full_content_bytes: bytes, 
                           prefix: bytes, prefix_start: int, 
                           file_path: str, file_id: str, parent_chunk_id: Optional[str],
                           nodes: List, contents: Dict, relations: List,
                           semantic_captures: List[Dict] = None):
        
        target_node = node
        if node.type == 'decorated_definition':
            definition = node.child_by_field_name('definition')
            if definition: target_node = definition
        elif node.type == 'export_statement':
            d = node.child_by_field_name('declaration') or node.child_by_field_name('value')
            if d: target_node = d

        body_node = target_node.child_by_field_name("body") or target_node.child_by_field_name("block")
        
        if not body_node:
            # Fallback: Hard split su bytes
            full_text_bytes = prefix + bytes(content_mv[node.start_byte:node.end_byte])
            self._create_hard_split(
                full_text_bytes, prefix_start, full_content_bytes,
                file_path, file_id, parent_chunk_id, nodes, contents, relations, semantic_captures
            )
            return

        header_node_mv = content_mv[node.start_byte : body_node.start_byte]
        full_header_len = len(prefix) + len(header_node_mv)
        
        if full_header_len > self.MAX_CHUNK_SIZE * 0.6:
            # Header Chunk separato
            header_buffer = bytearray(prefix)
            header_buffer.extend(header_node_mv)
            
            header_id = self._create_chunk(
                header_buffer, prefix_start, body_node.start_byte, full_content_bytes,
                file_path, file_id, parent_chunk_id,
                nodes, contents, relations, tags=self._extract_tags(node),
                semantic_captures=semantic_captures
            )
            self._process_scope(target_node, content_mv, full_content_bytes, file_path, file_id, header_id, nodes, contents, relations, semantic_captures=semantic_captures)
        else:
            # Flow-Down: Uniamo prefisso e header per il prossimo scope
            new_glue = bytearray(prefix)
            new_glue.extend(header_node_mv)
            
            self._process_scope(
                target_node, content_mv, full_content_bytes, file_path, file_id, parent_chunk_id, 
                nodes, contents, relations, 
                initial_glue=new_glue, initial_glue_start=prefix_start,
                is_breakdown_mode=True,
                semantic_captures=semantic_captures
            )

    def _create_hard_split(self, text_bytes: bytes, start_offset: int, full_content: bytes,
                           fpath: str, fid: str, pid: str,
                           nodes: List, contents: Dict, relations: List, semantic_captures: List[Dict] = None):
        total = len(text_bytes); cursor = 0
        first_fragment_id = None
        while cursor < total:
            end = min(cursor + self.MAX_CHUNK_SIZE, total)
            if end < total:
                nl = text_bytes.rfind(b'\n', cursor, end)
                if nl > cursor + (self.MAX_CHUNK_SIZE // 2): end = nl + 1
            chunk = text_bytes[cursor:end]
            current_pid = pid if not first_fragment_id else first_fragment_id
            cid = self._create_chunk(
                chunk, start_offset + cursor, start_offset + end, full_content,
                fpath, fid, current_pid, nodes, contents, relations, tags=[], semantic_captures=semantic_captures
            )
            if not first_fragment_id: first_fragment_id = cid
            cursor = end

    def _create_chunk(self, text_obj: Union[bytes, bytearray, memoryview], s_byte: int, e_byte: int, full_content_bytes: bytes,
                      fpath: str, fid: str, pid: Optional[str], 
                      nodes: List, contents: Dict, relations: List,
                      tags: List[str] = None,
                      semantic_captures: List[Dict] = None) -> Optional[str]:
        
        # Conversione finale solo quando serve
        if isinstance(text_obj, memoryview):
            text_bytes = text_obj.tobytes()
        elif isinstance(text_obj, bytearray):
            text_bytes = bytes(text_obj)
        else:
            text_bytes = text_obj

        text = text_bytes.decode('utf-8', 'ignore')
        if not text.strip(): return None
        
        # Hashing
        h = hashlib.sha256(text.encode('utf-8')).hexdigest()
        if h not in contents: contents[h] = ChunkContent(h, text)
        cid = str(uuid.uuid4())
        
        if s_byte < 0: s_byte = 0
        
        # Calcolo righe ottimizzato (uso full_content_bytes originale)
        s_line = full_content_bytes[:s_byte].count(b'\n') + 1
        e_line = s_line + text.count('\n')
        
        # Semantic Enrichment
        matches = []
        if semantic_captures:
            for cap in semantic_captures:
                cap_start, cap_end = cap['start'], cap['end']
                if (s_byte >= cap_start and e_byte <= cap_end) or \
                   (cap_start >= s_byte and cap_end <= e_byte):
                    matches.append(cap['metadata'])

        metadata = {}
        if matches: metadata["semantic_matches"] = matches
        if tags: metadata["tags"] = tags

        nodes.append(ChunkNode(
            cid, fid, fpath, h, 
            s_line, e_line, [s_byte, e_byte],
            metadata=metadata
        ))
        
        if pid:
             relations.append(CodeRelation(
                source_file=fpath, target_file=fpath, relation_type="child_of",
                source_id=cid, target_id=pid, metadata={"tool": "treesitter_repo_parser"}
            ))
            
        return cid

    def _extract_tags(self, child: Node) -> List[str]:
        tags: List[str] = []
        if child.type.startswith("async_") or any(c.type == "async" for c in child.children): tags.append("async")
        if child.type == "decorated_definition" or any(c.type == "decorator" for c in child.children): tags.append("decorated")
        if child.type == "export_statement" or (child.parent and child.parent.type == "export_statement"): tags.append("exported")
        if "constructor" in child.type: tags.append("constructor")
        for c in child.children:
            if c.type == "static": tags.append("static")
        return list(set(tags))
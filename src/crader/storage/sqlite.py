import datetime
import json
import logging
import os
import sqlite3
import struct
import uuid
from typing import Any, Dict, Generator, List, Optional, Tuple

try:
    import numpy as np

    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

from .base import GraphStorage

logger = logging.getLogger(__name__)


class SqliteGraphStorage(GraphStorage):
    def __init__(self, db_path: str = "sheep_index.db"):
        self._db_file = os.path.abspath(db_path)
        logger.info(f"ðŸ’¾ Storage Database: {self._db_file}")

        self._conn = sqlite3.connect(self._db_file, check_same_thread=False)
        self._cursor = self._conn.cursor()

        self._cursor.execute("PRAGMA synchronous = OFF")
        self._cursor.execute("PRAGMA journal_mode = WAL")
        self._cursor.execute("PRAGMA cache_size = 5000")

        # --- REPOSITORIES ---
        self._cursor.execute("""
            CREATE TABLE IF NOT EXISTS repositories (
                id TEXT PRIMARY KEY,
                url TEXT NOT NULL, branch TEXT NOT NULL, name TEXT,
                last_commit TEXT, status TEXT, updated_at TEXT, local_path TEXT,
                UNIQUE(url, branch)
            )
        """)

        # --- FILES ---
        self._cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id TEXT PRIMARY KEY, repo_id TEXT, commit_hash TEXT, file_hash TEXT,
                path TEXT, language TEXT, size_bytes INTEGER, category TEXT, indexed_at TEXT,
                UNIQUE(repo_id, path)
            )
        """)
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_path ON files (path)")
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_files_repo ON files (repo_id)")

        # --- NODES (No 'type' column) ---
        self._cursor.execute("""
            CREATE TABLE IF NOT EXISTS nodes (
                id TEXT PRIMARY KEY, 
                file_id TEXT,
                file_path TEXT,
                start_line INTEGER, end_line INTEGER, byte_start INTEGER, byte_end INTEGER,
                chunk_hash TEXT, 
                size INTEGER,
                metadata_json TEXT 
            )
        """)
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_file_id ON nodes (file_id)")
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_nodes_spatial ON nodes (file_path, byte_start)")

        # --- CONTENT & EDGES ---
        self._cursor.execute("CREATE TABLE IF NOT EXISTS contents (chunk_hash TEXT PRIMARY KEY, content TEXT)")
        self._cursor.execute("""
            CREATE TABLE IF NOT EXISTS edges (
                source_id TEXT, target_id TEXT, relation_type TEXT, metadata_json TEXT
            )
        """)
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_edges_source ON edges (source_id)")
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_edges_target ON edges (target_id)")

        # --- SEARCH: FTS (Unified Index) ---
        try:
            self._cursor.execute("""
                CREATE VIRTUAL TABLE IF NOT EXISTS nodes_fts 
                USING fts5(node_id UNINDEXED, file_path, semantic_tags, content, tokenize='trigram')
            """)
        except Exception:
            self._cursor.execute("""
                CREATE VIRTUAL TABLE IF NOT EXISTS nodes_fts 
                USING fts5(node_id UNINDEXED, file_path, semantic_tags, content)
            """)

        # --- SEARCH: VECTORS (Normalized) ---
        self._cursor.execute("""
            CREATE TABLE IF NOT EXISTS node_embeddings (
                id TEXT PRIMARY KEY, chunk_id TEXT, repo_id TEXT, file_path TEXT, directory TEXT, 
                branch TEXT, language TEXT, category TEXT, 
                start_line INTEGER, end_line INTEGER, 
                vector_hash TEXT, model_name TEXT, created_at TEXT, embedding BLOB
            )
        """)
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_emb_hash ON node_embeddings (vector_hash)")
        self._cursor.execute("CREATE INDEX IF NOT EXISTS idx_emb_repo ON node_embeddings (repo_id)")
        self._conn.commit()

    # --- REPO MANAGEMENT ---
    def get_repository(self, repo_id: str) -> Optional[Dict[str, Any]]:
        self._cursor.execute("SELECT * FROM repositories WHERE id = ?", (repo_id,))
        row = self._cursor.fetchone()
        if not row:
            return None
        cols = [d[0] for d in self._cursor.description]
        return dict(zip(cols, row))

    def get_repository_by_context(self, url: str, branch: str) -> Optional[Dict[str, Any]]:
        self._cursor.execute("SELECT * FROM repositories WHERE url = ? AND branch = ?", (url, branch))
        row = self._cursor.fetchone()
        if not row:
            return None
        cols = [d[0] for d in self._cursor.description]
        return dict(zip(cols, row))

    def register_repository(
        self, id: str, name: str, url: str, branch: str, commit_hash: str, local_path: str = None
    ) -> str:
        now = datetime.datetime.utcnow().isoformat()
        existing = self.get_repository_by_context(url, branch)
        if existing:
            repo_id = existing["id"]
            self._cursor.execute(
                """
                UPDATE repositories SET name=?, last_commit=?, status='indexing', updated_at=?, local_path=?
                WHERE id=?
            """,
                (name, commit_hash, now, local_path, repo_id),
            )
        else:
            repo_id = str(uuid.uuid4())
            self._cursor.execute(
                """
                INSERT INTO repositories (id, name, url, branch, last_commit, status, updated_at, local_path)
                VALUES (?, ?, ?, ?, ?, 'indexing', ?, ?)
            """,
                (repo_id, name, url, branch, commit_hash, now, local_path),
            )
        self._conn.commit()
        return repo_id

    def update_repository_status(self, repo_id: str, status: str, commit_hash: str = None):
        now = datetime.datetime.utcnow().isoformat()
        if commit_hash:
            self._cursor.execute(
                "UPDATE repositories SET status = ?, last_commit = ?, updated_at = ? WHERE id = ?",
                (status, commit_hash, now, repo_id),
            )
        else:
            self._cursor.execute(
                "UPDATE repositories SET status = ?, updated_at = ? WHERE id = ?", (status, now, repo_id)
            )
        self._conn.commit()

    def delete_previous_data(self, repo_id: str, branch: str):
        try:
            self._cursor.execute("DELETE FROM node_embeddings WHERE repo_id = ?", (repo_id,))
            self._cursor.execute(
                "DELETE FROM edges WHERE source_id IN (SELECT n.id FROM nodes n JOIN files f ON n.file_id = f.id WHERE f.repo_id = ?)",
                (repo_id,),
            )
            self._cursor.execute(
                "DELETE FROM nodes WHERE file_id IN (SELECT id FROM files WHERE repo_id = ?)", (repo_id,)
            )
            self._cursor.execute("DELETE FROM files WHERE repo_id = ?", (repo_id,))
            self._conn.commit()
        except Exception as e:
            logger.error(f"Errore delete_previous_data: {e}")

    def acquire_indexing_lock(
        self, url: str, branch: str, name: str, commit_hash: str, local_path: str = None, timeout_minutes: int = 30
    ) -> Tuple[bool, Optional[str]]:
        """
        Simple lock implementation using the repositories table status.
        """
        existing = self.get_repository_by_context(url, branch)
        if existing:
            repo_id = existing["id"]
            # If already indexing and not stale (timeout logic omitted for simplicity or assume manual reset)
            # For this tool, we might want to allow re-indexing if forced, but here we follow the contract.
            if existing["status"] == "indexing":
                # Check if stale? For now, just return False if strictly locked.
                # But for a debugger, maybe we want to be lenient.
                # Let's assume if it's indexing, we can't acquire unless we force (which is handled by caller usually).
                # But the caller (indexer.py) checks this return.
                # Let's just update timestamp and return True if we are the ones asking (which we are).
                # Actually, indexer.py calls this to START indexing.
                pass

            # Update to indexing
            self.update_repository_status(repo_id, "indexing", commit_hash)
            return True, repo_id
        else:
            # Create new
            repo_id = self.register_repository(None, name, url, branch, commit_hash, local_path)
            return True, repo_id

    def release_indexing_lock(self, repo_id: str, success: bool, commit_hash: str = None):
        status = "indexed" if success else "failed"
        self.update_repository_status(repo_id, status, commit_hash)
        # Return None to indicate no next commit to process (simple mode)
        return None

    def get_vectors_by_hashes(self, vector_hashes: List[str], model_name: str) -> Dict[str, List[float]]:
        if not vector_hashes:
            return {}
        # This is an optimization to avoid re-embedding.
        # We check node_embeddings for these hashes.
        # Note: node_embeddings has vector_hash column.

        # We need to return {hash: vector}.
        # But node_embeddings stores vector as blob.

        result = {}
        # Chunking for sqlite limits
        for i in range(0, len(vector_hashes), 900):
            batch = vector_hashes[i : i + 900]
            ph = ",".join(["?"] * len(batch))
            self._cursor.execute(
                f"SELECT vector_hash, embedding FROM node_embeddings WHERE model_name = ? AND vector_hash IN ({ph})",
                [model_name] + batch,
            )

            for row in self._cursor:
                v_hash, blob = row
                if not blob:
                    continue
                # Unpack
                # We don't know dimension easily here without parsing blob length
                # Blob is N floats. len(blob) / 4 = N
                dim = len(blob) // 4
                vec = struct.unpack(f"{dim}f", blob)
                result[v_hash] = list(vec)

        return result

    # --- WRITE ---
    def add_files(self, files: List[Any]):
        sql_batch = []
        for f in files:
            d = f.to_dict() if hasattr(f, "to_dict") else f
            sql_batch.append(
                (
                    d["id"],
                    d["repo_id"],
                    d.get("commit_hash", ""),
                    d["file_hash"],
                    d["path"],
                    d["language"],
                    d["size_bytes"],
                    d["category"],
                    d["indexed_at"],
                )
            )
        if sql_batch:
            self._cursor.executemany("INSERT OR REPLACE INTO files VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)", sql_batch)
            self._conn.commit()

    def add_nodes(self, nodes: List[Any]):
        sql_batch = []
        for n in nodes:
            d = n.to_dict() if hasattr(n, "to_dict") else n
            b_start = d["byte_range"][0]
            b_end = d["byte_range"][1]
            meta = json.dumps(d.get("metadata", {}))
            sql_batch.append(
                (
                    d["id"],
                    d.get("file_id"),
                    d["file_path"],
                    d["start_line"],
                    d["end_line"],
                    b_start,
                    b_end,
                    d.get("chunk_hash", ""),
                    b_end - b_start,
                    meta,
                )
            )
        if sql_batch:
            self._cursor.executemany("INSERT OR IGNORE INTO nodes VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", sql_batch)
            self._conn.commit()

    def add_contents(self, contents: List[Any]):
        sql_batch = []
        for c in contents:
            d = c.to_dict() if hasattr(c, "to_dict") else c
            sql_batch.append((d["chunk_hash"], d["content"]))
        if sql_batch:
            self._cursor.executemany("INSERT OR IGNORE INTO contents VALUES (?, ?)", sql_batch)
            self._conn.commit()

    def add_edge(self, source_id: str, target_id: str, relation_type: str, metadata: Dict[str, Any]):
        self._cursor.execute(
            "INSERT INTO edges VALUES (?, ?, ?, ?)", (source_id, target_id, relation_type, json.dumps(metadata))
        )

    def add_search_index(self, search_docs: List[Dict[str, Any]]):
        sql_batch = []
        for doc in search_docs:
            sql_batch.append((doc["node_id"], doc["file_path"], doc["tags"], doc["content"]))
        if sql_batch:
            self._cursor.executemany(
                "INSERT OR REPLACE INTO nodes_fts (node_id, file_path, semantic_tags, content) VALUES (?, ?, ?, ?)",
                sql_batch,
            )
            self._conn.commit()

    def save_embeddings(self, vector_documents: List[Dict[str, Any]]):
        sql_batch = []
        for doc in vector_documents:
            vector = doc["vector"]
            vector_blob = struct.pack(f"{len(vector)}f", *vector)
            sql_batch.append(
                (
                    doc["id"],
                    doc["chunk_id"],
                    doc.get("repo_id"),
                    doc.get("file_path"),
                    doc.get("directory"),
                    doc.get("branch"),
                    doc.get("language"),
                    doc.get("category"),
                    doc.get("start_line"),
                    doc.get("end_line"),
                    doc.get("vector_hash"),
                    doc.get("model_name"),
                    doc.get("created_at"),
                    vector_blob,
                )
            )
        if sql_batch:
            p = ",".join(["?"] * 14)
            self._cursor.executemany(f"INSERT OR REPLACE INTO node_embeddings VALUES ({p})", sql_batch)
            self._conn.commit()

    # --- RETRIEVAL (FIXED) ---

    # ==========================================
    # FILTERING HELPER
    # ==========================================

    def _build_filter_clause(self, filters: Dict[str, Any]) -> Tuple[str, List[Any]]:
        """
        Costruisce la clausola WHERE dinamica.
        [FIX] Supporta liste per tutti i campi (OR logic per include, AND per exclude).
        """
        if not filters:
            return "", []

        clauses = []
        params = []

        # Helper robusto per normalizzare a lista
        def as_list(val):
            if val is None:
                return []
            return val if isinstance(val, list) else [val]

        # 1. PATH (OR)
        if "path_prefix" in filters and filters["path_prefix"]:
            paths = as_list(filters["path_prefix"])
            path_clauses = []
            for p in paths:
                if not isinstance(p, str):
                    continue
                clean = p.strip(os.sep)
                path_clauses.append("f.path LIKE ?")
                params.append(f"{clean}%")
            if path_clauses:
                clauses.append(f"({' OR '.join(path_clauses)})")

        # 2. LANGUAGE (IN / NOT IN)
        if "language" in filters and filters["language"]:
            langs = as_list(filters["language"])
            if langs:
                ph = ",".join(["?"] * len(langs))
                clauses.append(f"f.language IN ({ph})")
                params.extend(langs)

        if "exclude_language" in filters and filters["exclude_language"]:
            ex_langs = as_list(filters["exclude_language"])
            if ex_langs:
                ph = ",".join(["?"] * len(ex_langs))
                clauses.append(f"f.language NOT IN ({ph})")
                params.extend(ex_langs)

        # 3. SEMANTIC FILTERS (JSON)
        def add_json_list_match(key, values, exclude=False):
            vals = as_list(values)
            if not vals:
                return

            ph = ",".join(["?"] * len(vals))
            op = "NOT EXISTS" if exclude else "EXISTS"

            subquery = f"""
                {op} (
                    SELECT 1 FROM json_each(n.metadata_json, '$.semantic_matches') 
                    WHERE json_extract(value, '$.{key}') IN ({ph})
                )
            """
            clauses.append(subquery)
            params.extend(vals)

        if "role" in filters:
            add_json_list_match("value", filters["role"])
        if "exclude_role" in filters:
            add_json_list_match("value", filters["exclude_role"], exclude=True)

        # 4. CATEGORY (HYBRID)
        if "category" in filters and filters["category"]:
            cats = as_list(filters["category"])
            if cats:
                ph = ",".join(["?"] * len(cats))
                file_logic = f"f.category IN ({ph})"
                chunk_logic = f"""
                    EXISTS (
                        SELECT 1 FROM json_each(n.metadata_json, '$.semantic_matches') 
                        WHERE json_extract(value, '$.category') IN ({ph})
                    )
                """
                clauses.append(f"({file_logic} OR {chunk_logic})")
                params.extend(cats)
                params.extend(cats)

        if "exclude_category" in filters and filters["exclude_category"]:
            ex_cats = as_list(filters["exclude_category"])
            if ex_cats:
                ph = ",".join(["?"] * len(ex_cats))
                file_logic = f"f.category NOT IN ({ph})"
                chunk_logic = f"""
                    NOT EXISTS (
                        SELECT 1 FROM json_each(n.metadata_json, '$.semantic_matches') 
                        WHERE json_extract(value, '$.category') IN ({ph})
                    )
                """
                clauses.append(f"({file_logic} AND {chunk_logic})")
                params.extend(ex_cats)
                params.extend(ex_cats)

        if not clauses:
            return "", []
        return " AND " + " AND ".join(clauses), params

    # ==========================================
    # RETRIEVAL METHODS
    # ==========================================

    def search_vectors(
        self,
        query_vector: List[float],
        limit: int = 20,
        repo_id: str = None,
        branch: str = None,
        filters: Dict[str, Any] = None,
    ) -> List[Dict[str, Any]]:
        if not HAS_NUMPY:
            return []

        # JOIN Completa: Embeddings -> Nodes -> Contents -> Files
        # Necessaria per applicare i filtri su path/lang (files) e semantic (nodes)
        sql = """
            SELECT ne.id, ne.embedding, ne.chunk_id, ne.file_path, 
                   ne.start_line, ne.end_line, 
                   ne.repo_id, ne.branch, n.metadata_json, c.content
            FROM node_embeddings ne
            JOIN nodes n ON ne.chunk_id = n.id
            JOIN contents c ON n.chunk_hash = c.chunk_hash
            JOIN files f ON n.file_id = f.id
            WHERE 1=1
        """
        params = []

        # Filtri Base (Context)
        if repo_id:
            sql += " AND ne.repo_id = ?"
            params.append(repo_id)
        if branch:
            sql += " AND ne.branch = ?"
            params.append(branch)

        # Filtri Avanzati (Agente)
        filter_sql, filter_params = self._build_filter_clause(filters)
        sql += filter_sql
        params.extend(filter_params)

        self._cursor.execute(sql, params)
        rows = self._cursor.fetchall()

        if not rows:
            return []

        # Calcolo SimilaritÃ  Cosine (In-Memory con Numpy per SQLite)
        ids, vectors, metadata_map = [], [], {}
        dim = len(query_vector)
        fmt = f"{dim}f"

        for r in rows:
            emb_id, blob = r[0], r[1]
            if not blob or len(blob) != dim * 4:
                continue
            try:
                vec = struct.unpack(fmt, blob)
                vectors.append(vec)
                ids.append(emb_id)

                metadata_map[emb_id] = {
                    "id": r[2],
                    "file_path": r[3],
                    "start_line": r[4],
                    "end_line": r[5],
                    "repo_id": r[6],
                    "branch": r[7],
                    "metadata": json.loads(r[8] or "{}"),
                    "content": r[9],
                }
            except Exception:
                continue

        if not vectors:
            return []

        np_vecs = np.array(vectors, dtype=np.float32)
        np_query = np.array(query_vector, dtype=np.float32)

        norm_vecs = np.linalg.norm(np_vecs, axis=1, keepdims=True)
        norm_query = np.linalg.norm(np_query)

        if norm_query == 0:
            return []
        norm_vecs[norm_vecs == 0] = 1e-10

        similarities = np.dot(np_vecs, np_query) / (norm_vecs.squeeze() * norm_query)

        # Top-K
        k_indices = np.argsort(similarities)[-limit:][::-1]

        results = []
        for idx in k_indices:
            emb_id = ids[idx]
            score = float(similarities[idx])
            meta = metadata_map[emb_id]
            results.append({**meta, "score": score})

        return results

    def search_fts(
        self, query: str, limit: int = 20, repo_id: str = None, branch: str = None, filters: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        # Pulizia e Preparazione Strategie FTS
        clean_query = query.replace('"', "").replace("'", "")
        words = clean_query.split()
        if not words:
            return []

        # "Blind Quoting" per gestire caratteri speciali (es. app.py -> "app.py")
        quoted_words = [f'"{w}"' for w in words]

        strategies = [
            f'"{clean_query}"',  # 1. Phrase Match
        ]
        if len(words) > 1:
            strategies.append(" AND ".join(quoted_words))  # 2. AND Match
            strategies.append(" OR ".join(quoted_words))  # 3. OR Match

        # Query Base FTS
        # JOIN necessarie per:
        # - Recuperare contenuto (contents)
        # - Filtrare per repo/path (files)
        # - Filtrare per metadati semantici (nodes)
        base_sql = """
            SELECT 
                nodes_fts.node_id, nodes_fts.file_path, n.start_line, n.end_line, 
                nodes_fts.rank, nodes_fts.content, f.repo_id, r.branch, n.metadata_json
            FROM nodes_fts
            JOIN nodes n ON nodes_fts.node_id = n.id
            JOIN files f ON n.file_id = f.id
            JOIN repositories r ON f.repo_id = r.id
            WHERE nodes_fts MATCH ? 
        """

        params_base = []

        # Filtri Base
        if repo_id:
            base_sql += " AND f.repo_id = ?"
            params_base.append(repo_id)
        if branch:
            base_sql += " AND r.branch = ?"
            params_base.append(branch)

        # Filtri Avanzati
        filter_sql, filter_params = self._build_filter_clause(filters)
        base_sql += filter_sql
        params_base.extend(filter_params)

        base_sql += " ORDER BY nodes_fts.rank ASC LIMIT ?"
        params_base.append(limit)

        # Loop Strategie (Fallback)
        for i, strategy_query in enumerate(strategies):
            try:
                # Eseguiamo query con la strategia corrente
                self._cursor.execute(base_sql, [strategy_query] + params_base)
                rows = self._cursor.fetchall()

                if rows:
                    results = []
                    for row in rows:
                        results.append(
                            {
                                "id": row[0],
                                "file_path": row[1],
                                "start_line": row[2],
                                "end_line": row[3],
                                "score": row[4],
                                "content": row[5],
                                "repo_id": row[6],
                                "branch": row[7],
                                "metadata": json.loads(row[8] or "{}"),
                            }
                        )
                    return results
            except Exception as e:
                # Log errore specifico ma continua (es. errore syntax FTS su caratteri strani)
                logger.debug(f"FTS Strategy {i} failed: {e}")
                continue

        return []

    # --- BATCH & UTILS ---
    def get_nodes_cursor(self, repo_id: str = None, branch: str = None) -> Generator[Dict[str, Any], None, None]:
        base_query = """
            SELECT n.id, n.file_path, n.chunk_hash, n.start_line, n.end_line, n.metadata_json,
                   f.repo_id, f.language, f.category
            FROM nodes n JOIN files f ON n.file_id = f.id JOIN repositories r ON f.repo_id = r.id 
            WHERE 1=1
        """
        params = []
        if repo_id:
            base_query += " AND f.repo_id = ?"
            params.append(repo_id)

        cursor = self._conn.cursor()
        cursor.execute(base_query, params)
        if cursor.description:
            cols = [d[0] for d in cursor.description]
            for row in cursor:
                yield dict(zip(cols, row))
        cursor.close()

    def get_nodes_to_embed(self, repo_id: str, model_name: str) -> Generator[Dict[str, Any], None, None]:
        # [MOD] Include n.metadata_json
        sql = """
            SELECT n.id, n.file_path, n.chunk_hash, n.start_line, n.end_line, n.metadata_json,
                   f.repo_id, r.branch, f.language, f.category 
            FROM files f JOIN repositories r ON f.repo_id = r.id JOIN nodes n ON f.id = n.file_id
            LEFT JOIN node_embeddings ne ON (n.id = ne.chunk_id AND ne.model_name = ?)
            WHERE f.repo_id = ? AND ne.id IS NULL
        """
        cursor = self._conn.cursor()
        cursor.execute(sql, (model_name, repo_id))
        if cursor.description:
            cols = [d[0] for d in cursor.description]
            for row in cursor:
                yield dict(zip(cols, row))
        cursor.close()

    def find_chunk_id(self, file_path: str, byte_range: List[int], repo_id: str = None) -> Optional[str]:
        if not byte_range:
            return None
        sql = "SELECT n.id FROM nodes n JOIN files f ON n.file_id = f.id WHERE f.path = ? AND n.byte_start <= ? + 1 AND n.byte_end >= ? - 1"
        params = [file_path, byte_range[0], byte_range[1]]
        if repo_id:
            sql += " AND f.repo_id = ?"
            params.append(repo_id)
        sql += " ORDER BY n.size ASC LIMIT 1"
        self._cursor.execute(sql, params)
        row = self._cursor.fetchone()
        return row[0] if row else None

    def ensure_external_node(self, node_id: str):
        try:
            self._cursor.execute("INSERT OR IGNORE INTO nodes (id) VALUES (?)", (node_id,))
        except:
            pass

    def get_files_bulk(self, file_paths: List[str], repo_id: str = None) -> Dict[str, Dict[str, Any]]:
        if not file_paths:
            return {}
        unique_paths = list(set(file_paths))
        result = {}
        for i in range(0, len(unique_paths), 900):
            batch = unique_paths[i : i + 900]
            ph = ",".join(["?"] * len(batch))
            sql = f"SELECT path, repo_id, language, category FROM files WHERE path IN ({ph})"
            params = list(batch)
            if repo_id:
                sql += " AND repo_id = ?"
                params.append(repo_id)
            self._cursor.execute(sql, params)
            if self._cursor.description:
                cols = [d[0] for d in self._cursor.description]
                for row in self._cursor:
                    result[row[0]] = dict(zip(cols, row))
        return result

    def get_contents_bulk(self, chunk_hashes: List[str]) -> Dict[str, str]:
        if not chunk_hashes:
            return {}
        result = {}
        for i in range(0, len(chunk_hashes), 900):
            batch = chunk_hashes[i : i + 900]
            ph = ",".join(["?"] * len(batch))
            self._cursor.execute(f"SELECT chunk_hash, content FROM contents WHERE chunk_hash IN ({ph})", batch)
            for row in self._cursor:
                result[row[0]] = row[1]
        return result

    def get_incoming_definitions_bulk(self, node_ids: List[str]) -> Dict[str, List[str]]:
        if not node_ids:
            return {}
        result = {}
        for i in range(0, len(node_ids), 900):
            batch = node_ids[i : i + 900]
            ph = ",".join(["?"] * len(batch))
            self._cursor.execute(
                f"SELECT target_id, metadata_json FROM edges WHERE target_id IN ({ph}) AND relation_type='calls'", batch
            )
            for tid, meta_json in self._cursor:
                if not meta_json:
                    continue
                try:
                    sym = json.loads(meta_json).get("symbol")
                    if sym:
                        if tid not in result:
                            result[tid] = set()
                        result[tid].add(sym)
                except:
                    pass
        return {k: list(v) for k, v in result.items()}

    def get_context_neighbors(self, node_id: str) -> Dict[str, List[Dict[str, Any]]]:
        res = {"parents": [], "calls": []}
        self._cursor.execute(
            "SELECT t.id, t.file_path, t.start_line, e.metadata_json, t.metadata_json FROM edges e JOIN nodes t ON e.target_id = t.id WHERE e.source_id = ? AND e.relation_type = 'child_of'",
            (node_id,),
        )
        for row in self._cursor:
            res["parents"].append(
                {
                    "id": row[0],
                    "file_path": row[1],
                    "start_line": row[2],
                    "edge_meta": json.loads(row[3] or "{}"),
                    "metadata": json.loads(row[4] or "{}"),
                }
            )
        self._cursor.execute(
            "SELECT t.id, t.file_path, e.metadata_json FROM edges e JOIN nodes t ON e.target_id = t.id WHERE e.source_id = ? AND e.relation_type IN ('calls', 'references') LIMIT 15",
            (node_id,),
        )
        for row in self._cursor:
            m = json.loads(row[2] or "{}")
            res["calls"].append({"id": row[0], "symbol": m.get("symbol", "unknown")})
        return res

    def get_neighbor_chunk(self, node_id: str, direction: str = "next") -> Optional[Dict[str, Any]]:
        self._cursor.execute("SELECT file_id, start_line, end_line FROM nodes WHERE id=?", (node_id,))
        curr = self._cursor.fetchone()
        if not curr:
            return None
        fid, s, e = curr
        if direction == "next":
            sql = "SELECT n.id, n.start_line, n.end_line, n.chunk_hash, c.content, n.metadata_json FROM nodes n JOIN contents c ON n.chunk_hash=c.chunk_hash WHERE n.file_id=? AND n.start_line >= ? AND n.id!=? ORDER BY n.start_line ASC LIMIT 1"
            p = (fid, e, node_id)
        else:
            sql = "SELECT n.id, n.start_line, n.end_line, n.chunk_hash, c.content, n.metadata_json FROM nodes n JOIN contents c ON n.chunk_hash=c.chunk_hash WHERE n.file_id=? AND n.end_line <= ? AND n.id!=? ORDER BY n.end_line DESC LIMIT 1"
            p = (fid, s, node_id)
        self._cursor.execute(sql, p)
        row = self._cursor.fetchone()
        if row:
            return {
                "id": row[0],
                "start_line": row[1],
                "end_line": row[2],
                "chunk_hash": row[3],
                "content": row[4],
                "metadata": json.loads(row[5] or "{}"),
            }
        return None

    def get_incoming_references(self, target_node_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        self._cursor.execute(
            "SELECT s.id, s.file_path, s.start_line, e.relation_type, e.metadata_json FROM edges e JOIN nodes s ON e.source_id = s.id WHERE e.target_id = ? AND e.relation_type IN ('calls', 'references', 'imports', 'instantiates') ORDER BY s.file_path, s.start_line LIMIT ?",
            (target_node_id, limit),
        )
        results = []
        for row in self._cursor:
            m = json.loads(row[4] or "{}")
            results.append(
                {
                    "source_id": row[0],
                    "file": row[1],
                    "line": row[2],
                    "relation": row[3],
                    "context_snippet": m.get("description", ""),
                }
            )
        return results

    def get_outgoing_calls(self, source_node_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        self._cursor.execute(
            "SELECT t.id, t.file_path, t.start_line, e.relation_type, e.metadata_json FROM edges e JOIN nodes t ON e.target_id = t.id WHERE e.source_id = ? AND e.relation_type IN ('calls', 'instantiates', 'imports') ORDER BY t.file_path, t.start_line LIMIT ?",
            (source_node_id, limit),
        )
        results = []
        for row in self._cursor:
            m = json.loads(row[4] or "{}")
            results.append(
                {"target_id": row[0], "file": row[1], "line": row[2], "relation": row[3], "symbol": m.get("symbol", "")}
            )
        return results

    def get_stats(self):
        self._conn.commit()
        return {
            "files": self._cursor.execute("SELECT COUNT(*) FROM files").fetchone()[0],
            "total_nodes": self._cursor.execute("SELECT COUNT(*) FROM nodes").fetchone()[0],
            "embeddings": self._cursor.execute("SELECT COUNT(*) FROM node_embeddings").fetchone()[0],
            "repositories": self._cursor.execute("SELECT COUNT(*) FROM repositories").fetchone()[0],
        }

    def commit(self):
        self._conn.commit()

    def close(self):
        try:
            self._conn.close()
        except:
            pass

    def get_all_files(self):
        yield from []

    def get_all_nodes(self):
        yield from []

    def get_all_contents(self):
        yield from []

    def get_all_edges(self):
        yield from []

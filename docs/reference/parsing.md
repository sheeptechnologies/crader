# Parsing API

The parsing layer is responsible for transforming raw code into structured graph data. It uses two complementary engines: **Tree-sitter** for robust syntactic parsing and **SCIP** for precise semantic analysis.

::: src.code_graph_indexer.parsing.parser

## TreeSitterRepoParser

```python
class TreeSitterRepoParser
```

A high-performance parser that breaks down source files into "Chunks" (Functions, Classes).

### `stream_semantic_chunks`

```python
def stream_semantic_chunks(self, file_list: List[str] = None) -> Generator
```

The core pipeline entry point. It yields a stream of graph elements to keep memory usage constant (O(1)).

### Implementation Details

#### Zero-Copy Optimization
The parser avoids string copying whenever possible to handle large files efficiently.
*   **MemoryViews**: It uses Python's `memoryview` on the raw bytes of the file.
*   **ByteArrays**: It uses mutable `bytearray` buffers to accumulate "glue" code (comments, whitespace) without creating intermediate immutable string objects.

#### Recursive Chunking Algorithm
The `_process_scope` method implements a greedy, recursive strategy to balance context vs. token limits.

1.  **Accumulation**: It iterates over AST siblings, accumulating "Glue" (comments, imports).
2.  **Barrier Detection**: When it hits a container (e.g., `class`, `def`), it flushes the glue.
3.  **Size Check**:
    *   If `Node + Glue < MAX_CHUNK_SIZE` (800 lines/tokens): It creates a single chunk.
    *   If `Node > MAX_CHUNK_SIZE`: It enters **Breakdown Mode** (`_handle_large_node`).
        *   It extracts the **Header** (Signature + Decorators) as a standalone chunk.
        *   It recurses into the **Body** (Block), passing the Header ID as the `parent_chunk_id`.
        *   This ensures that a 2000-line class results in multiple chunks, but every method inside still links back to the class header.

#### Noise Filtering
The parser applies multiple layers of filtering to ensure graph quality:
1.  **Technical Noise** (`GLOBAL_IGNORE_DIRS`): Fast O(1) checks for `.git`, `node_modules` in the path.
2.  **Semantic Noise** (`SEMANTIC_NOISE_DIRS`): Heuristics to identify `test/`, `fixtures/`.
3.  **Generated Code Detection**: Scans the first 500 bytes for headers like "Auto-generated by".
4.  **Minification Detection**: Rejects files with line lengths > 1000 chars.

---

::: src.code_graph_indexer.graph.indexers.scip

## SCIPIndexer

A wrapper around the **Source Code Indexing Protocol (SCIP)** CLI tools (e.g., `scip-python`, `scip-typescript`). It extracts "deep" semantic relationships like `calls`, `inherits`, and `imports`.

### `stream_relations`

```python
def stream_relations(self, ...) -> Generator[CodeRelation]
```

Orchestrates the external indexing process.

### Internal Works: The Two-Pass Extraction

1.  **Pass 1: Definition Table Building**
    *   Iterates over the SCIP index.
    *   Extracts all `Definition` roles.
    *   Builds a temporary **SQLite DB** (`DiskSymbolTable`) mapping `symbol_name -> {file, line, col}`.
    *   *Why SQLite?* A large Monorepo can have millions of symbols. Keeping a Python Dict causes OOM.

2.  **Pass 2: Occurrence Resolution**
    *   Iterates again over the index.
    *   Finds `Reference` roles (calls).
    *   Queries SQLite to find the target definition.
    *   Yields a resolved `CodeRelation` (Edge).

{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Sheep Codebase Indexer","text":"<p>The Sheep Codebase Indexer is an enterprise-grade library designed to transform raw source code into a queryable, semantic Knowledge Graph (CPG - Code Property Graph). </p> <p>Unlike simple \"RAG over text\" solutions, this library understands the structure of code, enabling AI agents and developers to query dependencies, find usages, and navigate large monorepos with precision.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>High-Performance Parsing: Uses Tree-sitter for robust, zero-copy parsing of modern languages (Python, TypeScript, Go, Java, Rust).</li> <li>Code Property Graph (CPG): Builds a rich graph connecting Definitions, References, Calls, and Inheritance.</li> <li>Hybrid Search: Combines Vector Search (semantic understanding) with Keyword Search (exact matching) via Reciprocal Rank Fusion (RRF).</li> <li>Enterprise Storage: Built on PostgreSQL with <code>pgvector</code>, ensuring ACID compliance, scalability, and robust concurrency.</li> <li>Precision Retrieval: Implements a multi-stage retrieval pipeline (Resolution -&gt; Search -&gt; Expansion) to provide contextually relevant code snippets to LLMs.</li> <li>Scalable Architecture: Designed for distributed indexing with separation of concerns between API (Readers) and Workers (Writers).</li> </ul>"},{"location":"#why-use-this","title":"Why Use This?","text":"<p>Standard text embeddings fail on code because code is highly structured. A function named <code>process_data</code> means nothing without knowing where it is defined, who calls it, and what types it uses. </p> <p>Sheep Codebase Indexer solves this by: 1.  Parsing the code structure into a graph. 2.  Embedding semantically meaningful chunks (not just random lines). 3.  Linking chunks via graph edges (e.g., <code>calls</code>, <code>inherits_from</code>).</p>"},{"location":"#where-to-start","title":"Where to Start?","text":"<ul> <li>Installation: Set up the library and its dependencies.</li> <li>Quickstart: Index your first repository in 5 minutes.</li> <li>Architecture: Understand the Indexing Pipeline and Storage Schema.</li> <li>API Reference: Detailed documentation for developers.</li> </ul>"},{"location":"faqs/","title":"Frequently Asked Questions","text":""},{"location":"faqs/#general-concepts","title":"General Concepts","text":""},{"location":"faqs/#what-is-sheep-codebase-indexer","title":"What is <code>sheep-codebase-indexer</code>?","text":"<p>Think of it as \"Google Indexing for your Private Codebase\", but built specifically for AI Agents. It is an ingestion engine that transforms raw source code (from Git) into a structured Knowledge Graph stored in PostgreSQL. It captures not just the text of the code, but the relationships (who calls whom, where is this defined, class hierarchy).</p>"},{"location":"faqs/#how-is-this-different-from-standard-rag-langchainllamaindex","title":"How is this different from standard RAG (LangChain/LlamaIndex)?","text":"<p>Standard RAG treats code as plain text documents. It splits files into chunks and embeds them. *   Standard RAG: \"Find lines similar to 'login error'.\" -&gt; Returns random snippets containing \"login\". *   We: \"Find the definition of <code>login</code> and its top 3 callers.\" -&gt; Returns the exact function body and the graph edges connecting it to the controllers that use it. We provide Structure. You can easily wrap our <code>CodeRetriever</code> as a generic \"Tool\" within LangChain or LlamaIndex.</p>"},{"location":"faqs/#what-is-a-code-property-graph-cpg","title":"What is a \"Code Property Graph\" (CPG)?","text":"<p>It's a data structure that combines: 1.  Abstract Syntax Tree (AST): The grammar of the code (Functions, Classes). 2.  Dependency Graph: semantic links (Imports, Calls, Inheritance). 3.  Embeddings: Vector representation of the code's meaning. We store all of this in a unified schema so you can query: \"Give me the embedding for the Function defined at line 50 that calls <code>User.save()</code>\".</p>"},{"location":"faqs/#use-cases","title":"Use Cases","text":""},{"location":"faqs/#what-can-i-build-with-this","title":"What can I build with this?","text":"<ol> <li>Context-Aware Coding Agents: An agent that doesn't hallucinate libraries because it can see the actual method signatures in the project.</li> <li>Repository Q&amp;A: A chatbot that answers \"How does the billing system handle retries?\" by traversing the call graph of the retry middleware.</li> <li>Automated Refactoring: Identify all 50 files that import a deprecated module to plan a migration.</li> <li>Onboarding Assistants: Help new engineers navigate legacy codebases by explaining flows rather than just files.</li> </ol>"},{"location":"faqs/#is-it-suitable-for-production","title":"Is it suitable for Production?","text":"<p>Yes. The system uses an Eventual Consistency model with Snapshot Isolation. *   You can index a new commit in the background. *   Your users continue searching the \"live\" snapshot without interruption. *   Once indexing is done, you atomically \"swap\" to the new snapshot. This is the same architectural pattern used by heavy-duty search engines.</p>"},{"location":"faqs/#architecture-design","title":"Architecture &amp; Design","text":""},{"location":"faqs/#why-postgresql-instead-of-a-dedicated-vector-db","title":"Why PostgreSQL instead of a dedicated Vector DB?","text":"<p>We believe in keeping the stack simple. PostgreSQL 15+ with <code>pgvector</code> offers: 1.  Transactional Integrity (ACID): We ensure the graph edges and the embeddings are always in sync. 2.  Complex JOINs: usage requires joining relational data (graph edges) with vector similarity. Postgres does this natively. 3.  Operational Maturity: Most teams already run Postgres. No need to manage a new piece of infrastructure like Pinecone or Weaviate just for this.</p>"},{"location":"faqs/#why-do-you-use-both-tree-sitter-and-scip","title":"Why do you use both Tree-sitter and SCIP?","text":"<p>They solve different problems: *   Tree-sitter is our \"Parser\". It is fast, runs locally, and understands the syntax (Where does the function start/end?). *   SCIP (Source Code Indexing Protocol) is our \"Linker\". It understands semantics (This usage of <code>User</code> refers to <code>models.py</code>). By combining them, we get the speed of regex-free parsing with the precision of a compiler.</p>"},{"location":"faqs/#operations-troubleshooting","title":"Operations &amp; Troubleshooting","text":""},{"location":"faqs/#does-it-support-huge-monorepos","title":"Does it support huge Monorepos?","text":"<p>Yes. *   Filtering: You can ignore <code>node_modules</code>, <code>vendor</code>, or specific folders via <code>GLOBAL_IGNORE_DIRS</code>. *   Incremental Indexing: We track commit hashes per file. If a file hasn't changed between commits, we reuse its existing nodes and embeddings, saving 90% of embedding costs. *   Parallelism: Parsing and Graph construction happen in parallel worker processes.</p>"},{"location":"faqs/#can-i-use-local-llms-ollama-llamacpp","title":"Can I use local LLMs (Ollama / Llama.cpp)?","text":"<p>Yes. The <code>EmbeddingProvider</code> is an abstract base class. You can implement a subclass that calls your local inference server (e.g., using <code>langchain</code> or direct HTTP calls) and pass it to the indexer. We default to OpenAI (<code>text-embedding-3-small</code>) because it provides the best cost/performance ratio for code today.</p>"},{"location":"faqs/#snapshot-locked-error","title":"<code>Snapshot locked</code> error","text":"<p>If the indexer process is kill -9'd, it might leave a snapshot in <code>indexing</code> state. *   Solution: Run <code>indexer.index(force=True, force_new=True)</code> to ignore the lock and start a fresh snapshot. The orphan snapshot will be cleaned up by the auto-pruner eventually.</p>"},{"location":"faqs/#search-returns-irrelevant-results","title":"Search returns irrelevant results","text":"<ul> <li>Check Filters: Are you filtering by <code>language='python'</code> but searching a TypeScript repo?</li> <li>Check Embeddings: Did the embedding process finish? Run <code>indexer.get_stats()</code> to see if <code>embeddings</code> count matches <code>total_nodes</code>.</li> <li>Tweak Strategy: If looking for specific error codes (e.g., <code>ERR_505</code>), force <code>strategy=\"keyword\"</code>.</li> </ul>"},{"location":"contributing/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident.</p>"},{"location":"contributing/development-setup/","title":"Development Setup","text":"<p>This guide covers how to set up the development environment for contributing to <code>sheep-codebase-indexer</code>.</p>"},{"location":"contributing/development-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: Version 3.11 or higher.</li> <li>Docker: Required for running the PostgreSQL + pgvector integration tests.</li> <li>Node.js: Required if you plan to touch the <code>debugger/frontend</code>.</li> <li>SCIP Tools: To run full end-to-end indexing on local code.</li> </ul>"},{"location":"contributing/development-setup/#1-environment-setup","title":"1. Environment Setup","text":"<p>We recommend using <code>venv</code> or <code>poetry</code>.</p> <pre><code># Clone the repository\ngit clone https://github.com/filippodaminato/sheep-codebase-indexer.git\ncd sheep-codebase-indexer\n\n# Create virtual env\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dependencies (Editable mode + Dev tools)\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/development-setup/#2-infrastructure-postgres","title":"2. Infrastructure (Postgres)","text":"<p>Start the local database for testing.</p> <pre><code>docker-compose up -d db\n</code></pre> <p>This starts PostgreSQL on port <code>6432</code> (mapped) with <code>pgvector</code> enabled. Connection String: <code>postgresql://sheep_user:sheep_password@localhost:6432/sheep_index</code></p>"},{"location":"contributing/development-setup/#3-running-tests","title":"3. Running Tests","text":"<p>We use <code>pytest</code>.</p> <pre><code># Run all tests\npytest tests/\n\n# Run specific functional tests (slow, integration)\npytest tests_files/test_workflow.py\n</code></pre>"},{"location":"contributing/development-setup/#4-code-style","title":"4. Code Style","text":"<ul> <li>Linting: We use <code>ruff</code>.</li> <li>Formatting: We use <code>black</code>.</li> <li>Type Checking: We use <code>mypy</code>.</li> </ul> <pre><code># Run full check\nruff check .\nblack .\nmypy src/\n</code></pre>"},{"location":"examples/advanced_usage/","title":"Advanced Usage &amp; Recipes","text":"<p>This guide covers scenarios beyond the basic \"Quickstart\", tailored for enterprise deployments and power users.</p>"},{"location":"examples/advanced_usage/#1-indexing-strategy","title":"1. Indexing Strategy","text":""},{"location":"examples/advanced_usage/#monorepo-filtering","title":"Monorepo Filtering","text":"<p>For large repositories, you generally want to exclude build artifacts, docs, and test fixtures to keep the graph clean and performance high.</p> <p>The indexer respects <code>GLOBAL_IGNORE_DIRS</code> by default, but you can customize this by modifying <code>src/code_graph_indexer/parsing/parsing_filters.py</code> or by passing custom logic if you extend the class.</p> <p>Default Exclusions: *   Technical Noise: <code>node_modules</code>, <code>.git</code>, <code>dist</code>, <code>__pycache__</code> *   Semantic Noise: <code>fixtures</code>, <code>migrations</code>, <code>locales</code></p>"},{"location":"examples/advanced_usage/#branch-specific-indexing","title":"Branch-Specific Indexing","text":"<p>You can maintain separate indices for different branches (e.g., <code>main</code> vs <code>develop</code>).</p> <p><pre><code>from code_graph_indexer import CodebaseIndexer\n\n# index main (Production)\nindexer_main = CodebaseIndexer(\n    repo_url=\"git@github.com:org/repo.git\",\n    branch=\"main\",\n    db_url=\"postgresql://user:pass@localhost/dbname\"\n)\nindexer_main.index()\n\n# index feature-branch (Development)\nindexer_dev = CodebaseIndexer(\n    repo_url=\"git@github.com:org/repo.git\",\n    branch=\"feature/new-search-api\",\n    db_url=\"postgresql://user:pass@localhost/dbname\"\n)\nindexer_dev.index()\n</code></pre> Note: The <code>Repository</code> entity in the DB is unique per (url, branch) pair.</p>"},{"location":"examples/advanced_usage/#2-advanced-search-retrieval","title":"2. Advanced Search &amp; Retrieval","text":""},{"location":"examples/advanced_usage/#using-metadata-filters","title":"Using Metadata Filters","text":"<p>The <code>filters</code> argument in <code>retrieve()</code> allows you to slice the graph by any metadata field extracted during parsing. This is pushed down to the database (SQL <code>WHERE</code> clause) for maximum speed.</p> <p>Filter by Language: <pre><code>results = retriever.retrieve(\n    query=\"authentication middleware\",\n    repo_id=repo_id,\n    filters={\"language\": \"python\"} \n)\n</code></pre></p> <p>Filter by Semantic Role: Find only Class Definitions related to \"User\": <pre><code>results = retriever.retrieve(\n    query=\"User\",\n    repo_id=repo_id,\n    filters={\"role\": \"class\"} # derived either from 'type' or 'category' in metadata\n)\n</code></pre></p> <p>Filter by File Path: Scope search to a specific module: <pre><code>results = retriever.retrieve(\n    query=\"calculate_tax\",\n    repo_id=repo_id,\n    filters={\"start_path\": \"src/billing/\"} # Implementation depends on storage backend\n)\n</code></pre></p>"},{"location":"examples/advanced_usage/#hybrid-search-tuning","title":"Hybrid Search Tuning","text":"<p>By default, <code>CodeRetriever</code> uses Reciprocal Rank Fusion (RRF) to combine Vector and Keyword results. You can force a specific strategy if you know what you are doing.</p> <ul> <li><code>strategy='vector'</code>: Best for concept search (\"how do I login?\").</li> <li><code>strategy='keyword'</code>: Best for exact error codes (\"Error 503\") or specific symbol names (\"UserFactory\").</li> <li><code>strategy='hybrid'</code>: (Default) Best of both worlds.</li> </ul> <pre><code># Force exact keyword match for an error code\nhits = retriever.retrieve(\n    query=\"ERR_CONNECTION_RESET\",\n    repo_id=repo_id,\n    strategy=\"keyword\"\n)\n</code></pre>"},{"location":"examples/advanced_usage/#3-custom-embedding-models","title":"3. Custom Embedding Models","text":"<p>The system uses the <code>EmbeddingProvider</code> interface. You can swap OpenAI with any other provider (Vertex AI, HuggingFace, Bedrock) by implementing a simple adapter.</p> <pre><code>from code_graph_indexer.providers.embedding import EmbeddingProvider\n\nclass VertexEmbeddingProvider(EmbeddingProvider):\n    def __init__(self, project_id, model=\"text-embedding-gecko\"):\n        self.client = ... # Initialize Vertex AI client\n\n    async def embed_async(self, texts: List[str]) -&gt; List[List[float]]:\n        # Implement call to Google Cloud\n        return await self.client.get_embeddings(texts)\n\n# Usage\nindexer = CodebaseIndexer(..., embedding_provider=VertexEmbeddingProvider(\"my-project\"))\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers the prerequisites and steps to install <code>sheep-codebase-indexer</code> in your environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the python library, ensure you have the following system components:</p> <ol> <li>Python 3.10+: The codebase uses modern Python features like <code>asyncio</code> and type hinting.</li> <li>PostgreSQL 15+: Required for robust data storage.</li> <li>pgvector: The PostgreSQL extension for vector similarity search.</li> <li>Git: Required for cloning and managing repositories.</li> <li>SCIP CLI (Required): For advanced semantic indexing (LSIF).<ul> <li>Install via npm: <code>npm install -g @sourcegraph/scip-typescript @sourcegraph/scip-python</code> (etc.)</li> </ul> </li> </ol>"},{"location":"getting-started/installation/#installation_1","title":"Installation","text":""},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>The recommended way to install is directly from the source code, as it is an internal library.</p> <pre><code>git clone https://github.com/your-org/sheep-codebase-indexer.git\ncd sheep-codebase-indexer\npip install .\n</code></pre>"},{"location":"getting-started/installation/#with-development-dependencies","title":"With Development Dependencies","text":"<p>If you plan to contribute or run tests:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>The library uses Environment Variables for configuration. You can set these in your shell or use a <code>.env</code> file.</p> Variable Description Default Required <code>DATABASE_URL</code> PostgreSQL Connection String. <code>postgresql://user:pass@localhost:5432/sheep</code> Yes <code>OPENAI_API_KEY</code> Key for generating embeddings (OpenAIProvider). None Yes <code>REPO_VOLUME</code> Local directory where repos are cloned/cached. <code>/var/tmp/sheep_volume</code> No <code>LOG_LEVEL</code> Python logging level (DEBUG, INFO). <code>INFO</code> No"},{"location":"getting-started/installation/#setting-up-postgresql-with-pgvector","title":"Setting up PostgreSQL with pgvector","text":"<p>Ensure your database has the <code>vector</code> extension enabled:</p> <pre><code>CREATE EXTENSION IF NOT EXISTS vector;\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will help you index a repository and perform your first semantic search in under 10 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker (for PostgreSQL)</li> <li>Python 3.11+</li> <li>OpenAI API Key (or another embedding provider)</li> </ul>"},{"location":"getting-started/quickstart/#1-start-infrastructure","title":"1. Start Infrastructure","text":"<p>Use the provided <code>docker-compose.yml</code> to spin up PostgreSQL with <code>pgvector</code>:</p> <pre><code>docker-compose up -d db\n</code></pre>"},{"location":"getting-started/quickstart/#2-install-library","title":"2. Install Library","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/quickstart/#3-creating-the-index-python-script","title":"3. Creating the Index (Python Script)","text":"<p>Create a file named <code>index_repo.py</code>. We will use the <code>CodebaseIndexer</code> to clone, parse, and embed the <code>flask</code> repository.</p> <pre><code>import os\nimport asyncio\nfrom code_graph_indexer.indexer import CodebaseIndexer\nfrom code_graph_indexer.providers.embedding import OpenAIEmbeddingProvider\n\n# 1. Configuration\nDB_URL = \"postgresql://sheep_user:sheep_password@localhost:6432/sheep_index\"\nREPO_URL = \"https://github.com/pallets/flask.git\"\nBRANCH = \"main\"\n\nasync def main():\n    # 2. Init Indexer\n    # The indexer manages cloning and DB connection pools automatically.\n    indexer = CodebaseIndexer(REPO_URL, BRANCH, db_url=DB_URL)\n\n    # 3. Init Provider (Async)\n    # Use a cost-effective model like text-embedding-3-small\n    provider = OpenAIEmbeddingProvider(\n        model=\"text-embedding-3-small\", \n        max_concurrency=10\n    )\n\n    try:\n        # 4. Phase 1: Indexing (Parsing &amp; Graph Construction)\n        print(\"\ud83d\ude80 Starting Indexing (Parsing, SCIP)...\")\n        snapshot_id = indexer.index(force=False)\n        print(f\"\u2705 Active Snapshot: {snapshot_id}\")\n\n        # 5. Phase 2: Embedding (Async Pipeline)\n        print(\"\ud83d\udcb8 Starting Embedding pipeline...\")\n        async for update in indexer.embed(provider, batch_size=200):\n            status = update['status']\n            if status == 'embedding_progress':\n                print(f\"   \u2728 Processing... {update.get('total_embedded')} vectors\", end='\\r')\n            elif status == 'completed':\n                print(f\"\\n\u2705 Done! New vectors: {update.get('newly_embedded')}\")\n\n    finally:\n        indexer.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run it: <pre><code>export OPENAI_API_KEY=\"sk-...\"\npython index_repo.py\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-semantic-search","title":"4. Semantic Search","text":"<p>Now that the data is indexed, let's search it. Create <code>search.py</code>:</p> <pre><code>import asyncio\nfrom code_graph_indexer.indexer import CodebaseIndexer\nfrom code_graph_indexer.retriever import CodeRetriever\nfrom code_graph_indexer.providers.embedding import OpenAIEmbeddingProvider\n\nDB_URL = \"postgresql://sheep_user:sheep_password@localhost:6432/sheep_index\"\nREPO_URL = \"https://github.com/pallets/flask.git\"\n\nasync def main():\n    # We reuse the indexer to get access to storage\n    indexer = CodebaseIndexer(REPO_URL, \"main\", db_url=DB_URL)\n    provider = OpenAIEmbeddingProvider(model=\"text-embedding-3-small\")\n\n    # Init Retriever\n    retriever = CodeRetriever(indexer.storage, provider)\n\n    # Resolve Repository ID\n    repo_id = indexer.storage.get_repository(indexer.storage.ensure_repository(REPO_URL, \"main\", \"flask\"))['id']\n\n    query = \"How does request routing work?\"\n    print(f\"Searching for: '{query}'...\")\n\n    results = retriever.retrieve(\n        query, \n        repo_id=repo_id, \n        limit=3, \n        strategy=\"hybrid\"\n    )\n\n    for r in results:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"File: {r.file_path} (Line {r.start_line})\")\n        print(f\"Score: {r.score:.4f}\")\n        print(f\"Labels: {r.semantic_labels}\")\n        print(\"-\" * 50)\n        print(r.content[:200] + \"...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/architecture/","title":"System Architecture","text":"<p>This document describes the high-level architecture of the <code>sheep-codebase-indexer</code>. The system is designed as a Code Property Graph (CPG) Builder that ingests source code repositories and transforms them into a queriable knowledge graph.</p>"},{"location":"guides/architecture/#high-level-overview","title":"High-Level Overview","text":"<p>The system operates in two main phases: Indexing (Write Path) and Retrieval (Read Path).</p>"},{"location":"guides/architecture/#write-path-from-git-to-graph","title":"Write Path: From Git to Graph","text":"<pre><code>graph TD\n    A[Git Repository] --&gt;|Clone| B(GitVolumeManager)\n    B --&gt;|Files| C{ParserOrchestrator}\n\n    subgraph \"Parallel Processing\"\n        direction TB\n        C --&gt;|Syntactic Analysis| D[TreeSitterRepoParser]\n        C --&gt;|Semantic Analysis| E[SCIPIndexer]\n    end\n\n    D --&gt;|Chunks| F[CodeEmbedder]\n    F --&gt;|Embeddings| G[PostgresGraphStorage]\n    E --&gt;|Edges| G</code></pre>"},{"location":"guides/architecture/#read-path-from-query-to-context","title":"Read Path: From Query to Context","text":"<pre><code>graph TD\n    H[User Query] --&gt;|Input| I[CodeRetriever]\n    I --&gt;|Vector/Keyword Search| J[SearchExecutor]\n\n    subgraph \"Enrichment\"\n        direction TB\n        J --&gt;|Node IDs| K[GraphWalker]\n        K --&gt;|Traverse Graph| L[Augmented Context]\n    end</code></pre>"},{"location":"guides/architecture/#indexing-pipeline","title":"Indexing Pipeline","text":"<p>The indexing process is orchestrated by <code>CodebaseIndexer</code>. It follows a Snapshot-based consistency model: every indexing run creates an immutable <code>Snapshot</code> of the repository.</p>"},{"location":"guides/architecture/#1-ingestion-gitvolumemanager","title":"1. Ingestion (<code>GitVolumeManager</code>)","text":"<ul> <li>Role: Manages the cloning and updating of Git repositories.</li> <li>Mechanism: Uses <code>git</code> CLI (via <code>GitPython</code>) to fetch data.</li> <li>Optimization: Maintains a persistent \"Bare Repository\" cache and creates lightweight <code>worktrees</code> for specific commits. This avoids re-downloading the entire history for every analysis.</li> </ul>"},{"location":"guides/architecture/#2-syntactic-parsing-treesitterrepoparser","title":"2. Syntactic Parsing (<code>TreeSitterRepoParser</code>)","text":"<ul> <li>Role: Breaks down source files into \"Chunks\" (Functions, Classes, Methods).</li> <li>Technology: Uses <code>tree-sitter</code>, a high-performance incremental parser.</li> <li>Logic:<ul> <li>Iterates over all files.</li> <li>Applies language-specific S-expression queries (<code>*.scm</code>) to extract definitions.</li> <li>Splits large files using a recursive \"scope-aware\" chunking strategy to preserve context.</li> </ul> </li> </ul>"},{"location":"guides/architecture/#3-semantic-analysis-scipindexer","title":"3. Semantic Analysis (<code>SCIPIndexer</code>)","text":"<ul> <li>Role: Extracts cross-file relationships (e.g., <code>calls</code>, <code>inherits</code>, <code>imports</code>).</li> <li>Technology: Wraps SCIP (Source Code Indexing Protocol) CLIs (e.g., <code>scip-python</code>, <code>scip-typescript</code>).</li> <li>Process:<ul> <li>Runs the language-specific indexer (often requiring a build environment).</li> <li>Streams the resulting Protobuf index.</li> <li>Resolves \"References\" to \"Definitions\" to create graph edges.</li> </ul> </li> </ul>"},{"location":"guides/architecture/#4-embedding-generation-codeembedder","title":"4. Embedding Generation (<code>CodeEmbedder</code>)","text":"<ul> <li>Role: Converts text code chunks into dense vectors.</li> <li>Pipeline:<ol> <li>Staging: Chunks are written to a temporary table.</li> <li>Deduplication: The system computes a hash of the content. If the same code block (same hash) was already embedded in a previous snapshot, the implementation reuses the existing vector.</li> <li>Delta Computing: Only new/changed chunks are sent to the LLM API.</li> <li>Batching: Requests are batched to respect API rate limits.</li> </ol> </li> </ul>"},{"location":"guides/architecture/#storage-layer-postgresgraphstorage","title":"Storage Layer (<code>PostgresGraphStorage</code>)","text":"<p>The persistence layer is valid PostgreSQL 15+ with the <code>pgvector</code> extension. It handles the Code Property Graph (CPG) schema, ensuring data integrity and high performance for both inserts and lookups.</p> <p>Key Responsibilities: *   Snapshot Isolation: Guarantees consistent reads during indexing updates (MVCC). *   Bulk Ingestion: Uses <code>COPY</code> protocol for millions of rows/sec. *   Hybrid Indexing: Manages IVFFlat/HNSW indexes for vectors and GIN indexes for keywords.</p> <p>Full Specification</p> <p>For a complete, exhaustive description of the Database Schema, Table structures, and Method behaviors, please consult the Storage API Reference.</p>"},{"location":"guides/architecture/#retrieval-architecture","title":"Retrieval Architecture","text":"<p>The <code>CodeRetriever</code> implements a \"Retrieval-Augmented Generation\" (RAG) specialized for code.</p>"},{"location":"guides/architecture/#1-hybrid-search","title":"1. Hybrid Search","text":"<p>Combines two search strategies:</p> <ul> <li>Dense Retrieval (Vector): Finds conceptually similar code (e.g. \"auth logic\" -&gt; <code>login()</code>).</li> <li>Sparse Retrieval (Keyword): Finds exact matches (e.g. <code>UserFactory</code>, <code>API_KEY</code>).</li> </ul>"},{"location":"guides/architecture/#2-result-fusion","title":"2. Result Fusion","text":"<p>Uses Reciprocal Rank Fusion (RRF) to combine the ranked lists from Vector and Keyword search into a single, high-quality result set.</p>"},{"location":"guides/architecture/#3-graph-expansion-graphwalker","title":"3. Graph Expansion (<code>GraphWalker</code>)","text":"<p>Once relevant \"Seed Nodes\" are found, the <code>GraphWalker</code> traverses the graph edges to fetch context that wasn't in the search results:</p> <ul> <li>Vertical Expansion: \"Who defines this function?\" (Parent Class).</li> <li>Horizontal Expansion: \"What does this function call?\" (Dependencies).</li> </ul> <p>This ensures the LLM receives a complete subgraph rather than a disconnected snippet.</p>"},{"location":"guides/data_model/","title":"Data Model","text":"<p>This document details the core entities and relationships in the <code>sheep-codebase-indexer</code>.</p>"},{"location":"guides/data_model/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":"<pre><code>erDiagram\n    REPOSITORY ||--o{ SNAPSHOT : has\n    SNAPSHOT ||--o{ FILE_RECORD : contains\n    FILE_RECORD ||--o{ CHUNK_NODE : defines\n    CHUNK_NODE ||--o{ CODE_RELATION : source\n    CHUNK_NODE ||--o{ CODE_RELATION : target\n\n    REPOSITORY {\n        string id PK\n        string url\n        string current_snapshot_id FK\n    }\n\n    SNAPSHOT {\n        string id PK\n        string commit_hash\n        string status\n    }\n\n    CHUNK_NODE {\n        string id PK\n        string chunk_hash\n        vector embedding\n        jsonb metadata\n    }\n\n    CODE_RELATION {\n        string source_id FK\n        string target_id FK\n        string type\n    }</code></pre>"},{"location":"guides/data_model/#core-entities","title":"Core Entities","text":""},{"location":"guides/data_model/#1-repository-modelsrepository","title":"1. Repository (<code>models.Repository</code>)","text":"<p>Represents a stable project identity. *   id: Unique hash of the remote URL. *   current_snapshot_id: Pointer to the currently active version of the code.</p>"},{"location":"guides/data_model/#2-snapshot-modelssnapshot","title":"2. Snapshot (<code>models.Snapshot</code>)","text":"<p>Represents an immutable version of the code at a specific commit. *   Status Lifecycle: <code>pending</code> -&gt; <code>indexing</code> -&gt; <code>completed</code> (or <code>failed</code>). *   Isolation: Allows indexing a new commit in the background while serving queries from the old commit.</p>"},{"location":"guides/data_model/#3-filerecord-modelsfilerecord","title":"3. FileRecord (<code>models.FileRecord</code>)","text":"<p>Represents a physical file on disk. *   file_hash: Content hash (SHA256) used for change detection. *   parsing_status: Tracks if <code>tree-sitter</code> successfully parsed the file.</p>"},{"location":"guides/data_model/#4-chunknode-modelschunknode","title":"4. ChunkNode (<code>models.ChunkNode</code>)","text":"<p>The fundamental unit of the graph (a \"Node\"). *   Granularity: Can be a Function, Class, Method, or a standalone specific block of code. *   Vector: Stores the 1536-dim embedding (if using OpenAI). *   Metadata: JSONB field containing \"semantic captures\" (e.g., <code>role: entry_point</code>, <code>access: public</code>).</p>"},{"location":"guides/data_model/#5-coderelation-modelscoderelation","title":"5. CodeRelation (<code>models.CodeRelation</code>)","text":"<p>A directed edge between two ChunkNodes. *   Types:     *   <code>child_of</code>: Structural containment (Method -&gt; Class).     *   <code>calls</code>: Function A invokes Function B.     *   <code>inherits</code>: Class A extends Class B.     *   <code>imports</code>: File A requires Module B.</p>"},{"location":"reference/embedding/","title":"Embedding API","text":"<p>The embedding module manages the conversion of code chunks into high-dimensional vector representations. It is designed for massive throughput and cost efficiency.</p> <p>::: src.code_graph_indexer.embedding.embedder</p>"},{"location":"reference/embedding/#codeembedder","title":"CodeEmbedder","text":"<pre><code>class CodeEmbedder\n</code></pre> <p>The asynchronous engine responsible for vectorization.</p>"},{"location":"reference/embedding/#run_indexing","title":"<code>run_indexing</code>","text":"<pre><code>async def run_indexing(self, snapshot_id: str, ...) -&gt; AsyncGenerator\n</code></pre> <p>Executes the embedding pipeline.</p>"},{"location":"reference/embedding/#the-staging-pipeline","title":"The Staging Pipeline","text":"<p>The embedder implements a customized ETL (Extract, Transform, Load) process to minimize calls to the expensive Embedding API (OpenAI/Vertex).</p>"},{"location":"reference/embedding/#1-transform-hash-cpu-parallel","title":"1. Transform &amp; Hash (CPU Parallel)","text":"<p>Before touching the database, the system uses a <code>ProcessPoolExecutor</code> to prepare data in parallel. For each chunk, it computes a Semantic Fingerprint (<code>v_hash</code>). *   Formula: <code>SHA256(Filepath + Struct + Content + OutgoingDefs + IncomingDefs)</code> *   This hash represents the exact semantic state of the code.</p>"},{"location":"reference/embedding/#2-staging-load-bulk-io","title":"2. Staging Load (Bulk I/O)","text":"<p>The prepared data is loaded using <code>COPY</code> into a temporary table: <pre><code>CREATE UNLOGGED TABLE temp_embedding_staging ...\n</code></pre> Unlogged tables are faster as they bypass the WAL (Write Ahead Log).</p>"},{"location":"reference/embedding/#3-deduplication-sql-set-logic","title":"3. Deduplication (SQL Set Logic)","text":"<p>This is the core cost-saving mechanism. <pre><code>-- \"Backfill\": Find existing vectors for identical code\nUPDATE temp_embedding_staging t\nSET embedding = e.embedding, \n    is_cached = true\nFROM node_embeddings e\nWHERE e.vector_hash = t.vector_hash;\n</code></pre> If you move a file, or rename a folder, the <code>vector_hash</code> might change (due to path), but if the content is stable, we could potentially relax the hash strictness. Currently, it is strict.</p>"},{"location":"reference/embedding/#4-delta-processing-async-workers","title":"4. Delta Processing (Async Workers)","text":"<p>Only rows where <code>embedding IS NULL</code> are fetched. *   Producer: Pushes batches to an <code>asyncio.Queue</code>. *   Consumers: A pool of workers (size = <code>max_concurrency</code>) pull batches, call the API, and write results.</p>"},{"location":"reference/embedding/#_compute_prompt_and_hash","title":"<code>_compute_prompt_and_hash</code>","text":"<pre><code>def _compute_prompt_and_hash(node: Dict) -&gt; Tuple[str, str]\n</code></pre> <p>Constructs the \"Context Window\" for the LLM. It's not just the code!</p> <p>Prompt Structure: 1.  Header: File Path, Language, Category. 2.  Semantic Tags: Roles (e.g., <code>Role: API Endpoint</code>), Modifiers (e.g., <code>Tags: async, static</code>). 3.  Graph Context: Incoming Definitions (e.g., <code>Defines: UserFactory, AuthMiddleware</code>). 4.  Body: The actual source code.</p> <p>This rich context ensures that even small chunks (like a 5-line function) have enough semantic meaning for high-quality retrieval.</p>"},{"location":"reference/indexer/","title":"Indexer API Reference","text":"<p>The <code>indexer</code> module is the command center of the library. It acts as the coordinator between the Git filesystem, the Parsing workers, the Embedding provider, and the Storage layer.</p> <p>::: src.code_graph_indexer.indexer</p>"},{"location":"reference/indexer/#codebaseindexer","title":"CodebaseIndexer","text":"<pre><code>class CodebaseIndexer(repo_url: str, branch: str, db_url: Optional[str] = None, worker_telemetry_init: Optional[Callable] = None)\n</code></pre> <p>The persistent controller for a specific repository.</p>"},{"location":"reference/indexer/#constructor-arguments","title":"Constructor Arguments","text":"<ul> <li> <p><code>repo_url</code> (str):     The complete Git remote URL. Supports SSH (<code>git@...</code>) and HTTPS (<code>https://...</code>). This acts as part of the unique key for the repository.</p> </li> <li> <p><code>branch</code> (str):     The target branch to index (e.g., <code>main</code>, <code>master</code>, <code>develop</code>).</p> </li> <li> <p><code>db_url</code> (Optional[str]):     A standard PostgreSQL connection string (<code>postgresql://server:port/db</code>). If not provided, it looks for <code>DATABASE_URL</code> in environment variables.</p> </li> <li> <p><code>worker_telemetry_init</code> (Optional[Callable]):     A hook function called at the start of every worker process. Useful for initializing tools like Sentry, OpenTelemetry, or custom logging config in parallel processes.</p> </li> </ul>"},{"location":"reference/indexer/#methods","title":"Methods","text":""},{"location":"reference/indexer/#index","title":"<code>index</code>","text":"<pre><code>def index(self, force: bool = False, auto_prune: bool = False) -&gt; str\n</code></pre> <p>Description: Triggers the \"Structure Analysis\" phase. This involves cloning, parsing, and graph construction. This is a synchronous, blocking operation (though it uses internal parallelism).</p> <p>Arguments: *   <code>force</code> (bool): If <code>True</code>, ignores the \"Stale Check\". Even if the latest commit is already indexed, it will create a new Snapshot and re-parse everything. Useful for development or fixing corrupted indices. *   <code>auto_prune</code> (bool): If <code>True</code>, automatically sets old snapshots to <code>archived</code> or deletes them after successful indexing.</p> <p>Returns: *   <code>snapshot_id</code> (str): The UUID of the newly created (or existing reused) snapshot.</p> <p>Raises: *   <code>GitCommandError</code>: If cloning or fetching fails (e.g. auth error, network down). *   <code>DatabaseError</code>: If connection fails.</p>"},{"location":"reference/indexer/#embed","title":"<code>embed</code>","text":"<pre><code>async def embed(self, provider: EmbeddingProvider, batch_size: int = 1000, mock_api: bool = False, force_snapshot_id: str = None) -&gt; AsyncGenerator[Dict[str, Any], None]\n</code></pre> <p>Description: Triggers the \"Semantic Analysis\" phase. This is an Asynchronous Generator. You must iterate over it to drive the process forward. It is decoupled from <code>index()</code> to allow for different scheduling (e.g., Index now, Embed tonight).</p> <p>Arguments: *   <code>provider</code> (EmbeddingProvider): An instance of a provider wrapper (e.g. <code>OpenAIEmbeddingProvider</code>). *   <code>batch_size</code> (int): Number of vectors to flush to DB in one transaction. Default 1000. *   <code>mock_api</code> (bool): If <code>True</code>, generates random vectors. STRICTLY FOR TESTING. *   <code>force_snapshot_id</code> (str): target a specific historical snapshot instead of the current one.</p> <p>Yields: A stream of status dictionaries. *   <code>{'status': 'staging_progress', 'total': 100, 'staged': 50}</code> *   <code>{'status': 'embedding_progress', 'total_to_embed': 500, 'total_embedded': 100}</code> *   <code>{'status': 'completed', 'newly_embedded': 450, 'recovered_from_history': 50}</code></p>"},{"location":"reference/indexer/#_init_worker_process-internal","title":"<code>_init_worker_process</code> (Internal)","text":"<pre><code>def _init_worker_process(worktree_path: str, db_url: str)\n</code></pre> <p>Global State Initialization: This static method is the entry point for every <code>multiprocessing.Process</code> spawned by the indexer. 1.  disables Signals to avoid <code>KeyboardInterrupt</code> corruption. 2.  Initializes a thread-local <code>TreeSitterRepoParser</code>. 3.  Opens a new DB connection (as connections strictly cannot be shared across forks).</p>"},{"location":"reference/indexer/#utility-pruning","title":"Utility: Pruning","text":""},{"location":"reference/indexer/#prune_snapshots","title":"<code>prune_snapshots</code>","text":"<pre><code>def prune_snapshots(self, keep: int = 3)\n</code></pre> <p>Maintenance method to keep the database size in check. *   Keeps the last <code>keep</code> (default 3) snapshots for this repository. *   Hard deletes older snapshots and cascades deletes to <code>nodes</code>, <code>edges</code>, and <code>embeddings</code>. *   Warning: This is destructive and irreversible.</p>"},{"location":"reference/models/","title":"Data Models","text":"<p>The <code>models</code> module defines the data transfer objects (DTOs) used throughout the system. These reflect the database schema.</p> <p>::: src.code_graph_indexer.models</p>"},{"location":"reference/models/#core-entities","title":"Core Entities","text":""},{"location":"reference/models/#repository","title":"<code>Repository</code>","text":"<pre><code>@dataclass\nclass Repository\n</code></pre> <p>The persistent identity of a project. *   current_snapshot_id: Pointer to the \"LIVE\" version. *   reindex_requested_at: Flag used for distributed locking/coordination.</p>"},{"location":"reference/models/#snapshot","title":"<code>Snapshot</code>","text":"<pre><code>@dataclass\nclass Snapshot\n</code></pre> <p>An immutable point-in-time capture of the repository. *   status: <code>pending</code> -&gt; <code>indexing</code> -&gt; <code>completed</code>. *   file_manifest: A JSON tree structure caching the file list for O(1) directory navigation.</p>"},{"location":"reference/models/#chunknode","title":"<code>ChunkNode</code>","text":"<pre><code>@dataclass\nclass ChunkNode\n</code></pre> <p>The atom of the Knowledge Graph. *   chunk_hash: SHA-256 of the content (used for deduplication). *   byte_range: <code>[start_byte, end_byte]</code> for precise slicing. *   metadata: Flexible JSON store for semantic tags (<code>role</code>, <code>category</code>).</p>"},{"location":"reference/models/#coderelation","title":"<code>CodeRelation</code>","text":"<pre><code>@dataclass\nclass CodeRelation\n</code></pre> <p>A directed edge between nodes (or files). *   relation_type:     *   <code>child_of</code>: Structural hierarchy.     *   <code>calls</code> / <code>references</code>: Usage.     *   <code>inherits</code>: OOP Inheritance.     *   <code>imports</code>: Module Dependency.</p>"},{"location":"reference/models/#retrieval-objects","title":"Retrieval Objects","text":""},{"location":"reference/models/#retrievedcontext","title":"<code>RetrievedContext</code>","text":"<pre><code>@dataclass\nclass RetrievedContext\n</code></pre> <p>The rich object returned to the client/agent after a search. *   nav_hints: Pre-calculated IDs for \"Next Chunk\", \"Previous Chunk\", and \"Parent Container\" to enable UI navigation without extra queries. *   outgoing_definitions: List of symbols called by this chunk (e.g. <code>User.save()</code>) to provide immediate context to an LLM.</p>"},{"location":"reference/models/#render","title":"<code>render</code>","text":"<pre><code>def render(self) -&gt; str\n</code></pre> <p>Formats the context into a Markdown-friendly string optimized for LLM consumption. Includes specific headers <code>[CONTEXT]</code>, <code>[CODE]</code>, <code>[RELATIONS]</code> to help the model parse the input.</p>"},{"location":"reference/navigation/","title":"Navigation API Reference","text":"<p>The <code>navigator</code> module enables Structural Exploration of the code. While <code>Retriever</code> finds where things are, <code>Navigator</code> explains how they relate.</p>"},{"location":"reference/navigation/#codenavigator","title":"CodeNavigator","text":"<pre><code>class CodeNavigator(storage: GraphStorage)\n</code></pre> <p>Offers an IDE-like traversal API (\"Go to Definition\", \"Find Usages\").</p>"},{"location":"reference/navigation/#methods","title":"Methods","text":""},{"location":"reference/navigation/#read_neighbor_chunk","title":"<code>read_neighbor_chunk</code>","text":"<pre><code>def read_neighbor_chunk(self, node_id: str, direction: str = \"next\") -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Description: Simulates \"Scrolling\". Gets the code block immediately preceding or following the current one in the file.</p> <ul> <li>Use Case: An Agent is reading a function <code>foo()</code> and sees a call to <code>_helper()</code>. Use <code>next</code> to see if <code>_helper()</code> is defined right below.</li> </ul> <p>Arguments:</p> <ul> <li><code>direction</code> (str): <code>\"next\"</code> (down) or <code>\"prev\"</code> (up).</li> </ul>"},{"location":"reference/navigation/#read_parent_chunk","title":"<code>read_parent_chunk</code>","text":"<pre><code>def read_parent_chunk(self, node_id: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Description: Jumps up the scope hierarchy. *   <code>Method</code> -&gt; <code>Class</code> *   <code>Class</code> -&gt; <code>Module</code> (File)</p>"},{"location":"reference/navigation/#analyze_impact-incoming","title":"<code>analyze_impact</code> (Incoming)","text":"<pre><code>def analyze_impact(self, node_id: str, limit: int = 20) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: \"Who calls this?\" (Reverse Call Graph). Identifies all functions or classes that depend on <code>node_id</code>. Critical for refactoring agents to know what might break.</p> <p>Returns: <pre><code>[\n    {\n        \"file\": \"src/controllers.py\",\n        \"line\": 45,\n        \"relation\": \"calls\",\n        \"context_snippet\": \"user.login()\"\n    }\n]\n</code></pre></p>"},{"location":"reference/navigation/#analyze_dependencies-outgoing","title":"<code>analyze_dependencies</code> (Outgoing)","text":"<pre><code>def analyze_dependencies(self, node_id: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: \"What does this call?\" (Forward Call Graph). Lists all external symbols used by <code>node_id</code>.</p>"},{"location":"reference/navigation/#visualize_pipeline","title":"<code>visualize_pipeline</code>","text":"<pre><code>def visualize_pipeline(self, node_id: str, max_depth: int = 2) -&gt; Dict[str, Any]\n</code></pre> <p>Description: Constructs a recursive tree of the call graph starting from <code>node_id</code>. *   Use Case: Generating a UI visualization (Node-Link diagram) for the user to understand complex logic flows.</p>"},{"location":"reference/parsing/","title":"Parsing API","text":"<p>The parsing layer is responsible for transforming raw code into structured graph data. It uses two complementary engines: Tree-sitter for robust syntactic parsing and SCIP for precise semantic analysis.</p> <p>::: src.code_graph_indexer.parsing.parser</p>"},{"location":"reference/parsing/#treesitterrepoparser","title":"TreeSitterRepoParser","text":"<pre><code>class TreeSitterRepoParser\n</code></pre> <p>A high-performance parser that breaks down source files into \"Chunks\" (Functions, Classes).</p>"},{"location":"reference/parsing/#stream_semantic_chunks","title":"<code>stream_semantic_chunks</code>","text":"<pre><code>def stream_semantic_chunks(self, file_list: List[str] = None) -&gt; Generator\n</code></pre> <p>The core pipeline entry point. It yields a stream of graph elements to keep memory usage constant (O(1)).</p>"},{"location":"reference/parsing/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/parsing/#zero-copy-optimization","title":"Zero-Copy Optimization","text":"<p>The parser avoids string copying whenever possible to handle large files efficiently. *   MemoryViews: It uses Python's <code>memoryview</code> on the raw bytes of the file. *   ByteArrays: It uses mutable <code>bytearray</code> buffers to accumulate \"glue\" code (comments, whitespace) without creating intermediate immutable string objects.</p>"},{"location":"reference/parsing/#recursive-chunking-algorithm","title":"Recursive Chunking Algorithm","text":"<p>The <code>_process_scope</code> method implements a greedy, recursive strategy to balance context vs. token limits.</p> <ol> <li>Accumulation: It iterates over AST siblings, accumulating \"Glue\" (comments, imports).</li> <li>Barrier Detection: When it hits a container (e.g., <code>class</code>, <code>def</code>), it flushes the glue.</li> <li>Size Check:<ul> <li>If <code>Node + Glue &lt; MAX_CHUNK_SIZE</code> (800 lines/tokens): It creates a single chunk.</li> <li>If <code>Node &gt; MAX_CHUNK_SIZE</code>: It enters Breakdown Mode (<code>_handle_large_node</code>).<ul> <li>It extracts the Header (Signature + Decorators) as a standalone chunk.</li> <li>It recurses into the Body (Block), passing the Header ID as the <code>parent_chunk_id</code>.</li> <li>This ensures that a 2000-line class results in multiple chunks, but every method inside still links back to the class header.</li> </ul> </li> </ul> </li> </ol>"},{"location":"reference/parsing/#noise-filtering","title":"Noise Filtering","text":"<p>The parser applies multiple layers of filtering to ensure graph quality: 1.  Technical Noise (<code>GLOBAL_IGNORE_DIRS</code>): Fast O(1) checks for <code>.git</code>, <code>node_modules</code> in the path. 2.  Semantic Noise (<code>SEMANTIC_NOISE_DIRS</code>): Heuristics to identify <code>test/</code>, <code>fixtures/</code>. 3.  Generated Code Detection: Scans the first 500 bytes for headers like \"Auto-generated by\". 4.  Minification Detection: Rejects files with line lengths &gt; 1000 chars.</p> <p>::: src.code_graph_indexer.graph.indexers.scip</p>"},{"location":"reference/parsing/#scipindexer","title":"SCIPIndexer","text":"<p>A wrapper around the Source Code Indexing Protocol (SCIP) CLI tools (e.g., <code>scip-python</code>, <code>scip-typescript</code>). It extracts \"deep\" semantic relationships like <code>calls</code>, <code>inherits</code>, and <code>imports</code>.</p>"},{"location":"reference/parsing/#stream_relations","title":"<code>stream_relations</code>","text":"<pre><code>def stream_relations(self, ...) -&gt; Generator[CodeRelation]\n</code></pre> <p>Orchestrates the external indexing process.</p>"},{"location":"reference/parsing/#internal-works-the-two-pass-extraction","title":"Internal Works: The Two-Pass Extraction","text":"<ol> <li> <p>Pass 1: Definition Table Building</p> <ul> <li>Iterates over the SCIP index.</li> <li>Extracts all <code>Definition</code> roles.</li> <li>Builds a temporary SQLite DB (<code>DiskSymbolTable</code>) mapping <code>symbol_name -&gt; {file, line, col}</code>.</li> <li>Why SQLite? A large Monorepo can have millions of symbols. Keeping a Python Dict causes OOM.</li> </ul> </li> <li> <p>Pass 2: Occurrence Resolution</p> <ul> <li>Iterates again over the index.</li> <li>Finds <code>Reference</code> roles (calls).</li> <li>Queries SQLite to find the target definition.</li> <li>Yields a resolved <code>CodeRelation</code> (Edge).</li> </ul> </li> </ol>"},{"location":"reference/reading/","title":"Reading API Reference","text":"<p>The <code>reader</code> module provides a Virtual Filesystem interface over the indexed snapshots. It allows agents to \"mount\" and browse a repository as if it were on a local disk, but with Time-Travel capabilities.</p>"},{"location":"reference/reading/#codereader","title":"CodeReader","text":"<pre><code>class CodeReader(storage: GraphStorage)\n</code></pre> <p>Key Features:</p> <ul> <li>Virtual FS: Files are not stored as plain text on disk but as chunked nodes in Postgres. The Reader reconstructs them on-the-fly.</li> <li>O(1) Listings: Directory contents are served from a pre-computed JSON manifest, making <code>ls</code> operations instant even for huge monorepos.</li> <li>Time Travel: Every read operation is scoped to a <code>snapshot_id</code>, allowing you to read the filesystem exactly as it appeared in any past commit.</li> </ul>"},{"location":"reference/reading/#methods","title":"Methods","text":""},{"location":"reference/reading/#read_file","title":"<code>read_file</code>","text":"<pre><code>def read_file(self, snapshot_id: str, file_path: str, start_line: int = None, end_line: int = None) -&gt; Dict[str, Any]\n</code></pre> <p>Description: Reads file content. Supports partial reads (byte-range fetches) which is efficient for large files where you only need a specific function.</p> <p>Arguments:</p> <ul> <li><code>snapshot_id</code> (str): The version to read from.</li> <li><code>file_path</code> (str): Relative path (e.g. <code>src/utils.py</code>).</li> <li><code>start_line</code> (int, optional): 1-indexed start line.</li> <li><code>end_line</code> (int, optional): 1-indexed end line.</li> </ul> <p>Returns: <pre><code>{\n    \"file_path\": \"src/utils.py\",\n    \"content\": \"def foo():\\n    pass\",\n    \"start_line\": 10,\n    \"end_line\": 11\n}\n</code></pre></p>"},{"location":"reference/reading/#list_directory","title":"<code>list_directory</code>","text":"<pre><code>def list_directory(self, snapshot_id: str, path: str = \"\") -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: Lists the children of a directory.</p> <p>Returns: A list of entries sorted with directories first. <pre><code>[\n    {\"name\": \"components\", \"type\": \"dir\", \"path\": \"src/components\"},\n    {\"name\": \"App.js\", \"type\": \"file\", \"path\": \"src/App.js\"}\n]\n</code></pre></p>"},{"location":"reference/reading/#find_directories","title":"<code>find_directories</code>","text":"<pre><code>def find_directories(self, snapshot_id: str, name_pattern: str, limit: int = 10) -&gt; List[str]\n</code></pre> <p>Description: Performs a \"Fuzzy Find\" for directory names. Useful when the agent knows a folder exists (e.g., \"auth\") but not where (e.g., <code>src/domain/auth</code> vs <code>libs/auth</code>).</p> <p>Implementation Note: This executes In-Memory against the cached JSON manifest. It does NOT impact the database.</p>"},{"location":"reference/retrieval/","title":"Retrieval API Reference","text":"<p>The <code>retriever</code> module acts as the \"Brain\" interface. It translates user intent (queries) into actionable code context.</p> <p>::: src.code_graph_indexer.retriever</p>"},{"location":"reference/retrieval/#coderetriever","title":"CodeRetriever","text":"<pre><code>class CodeRetriever(storage: GraphStorage, embedding_provider: EmbeddingProvider)\n</code></pre> <p>The main facade class. It requires access to both the database (for keyword search/graph walk) and the embedding API (for converting queries to vectors).</p>"},{"location":"reference/retrieval/#methods","title":"Methods","text":""},{"location":"reference/retrieval/#retrieve","title":"<code>retrieve</code>","text":"<pre><code>def retrieve(self, query: str, repo_id: str, snapshot_id: Optional[str] = None, limit: int = 10, strategy: str = \"hybrid\", filters: Dict[str, Any] = None) -&gt; List[RetrievedContext]\n</code></pre> <p>Description: Performs the end-to-end search pipeline: <code>Vectorize Query -&gt; Search DB -&gt; Fusion -&gt; Graph Expansion</code>.</p> <p>Arguments:</p> <ul> <li> <p><code>query</code> (str):     The search string. Can be natural language (\"How to auth?\") or code snippets (\"def login()\").</p> </li> <li> <p><code>repo_id</code> (str):     The UUID of the repository to search.</p> </li> <li> <p><code>snapshot_id</code> (Optional[str]):     If <code>None</code> (default), automatically resolves to the Latest Completed Snapshot for the repo. Providing an ID allows \"Time-Travel\" search on older versions.</p> </li> <li> <p><code>limit</code> (int):     The maximum number of results to return. Default 10. Note: Internally, it fetches <code>limit * 2</code> capabilities to allow for RRF re-ranking.</p> </li> <li> <p><code>strategy</code> (str):</p> <ul> <li><code>\"hybrid\"</code>: (Recommended) Runs both Vector and FTS, merges with RRF.</li> <li><code>\"vector\"</code>: Semantic similarity only. Good for broad concepts.</li> <li><code>\"keyword\"</code>: Text match only. Good for exact identifiers.</li> </ul> </li> <li> <p><code>filters</code> (Dict):     Metadata filters pushed down to SQL.</p> <ul> <li><code>\"path_prefix\"</code>: <code>str</code> (e.g. <code>\"src/api\"</code>). Matches files starting with this string.</li> <li><code>\"language\"</code>: <code>List[str]</code> (e.g. <code>[\"python\", \"js\"]</code>). Matches file extensions.</li> <li><code>\"exclude_category\"</code>: <code>List[str]</code> (e.g. <code>[\"test\"]</code>). Hides test files.</li> </ul> </li> </ul> <p>Returns: *   <code>List[RetrievedContext]</code>: A list of rich context objects, sorted by relevance score.</p>"},{"location":"reference/retrieval/#data-objects","title":"Data Objects","text":""},{"location":"reference/retrieval/#retrievedcontext","title":"<code>RetrievedContext</code>","text":"<pre><code>@dataclass\nclass RetrievedContext\n</code></pre> <p>The output unit of the retrieval process.</p>"},{"location":"reference/retrieval/#attributes","title":"Attributes","text":"<ul> <li><code>file_path</code> (str): Source file path relative to repo root.</li> <li><code>content</code> (str): The actual code block.</li> <li><code>start_line</code>, <code>end_line</code> (int): Line numbers (1-indexed).</li> <li><code>score</code> (float): The computed relevance score (normalized 0-1).</li> <li><code>semantic_labels</code> (List[str]): Tags extracted during parsing (e.g. <code>[\"class_definition\", \"public_api\"]</code>).</li> </ul>"},{"location":"reference/retrieval/#enriched-attributes-graph-context","title":"Enriched Attributes (Graph Context)","text":"<p>Attributes populated by the <code>GraphWalker</code> step.</p> <ul> <li> <p><code>parent_context</code> (Dict): Information about the containing scope.</p> <ul> <li>Example: <code>{\"type\": \"class\", \"name\": \"PaymentProcessor\", \"line\": 45}</code>.</li> <li>Usage: Tells the LLM where this function lives.</li> </ul> </li> <li> <p><code>outgoing_definitions</code> (List[Dict]): Summaries of external symbols used by this code.</p> <ul> <li>Example: <code>[{\"name\": \"stripe.charge\", \"role\": \"call\"}, {\"name\": \"User\", \"role\": \"instantiation\"}]</code>.</li> <li>Usage: Tells the LLM what dependencies this function has, without needing to retrieve those files.</li> </ul> </li> </ul>"},{"location":"reference/retrieval/#methods_1","title":"Methods","text":"<ul> <li><code>render()</code> -&gt; <code>str</code>     Returns a Markdown-formatted representation optimized for LLM prompting.     <pre><code>### File: src/main.py (L10-20)\n[CONTEXT] Inside class App\n[CODE]\ndef run(): ...\n[RELATIONS] Calls: logger.info, db.connect\n</code></pre></li> </ul>"},{"location":"reference/storage/","title":"Storage API Reference","text":"<p>The <code>storage</code> module is the foundation of the state management for the entire indexing system. It provides a robust, transactional abstraction over the persistence layer.</p> <p>::: src.code_graph_indexer.storage.postgres</p>"},{"location":"reference/storage/#database-schema","title":"Database Schema","text":"<p>The system is designed to run on PostgreSQL 15+ with the <code>pgvector</code> extension (<code>v0.5+</code>).</p>"},{"location":"reference/storage/#key-concepts","title":"Key Concepts","text":"<ul> <li>Repository Isolation: All data is partitioned logically by <code>repository_id</code>. A single database cluster can host thousands of repositories.</li> <li>Snapshot Lifecycle: Data is versioned using \"Snapshots\".<ul> <li><code>pending</code>: Data is being written (Indexing phase).</li> <li><code>indexing</code>: Active processing (Embedding phase).</li> <li><code>completed</code>: Immutable, ready for search.</li> <li><code>failed</code>: Aborted run.</li> </ul> </li> </ul>"},{"location":"reference/storage/#table-structure","title":"Table Structure","text":""},{"location":"reference/storage/#repositories","title":"<code>repositories</code>","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the repository config. <code>url</code> TEXT Git remote URL (e.g. <code>git@github.com...</code>). <code>branch</code> TEXT Branch name (e.g. <code>main</code>). <code>current_snapshot_id</code> UUID (FK) Pointer to the currently serving snapshot. Critical for Readers."},{"location":"reference/storage/#snapshots","title":"<code>snapshots</code>","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the indexing run. <code>repository_id</code> UUID (FK) Parent repo. <code>commit_hash</code> CHAR(40) Git SHA being indexed. <code>status</code> ENUM State of the snapshot process. <code>file_manifest</code> JSONB Compressed file listing for O(1) existence checks."},{"location":"reference/storage/#nodes-the-graph-vertices","title":"<code>nodes</code> (The Graph Vertices)","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the code chunk. <code>snapshot_id</code> UUID (FK) Partition key. <code>file_path</code> TEXT Path relative to repo root (e.g. <code>src/main.py</code>). <code>chunk_hash</code> CHAR(64) SHA-256 of the content (for deduplication). <code>content</code> TEXT The actual source code segment. <code>metadata</code> JSONB Semantic tags (<code>role</code>, <code>category</code>, <code>language</code>). <code>ts_vector</code> TSVECTOR Full-Text Search index (english)."},{"location":"reference/storage/#edges-the-graph-relations","title":"<code>edges</code> (The Graph Relations)","text":"Column Type Description <code>source_id</code> UUID (FK) Origin Node. <code>target_id</code> UUID (FK) Destination Node. <code>relation_type</code> TEXT Discriminator: <code>calls</code>, <code>defines</code>, <code>imports</code>, <code>inherits</code>, <code>child_of</code>."},{"location":"reference/storage/#node_embeddings-vectors","title":"<code>node_embeddings</code> (Vectors)","text":"Column Type Description <code>chunk_id</code> UUID (FK) Link to <code>nodes</code>. <code>embedding</code> VECTOR(1536) The dense vector representation. <code>vector_hash</code> CHAR(64) Identity hash for vector reuse optimization."},{"location":"reference/storage/#postgresgraphstorage","title":"PostgresGraphStorage","text":"<pre><code>class PostgresGraphStorage(GraphStorage)\n</code></pre> <p>The concrete implementation of the storage interface. It manages connections, transactions, and optimized bulk operations.</p>"},{"location":"reference/storage/#initialization","title":"Initialization","text":""},{"location":"reference/storage/#__init__","title":"<code>__init__</code>","text":"<p><pre><code>def __init__(self, connector: DatabaseConnector, vector_dim: int = 1536)\n</code></pre> *   connector: An instance of <code>PooledConnector</code> or <code>SingleConnector</code> (for workers). *   vector_dim: The dimension of the embeddings (default 1536 for OpenAI <code>text-embedding-3-small</code> / <code>ada-002</code>).</p>"},{"location":"reference/storage/#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"reference/storage/#ensure_repository","title":"<code>ensure_repository</code>","text":"<p><pre><code>def ensure_repository(self, url: str, branch: str, name: str) -&gt; str\n</code></pre> Idempotent. Ensures a record exists for the given (URL, Branch) pair. *   Returns: The <code>repository_id</code> (UUID). *   Concurrency: Uses <code>ON CONFLICT DO UPDATE</code> to handle race conditions safely.</p>"},{"location":"reference/storage/#create_snapshot","title":"<code>create_snapshot</code>","text":"<p><pre><code>def create_snapshot(self, repository_id: str, commit_hash: str, force_new: bool = False) -&gt; Tuple[Optional[str], bool]\n</code></pre> Creates a new <code>pending</code> snapshot. *   force_new: If <code>True</code>, creates a new snapshot even if one exists for this commit hash. *   Returns: <code>(snapshot_id, created)</code>. If <code>created</code> is False, it means a reusable snapshot was found.</p>"},{"location":"reference/storage/#activate_snapshot","title":"<code>activate_snapshot</code>","text":"<p><pre><code>def activate_snapshot(self, repository_id: str, snapshot_id: str, stats: Dict = None, manifest: Dict = None)\n</code></pre> Critical Operation. Atomically promotes a snapshot to \"Current\". 1.  Updates <code>snapshots</code> table: Status -&gt; <code>completed</code>, sets <code>completed_at</code>. 2.  Updates <code>repositories</code> table: <code>current_snapshot_id</code> -&gt; <code>snapshot_id</code>. 3.  Because this happens in a single transaction, readers (Search) switch instantly to the new version with zero downtime.</p>"},{"location":"reference/storage/#write-operations-optimized","title":"Write Operations (Optimized)","text":""},{"location":"reference/storage/#add_nodes_raw","title":"<code>add_nodes_raw</code>","text":"<p><pre><code>def add_nodes_raw(self, nodes_tuples: List[Tuple])\n</code></pre> Bypasses ORM for speed. *   Input: List of tuples matching the <code>nodes</code> table layout (excluding generated cols). *   Mechanism: Uses <code>cursor.copy_expert()</code> with <code>COPY ... FROM STDIN (BINARY)</code>. *   Performance: Capable of inserting 50k-100k rows/second. *   Raises: <code>psycopg2.errors.UniqueViolation</code> if UUIDs collide (rare).</p>"},{"location":"reference/storage/#ingest_scip_relations","title":"<code>ingest_scip_relations</code>","text":"<p><pre><code>def ingest_scip_relations(self, relations_tuples: List[Tuple], snapshot_id: str)\n</code></pre> Performs a \"Spatial Join\" resolution. SCIP gives us <code>(path, start_line, end_line) -&gt; Target</code>. We need to map this to <code>source_node_id -&gt; target_node_id</code>. 1.  Loads raw data into <code>temp_scip_staging</code>. 2.  Executes a complex SQL <code>JOIN</code> between <code>temp_scip_staging</code> and <code>nodes</code> based on file path and byte ranges. 3.  Inserts resolved edges into <code>edges</code>.</p>"},{"location":"reference/storage/#read-operations-search","title":"Read Operations (Search)","text":""},{"location":"reference/storage/#search_vectors","title":"<code>search_vectors</code>","text":"<p><pre><code>def search_vectors(self, query_vector: List[float], limit: int, snapshot_id: str, filters: Dict[str, Any] = None) -&gt; List[Dict]\n</code></pre> Performs Approximate Nearest Neighbor (ANN) search. *   query_vector: The float list from the embedding provider. *   snapshot_id: Mandatory. Scopes search to the active version. *   filters: Optional metadata filters. Supported keys:     *   <code>language</code>: Exact match on file extension.     *   <code>role</code>: Semantic role (e.g. <code>class</code>, <code>function</code>).     *   <code>path_prefix</code>: Starts-with match on file path. *   SQL Strategy:     <pre><code>ORDER BY embedding &lt;=&gt; :query_vector LIMIT :limit\n</code></pre>     This triggers the <code>HNSW</code> index scan.</p>"},{"location":"reference/storage/#search_fts","title":"<code>search_fts</code>","text":"<p><pre><code>def search_fts(self, query: str, limit: int, snapshot_id: str, filters: Dict = None) -&gt; List[Dict]\n</code></pre> Performs Full-Text Search. *   Uses PostgreSQL's <code>websearch_to_tsquery</code> which supports Google-like syntax:     *   <code>\"exact phrase\"</code>     *   <code>login -test</code> (exclude word)     *   <code>auth or security</code> *   Ranking: Uses <code>ts_rank</code> for relevancy.</p>"},{"location":"reference/storage/#get_file_manifest","title":"<code>get_file_manifest</code>","text":"<p><pre><code>def get_file_manifest(self, snapshot_id: str) -&gt; Dict[str, Any]\n</code></pre> Retrieves the cached directory tree structure. *   Use Case: Used by the UI/Frontend to render the file explorer tree without querying the <code>files</code> table recursively. *   Return Format: A nested dictionary representing the folder structure.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Sheep Codebase Indexer","text":"<p>Enterprise-Grade Code Intelligence for AI Agents</p> <p>The Sheep Codebase Indexer is a production-ready library that transforms source code into a queryable Code Property Graph (CPG) with semantic embeddings. Built for AI-powered code analysis, intelligent retrieval, and agentic workflows.</p>"},{"location":"#why-this-exists","title":"Why This Exists","text":"<p>Standard text-based RAG fails on code because code is structured, not prose. A function named <code>process_data()</code> is meaningless without understanding: - Where it's defined - Who calls it - What types it uses - Its dependencies</p> <p>Sheep Codebase Indexer solves this by:</p> <ol> <li>Parsing code structure into an AST-based graph</li> <li>Embedding semantically meaningful chunks (not random lines)</li> <li>Linking chunks via graph edges (<code>calls</code>, <code>inherits_from</code>, <code>references</code>)</li> <li>Retrieving with hybrid search (vector + keyword + graph traversal)</li> </ol>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#precise-code-understanding","title":"\ud83d\udd0d Precise Code Understanding","text":"<ul> <li>Tree-sitter Parsing: Zero-copy, incremental AST parsing for Python, TypeScript, Go, Java, Rust</li> <li>SCIP Integration: Industry-standard protocol for symbol resolution and cross-references  </li> <li>Semantic Chunking: Respects function/class boundaries with configurable overlap</li> </ul>"},{"location":"#enterprise-storage","title":"\ud83d\uddc4\ufe0f Enterprise Storage","text":"<ul> <li>PostgreSQL + pgvector: ACID compliance, scalability, vector search</li> <li>Snapshot Isolation: Atomic updates with zero-downtime reindexing</li> <li>COPY Protocol: 10x-50x faster bulk inserts</li> <li>Connection Pooling: Efficient concurrent access</li> </ul>"},{"location":"#high-performance","title":"\ud83d\ude80 High Performance","text":"<ul> <li>Parallel Processing: Multi-process indexing with <code>ProcessPoolExecutor</code></li> <li>Streaming Pipeline: Memory-efficient for large repositories</li> <li>Incremental Updates: Only reindex changed files</li> <li>Batch Embeddings: Optimized vector generation</li> </ul>"},{"location":"#advanced-retrieval","title":"\ud83d\udd0e Advanced Retrieval","text":"<ul> <li>Hybrid Search: Vector similarity + Full-text search with RRF</li> <li>Graph Traversal: Navigate call graphs, inheritance, dependencies</li> <li>Context Expansion: Auto-include related code (callers, callees, definitions)</li> <li>Multi-Stage Pipeline: Resolution \u2192 Search \u2192 Expansion</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>Source Code \u2192 Tree-sitter Parser \u2192 Semantic Chunks\n                                         \u2193\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2193                    \u2193                    \u2193\n            SCIP Relations        Embeddings          PostgreSQL Storage\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2193\n                              Knowledge Graph (CPG + Vectors)\n</code></pre> <p>The system follows a multi-stage pipeline:</p> <ol> <li>Parsing: Tree-sitter extracts AST and creates semantic chunks</li> <li>Relation Extraction: SCIP identifies definitions, references, calls</li> <li>Embedding: Chunks are vectorized for semantic search</li> <li>Storage: Graph stored in PostgreSQL with pgvector</li> <li>Retrieval: Hybrid search + graph traversal for context</li> </ol> <p>See Architecture Guide for detailed design.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install sheep-codebase-indexer\n</code></pre> <p>See Installation Guide for detailed setup.</p>"},{"location":"#index-a-repository","title":"Index a Repository","text":"<pre><code>from code_graph_indexer import CodebaseIndexer\nfrom code_graph_indexer.storage.connector import PooledConnector\n\n# Initialize\nconnector = PooledConnector(\n    db_url=\"postgresql://user:pass@localhost:5432/codebase\"\n)\n\nindexer = CodebaseIndexer(\n    repo_path=\"./my-project\",\n    storage_connector=connector\n)\n\n# Index\nindexer.index(\n    repo_url=\"https://github.com/org/repo\",\n    branch=\"main\"\n)\n</code></pre>"},{"location":"#search-and-retrieve","title":"Search and Retrieve","text":"<pre><code>from code_graph_indexer import CodeRetriever\n\nretriever = CodeRetriever(connector)\n\n# Hybrid search (vector + keyword)\nresults = retriever.search(\n    query=\"authentication middleware\",\n    limit=10,\n    include_context=True\n)\n</code></pre> <p>See Quickstart Guide for complete examples.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: Setup with Docker and troubleshooting</li> <li>Quickstart: Index your first repo in 5 minutes</li> </ul>"},{"location":"#guides","title":"Guides","text":"<ul> <li>Architecture: System design and components</li> <li>Data Model: Database schema and relationships</li> <li>Indexing Pipeline: Parsing and chunking internals</li> <li>Embedding Strategy: Vector generation and optimization</li> <li>Retrieval Strategies: Hybrid search and graph traversal</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Indexer: <code>CodebaseIndexer</code> API</li> <li>Storage: <code>PostgresGraphStorage</code> API</li> <li>Retrieval: <code>CodeRetriever</code>, <code>SearchExecutor</code>, <code>GraphWalker</code></li> <li>Parsing: <code>TreeSitterRepoParser</code>, <code>SCIPIndexer</code></li> <li>Embedding: <code>CodeEmbedder</code>, <code>EmbeddingProvider</code></li> </ul>"},{"location":"#testing-deployment","title":"Testing &amp; Deployment","text":"<ul> <li>Testing Guide: Testing philosophy and practices</li> <li>Production Deployment: PostgreSQL tuning and scaling</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Development Setup: Dev environment and workflow</li> <li>Code of Conduct: Community guidelines</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#ai-code-assistants","title":"AI Code Assistants","text":"<p>Provide LLMs with precise, contextual code snippets.</p>"},{"location":"#code-search-navigation","title":"Code Search &amp; Navigation","text":"<p>Find definitions, usages, and relationships across large codebases.</p>"},{"location":"#impact-analysis","title":"Impact Analysis","text":"<p>Understand ripple effects of code changes.</p>"},{"location":"#documentation-generation","title":"Documentation Generation","text":"<p>Auto-generate docs with call graphs and usage examples.</p>"},{"location":"#performance","title":"Performance","text":"Repository Files LOC Indexing Time Memory Small 150 15K 45s 500MB Medium 1,200 120K 6m 2GB Large 8,500 850K 42m 8GB <p>MacBook Pro M1, 16GB RAM, PostgreSQL 14</p>"},{"location":"#contributing_1","title":"Contributing","text":"<p>We welcome contributions! See CONTRIBUTING.md for guidelines.</p>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"#support","title":"Support","text":"<ul> <li>Documentation: Full docs at guides</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"faqs/","title":"Frequently Asked Questions","text":""},{"location":"faqs/#general-concepts","title":"General Concepts","text":""},{"location":"faqs/#what-is-sheep-codebase-indexer","title":"What is <code>sheep-codebase-indexer</code>?","text":"<p>Think of it as \"Google Indexing for your Private Codebase\", but built specifically for AI Agents. It is an ingestion engine that transforms raw source code (from Git) into a structured Knowledge Graph stored in PostgreSQL. It captures not just the text of the code, but the relationships (who calls whom, where is this defined, class hierarchy).</p>"},{"location":"faqs/#how-is-this-different-from-standard-rag-langchainllamaindex","title":"How is this different from standard RAG (LangChain/LlamaIndex)?","text":"<p>Standard RAG treats code as plain text documents. It splits files into chunks and embeds them. *   Standard RAG: \"Find lines similar to 'login error'.\" -&gt; Returns random snippets containing \"login\". *   We: \"Find the definition of <code>login</code> and its top 3 callers.\" -&gt; Returns the exact function body and the graph edges connecting it to the controllers that use it. We provide Structure. You can easily wrap our <code>CodeRetriever</code> as a generic \"Tool\" within LangChain or LlamaIndex.</p>"},{"location":"faqs/#what-is-a-code-property-graph-cpg","title":"What is a \"Code Property Graph\" (CPG)?","text":"<p>It's a data structure that combines: 1.  Abstract Syntax Tree (AST): The grammar of the code (Functions, Classes). 2.  Dependency Graph: semantic links (Imports, Calls, Inheritance). 3.  Embeddings: Vector representation of the code's meaning. We store all of this in a unified schema so you can query: \"Give me the embedding for the Function defined at line 50 that calls <code>User.save()</code>\".</p>"},{"location":"faqs/#use-cases","title":"Use Cases","text":""},{"location":"faqs/#what-can-i-build-with-this","title":"What can I build with this?","text":"<ol> <li>Context-Aware Coding Agents: An agent that doesn't hallucinate libraries because it can see the actual method signatures in the project.</li> <li>Repository Q&amp;A: A chatbot that answers \"How does the billing system handle retries?\" by traversing the call graph of the retry middleware.</li> <li>Automated Refactoring: Identify all 50 files that import a deprecated module to plan a migration.</li> <li>Onboarding Assistants: Help new engineers navigate legacy codebases by explaining flows rather than just files.</li> </ol>"},{"location":"faqs/#is-it-suitable-for-production","title":"Is it suitable for Production?","text":"<p>Yes. The system uses an Eventual Consistency model with Snapshot Isolation. *   You can index a new commit in the background. *   Your users continue searching the \"live\" snapshot without interruption. *   Once indexing is done, you atomically \"swap\" to the new snapshot. This is the same architectural pattern used by heavy-duty search engines.</p>"},{"location":"faqs/#architecture-design","title":"Architecture &amp; Design","text":""},{"location":"faqs/#why-postgresql-instead-of-a-dedicated-vector-db","title":"Why PostgreSQL instead of a dedicated Vector DB?","text":"<p>We believe in keeping the stack simple. PostgreSQL 15+ with <code>pgvector</code> offers: 1.  Transactional Integrity (ACID): We ensure the graph edges and the embeddings are always in sync. 2.  Complex JOINs: usage requires joining relational data (graph edges) with vector similarity. Postgres does this natively. 3.  Operational Maturity: Most teams already run Postgres. No need to manage a new piece of infrastructure like Pinecone or Weaviate just for this.</p>"},{"location":"faqs/#why-do-you-use-both-tree-sitter-and-scip","title":"Why do you use both Tree-sitter and SCIP?","text":"<p>They solve different problems: *   Tree-sitter is our \"Parser\". It is fast, runs locally, and understands the syntax (Where does the function start/end?). *   SCIP (Source Code Indexing Protocol) is our \"Linker\". It understands semantics (This usage of <code>User</code> refers to <code>models.py</code>). By combining them, we get the speed of regex-free parsing with the precision of a compiler.</p>"},{"location":"faqs/#operations-troubleshooting","title":"Operations &amp; Troubleshooting","text":""},{"location":"faqs/#does-it-support-huge-monorepos","title":"Does it support huge Monorepos?","text":"<p>Yes. *   Filtering: You can ignore <code>node_modules</code>, <code>vendor</code>, or specific folders via <code>GLOBAL_IGNORE_DIRS</code>. *   Incremental Indexing: We track commit hashes per file. If a file hasn't changed between commits, we reuse its existing nodes and embeddings, saving 90% of embedding costs. *   Parallelism: Parsing and Graph construction happen in parallel worker processes.</p>"},{"location":"faqs/#can-i-use-local-llms-ollama-llamacpp","title":"Can I use local LLMs (Ollama / Llama.cpp)?","text":"<p>Yes. The <code>EmbeddingProvider</code> is an abstract base class. You can implement a subclass that calls your local inference server (e.g., using <code>langchain</code> or direct HTTP calls) and pass it to the indexer. We default to OpenAI (<code>text-embedding-3-small</code>) because it provides the best cost/performance ratio for code today.</p>"},{"location":"faqs/#snapshot-locked-error","title":"<code>Snapshot locked</code> error","text":"<p>If the indexer process is kill -9'd, it might leave a snapshot in <code>indexing</code> state. *   Solution: Run <code>indexer.index(force=True, force_new=True)</code> to ignore the lock and start a fresh snapshot. The orphan snapshot will be cleaned up by the auto-pruner eventually.</p>"},{"location":"faqs/#search-returns-irrelevant-results","title":"Search returns irrelevant results","text":"<ul> <li>Check Filters: Are you filtering by <code>language='python'</code> but searching a TypeScript repo?</li> <li>Check Embeddings: Did the embedding process finish? Run <code>indexer.get_stats()</code> to see if <code>embeddings</code> count matches <code>total_nodes</code>.</li> <li>Tweak Strategy: If looking for specific error codes (e.g., <code>ERR_505</code>), force <code>strategy=\"keyword\"</code>.</li> </ul>"},{"location":"contributing/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident.</p>"},{"location":"contributing/development-setup/","title":"Development Setup","text":"<p>This guide covers how to set up the development environment for contributing to <code>sheep-codebase-indexer</code>.</p>"},{"location":"contributing/development-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: Version 3.11 or higher.</li> <li>Docker: Required for running the PostgreSQL + pgvector integration tests.</li> <li>Node.js: Required if you plan to touch the <code>debugger/frontend</code>.</li> <li>SCIP Tools: To run full end-to-end indexing on local code.</li> </ul>"},{"location":"contributing/development-setup/#1-environment-setup","title":"1. Environment Setup","text":"<p>We recommend using <code>venv</code> or <code>poetry</code>.</p> <pre><code># Clone the repository\ngit clone https://github.com/filippodaminato/sheep-codebase-indexer.git\ncd sheep-codebase-indexer\n\n# Create virtual env\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dependencies (Editable mode + Dev tools)\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/development-setup/#2-infrastructure-postgres","title":"2. Infrastructure (Postgres)","text":"<p>Start the local database for testing.</p> <pre><code>docker-compose up -d db\n</code></pre> <p>This starts PostgreSQL on port <code>6432</code> (mapped) with <code>pgvector</code> enabled. Connection String: <code>postgresql://sheep_user:sheep_password@localhost:6432/sheep_index</code></p>"},{"location":"contributing/development-setup/#3-running-tests","title":"3. Running Tests","text":"<p>We use <code>pytest</code>.</p> <pre><code># Run all tests\npytest tests/\n\n# Run specific functional tests (slow, integration)\npytest tests_files/test_workflow.py\n</code></pre>"},{"location":"contributing/development-setup/#4-code-style","title":"4. Code Style","text":"<ul> <li>Linting: We use <code>ruff</code>.</li> <li>Formatting: We use <code>black</code>.</li> <li>Type Checking: We use <code>mypy</code>.</li> </ul> <pre><code># Run full check\nruff check .\nblack .\nmypy src/\n</code></pre>"},{"location":"deployment/production/","title":"Production Deployment Guide","text":"<p>This guide covers deploying Sheep Codebase Indexer in production environments with focus on performance, reliability, and scalability.</p>"},{"location":"deployment/production/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Load Balancer \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n\u2502 API 1 \u2502 \u2502 API 2 \u2502  (Readers - Stateless)\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   PostgreSQL    \u2502  (Primary + Replicas)\n\u2502   + pgvector    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Worker Pool    \u2502  (Writers - Indexing)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/production/#postgresql-tuning","title":"PostgreSQL Tuning","text":""},{"location":"deployment/production/#hardware-requirements","title":"Hardware Requirements","text":"Scale CPU RAM Storage IOPS Small (100K chunks) 4 cores 8GB 50GB SSD 1K Medium (1M chunks) 8 cores 32GB 200GB SSD 5K Large (10M chunks) 16 cores 64GB 1TB NVMe 20K"},{"location":"deployment/production/#configuration","title":"Configuration","text":"<pre><code># postgresql.conf\n\n# Memory Settings\nshared_buffers = 8GB              # 25% of RAM\neffective_cache_size = 24GB       # 75% of RAM\nwork_mem = 256MB                  # For sorting/hashing\nmaintenance_work_mem = 2GB        # For VACUUM, CREATE INDEX\n\n# Checkpoint Settings\ncheckpoint_timeout = 15min\ncheckpoint_completion_target = 0.9\nmax_wal_size = 4GB\n\n# Connection Settings\nmax_connections = 200\nshared_preload_libraries = 'pg_stat_statements,pgvector'\n\n# Query Planner\nrandom_page_cost = 1.1            # For SSD\neffective_io_concurrency = 200    # For SSD\n\n# Parallel Query\nmax_parallel_workers_per_gather = 4\nmax_parallel_workers = 8\n</code></pre>"},{"location":"deployment/production/#vector-index-optimization","title":"Vector Index Optimization","text":"<pre><code>-- For datasets &lt; 1M vectors: IVFFlat\nCREATE INDEX idx_embeddings_ivfflat \nON node_embeddings \nUSING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- For datasets &gt; 1M vectors: HNSW\nCREATE INDEX idx_embeddings_hnsw \nON node_embeddings \nUSING hnsw (embedding vector_cosine_ops)\nWITH (m = 16, ef_construction = 64);\n\n-- Tune HNSW search\nSET hnsw.ef_search = 100;  -- Higher = more accurate, slower\n</code></pre>"},{"location":"deployment/production/#index-maintenance","title":"Index Maintenance","text":"<pre><code>-- Regular maintenance\nVACUUM ANALYZE node_embeddings;\nVACUUM ANALYZE nodes;\nVACUUM ANALYZE edges;\n\n-- Rebuild indexes periodically\nREINDEX INDEX CONCURRENTLY idx_embeddings_hnsw;\n\n-- Update statistics\nANALYZE;\n</code></pre>"},{"location":"deployment/production/#connection-pooling","title":"Connection Pooling","text":""},{"location":"deployment/production/#pgbouncer-configuration","title":"PgBouncer Configuration","text":"<pre><code># pgbouncer.ini\n\n[databases]\ncodebase = host=localhost port=5432 dbname=codebase\n\n[pgbouncer]\nlisten_addr = 0.0.0.0\nlisten_port = 6432\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\n\n# Pool settings\npool_mode = transaction\nmax_client_conn = 1000\ndefault_pool_size = 25\nreserve_pool_size = 5\nreserve_pool_timeout = 3\n\n# Performance\nserver_idle_timeout = 600\nserver_lifetime = 3600\n</code></pre>"},{"location":"deployment/production/#application-configuration","title":"Application Configuration","text":"<pre><code>from code_graph_indexer.storage.connector import PooledConnector\n\nconnector = PooledConnector(\n    db_url=\"postgresql://user:pass@pgbouncer:6432/codebase\",\n    min_size=10,\n    max_size=50,\n    max_queries=50000,  # Recycle connections\n    max_inactive_connection_lifetime=300\n)\n</code></pre>"},{"location":"deployment/production/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"deployment/production/#horizontal-scaling","title":"Horizontal Scaling","text":""},{"location":"deployment/production/#read-replicas","title":"Read Replicas","text":"<pre><code># Primary for writes\nprimary_connector = PooledConnector(\n    db_url=\"postgresql://user:pass@primary:5432/codebase\"\n)\n\n# Replica for reads\nreplica_connector = PooledConnector(\n    db_url=\"postgresql://user:pass@replica:5432/codebase\"\n)\n\n# Use replica for search\nretriever = CodeRetriever(replica_connector)\n\n# Use primary for indexing\nindexer = CodebaseIndexer(\n    repo_path=\"./repo\",\n    storage_connector=primary_connector\n)\n</code></pre>"},{"location":"deployment/production/#load-balancing","title":"Load Balancing","text":"<pre><code># nginx.conf\nupstream api_servers {\n    least_conn;\n    server api1:8000 weight=1;\n    server api2:8000 weight=1;\n    server api3:8000 weight=1;\n}\n\nserver {\n    listen 80;\n\n    location / {\n        proxy_pass http://api_servers;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n</code></pre>"},{"location":"deployment/production/#vertical-scaling","title":"Vertical Scaling","text":""},{"location":"deployment/production/#postgresql-partitioning","title":"PostgreSQL Partitioning","text":"<pre><code>-- Partition by repository\nCREATE TABLE nodes_partitioned (\n    id UUID,\n    repository_id UUID,\n    file_path TEXT,\n    content_hash TEXT,\n    metadata JSONB\n) PARTITION BY HASH (repository_id);\n\n-- Create partitions\nCREATE TABLE nodes_part_0 PARTITION OF nodes_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 0);\nCREATE TABLE nodes_part_1 PARTITION OF nodes_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n-- ... etc\n</code></pre>"},{"location":"deployment/production/#monitoring","title":"Monitoring","text":""},{"location":"deployment/production/#metrics-to-track","title":"Metrics to Track","text":"<pre><code>from prometheus_client import Counter, Histogram, Gauge\n\n# Request metrics\nsearch_requests = Counter('search_requests_total', 'Total search requests')\nsearch_duration = Histogram('search_duration_seconds', 'Search duration')\n\n# Database metrics\ndb_connections = Gauge('db_connections_active', 'Active DB connections')\nquery_duration = Histogram('db_query_duration_seconds', 'Query duration')\n\n# Indexing metrics\nchunks_indexed = Counter('chunks_indexed_total', 'Total chunks indexed')\nindexing_duration = Histogram('indexing_duration_seconds', 'Indexing duration')\n</code></pre>"},{"location":"deployment/production/#postgresql-monitoring","title":"PostgreSQL Monitoring","text":"<pre><code>-- Active queries\nSELECT pid, usename, state, query, now() - query_start AS duration\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY duration DESC;\n\n-- Index usage\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Cache hit ratio\nSELECT \n    sum(heap_blks_read) as heap_read,\n    sum(heap_blks_hit) as heap_hit,\n    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio\nFROM pg_statio_user_tables;\n</code></pre>"},{"location":"deployment/production/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Key metrics to visualize: - Query latency (p50, p95, p99) - Throughput (requests/second) - Error rate - Database connections - Cache hit ratio - Index usage</p>"},{"location":"deployment/production/#backup-strategy","title":"Backup Strategy","text":""},{"location":"deployment/production/#continuous-archiving","title":"Continuous Archiving","text":"<pre><code># postgresql.conf\nwal_level = replica\narchive_mode = on\narchive_command = 'cp %p /backup/wal/%f'\n</code></pre>"},{"location":"deployment/production/#base-backups","title":"Base Backups","text":"<pre><code>#!/bin/bash\n# backup.sh\n\n# Full backup\npg_basebackup -D /backup/base/$(date +%Y%m%d) \\\n    -Ft -z -P -h localhost -U postgres\n\n# Retention: keep last 7 days\nfind /backup/base -type d -mtime +7 -exec rm -rf {} \\;\n</code></pre>"},{"location":"deployment/production/#point-in-time-recovery","title":"Point-in-Time Recovery","text":"<pre><code># recovery.conf\nrestore_command = 'cp /backup/wal/%f %p'\nrecovery_target_time = '2024-01-01 12:00:00'\n</code></pre>"},{"location":"deployment/production/#security","title":"Security","text":""},{"location":"deployment/production/#network-security","title":"Network Security","text":"<pre><code># Use SSL for database connections\nconnector = PooledConnector(\n    db_url=\"postgresql://user:pass@host:5432/db?sslmode=require\"\n)\n</code></pre>"},{"location":"deployment/production/#access-control","title":"Access Control","text":"<pre><code>-- Create read-only user for API\nCREATE ROLE api_reader WITH LOGIN PASSWORD 'secure_password';\nGRANT CONNECT ON DATABASE codebase TO api_reader;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO api_reader;\n\n-- Create write user for indexer\nCREATE ROLE indexer_writer WITH LOGIN PASSWORD 'secure_password';\nGRANT CONNECT ON DATABASE codebase TO indexer_writer;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO indexer_writer;\n</code></pre>"},{"location":"deployment/production/#api-authentication","title":"API Authentication","text":"<pre><code>from fastapi import Depends, HTTPException\nfrom fastapi.security import HTTPBearer\n\nsecurity = HTTPBearer()\n\nasync def verify_token(credentials = Depends(security)):\n    if credentials.credentials != os.getenv(\"API_TOKEN\"):\n        raise HTTPException(status_code=401, detail=\"Invalid token\")\n    return credentials\n</code></pre>"},{"location":"deployment/production/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/production/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  postgres:\n    image: pgvector/pgvector:pg16\n    environment:\n      POSTGRES_DB: codebase\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n    command: &gt;\n      postgres\n      -c shared_buffers=2GB\n      -c effective_cache_size=6GB\n      -c work_mem=128MB\n\n  pgbouncer:\n    image: pgbouncer/pgbouncer:latest\n    environment:\n      DATABASES_HOST: postgres\n      DATABASES_PORT: 5432\n      DATABASES_DBNAME: codebase\n      PGBOUNCER_POOL_MODE: transaction\n      PGBOUNCER_MAX_CLIENT_CONN: 1000\n    ports:\n      - \"6432:6432\"\n    depends_on:\n      - postgres\n\n  api:\n    build: .\n    environment:\n      DB_URL: postgresql://postgres:${DB_PASSWORD}@pgbouncer:6432/codebase\n      EMBEDDING_PROVIDER: openai\n      OPENAI_API_KEY: ${OPENAI_API_KEY}\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - pgbouncer\n    deploy:\n      replicas: 3\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"deployment/production/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"deployment/production/#expected-performance","title":"Expected Performance","text":"Operation Throughput Latency (p95) Vector Search 1000 req/s 50ms Keyword Search 2000 req/s 30ms Hybrid Search 800 req/s 80ms Graph Traversal 500 req/s 100ms Indexing 10K chunks/min -"},{"location":"deployment/production/#load-testing","title":"Load Testing","text":"<pre><code>import asyncio\nimport aiohttp\n\nasync def load_test(num_requests=1000):\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for i in range(num_requests):\n            task = session.post(\n                \"http://localhost:8000/search\",\n                json={\"query\": f\"test query {i}\", \"limit\": 10}\n            )\n            tasks.append(task)\n\n        responses = await asyncio.gather(*tasks)\n        return responses\n</code></pre>"},{"location":"deployment/production/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/production/#high-cpu-usage","title":"High CPU Usage","text":"<pre><code>-- Find expensive queries\nSELECT query, calls, total_time, mean_time\nFROM pg_stat_statements\nORDER BY total_time DESC\nLIMIT 10;\n</code></pre>"},{"location":"deployment/production/#high-memory-usage","title":"High Memory Usage","text":"<pre><code>-- Check work_mem usage\nSELECT name, setting, unit\nFROM pg_settings\nWHERE name IN ('work_mem', 'maintenance_work_mem', 'shared_buffers');\n</code></pre>"},{"location":"deployment/production/#slow-queries","title":"Slow Queries","text":"<pre><code>-- Enable query logging\nALTER SYSTEM SET log_min_duration_statement = 1000;  -- Log queries &gt; 1s\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"deployment/production/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"deployment/production/#failover-plan","title":"Failover Plan","text":"<ol> <li>Detect failure: Monitor primary database</li> <li>Promote replica: <code>pg_ctl promote</code></li> <li>Update connection strings: Point to new primary</li> <li>Verify data integrity: Check replication lag</li> </ol>"},{"location":"deployment/production/#recovery-checklist","title":"Recovery Checklist","text":"<ul> <li>[ ] Restore from backup</li> <li>[ ] Replay WAL logs</li> <li>[ ] Verify data integrity</li> <li>[ ] Update DNS/load balancer</li> <li>[ ] Test application connectivity</li> <li>[ ] Monitor for errors</li> </ul>"},{"location":"deployment/production/#cost-optimization","title":"Cost Optimization","text":""},{"location":"deployment/production/#database-costs","title":"Database Costs","text":"<ul> <li>Use managed PostgreSQL (AWS RDS, GCP Cloud SQL) for easier management</li> <li>Enable auto-scaling for read replicas</li> <li>Use reserved instances for predictable workloads</li> </ul>"},{"location":"deployment/production/#embedding-costs","title":"Embedding Costs","text":"<ul> <li>Cache embeddings aggressively (30-50% savings)</li> <li>Use incremental updates (90% savings)</li> <li>Consider local models for development</li> </ul>"},{"location":"deployment/production/#infrastructure-costs","title":"Infrastructure Costs","text":"<ul> <li>Use spot instances for indexing workers</li> <li>Scale down during off-peak hours</li> <li>Implement request caching</li> </ul>"},{"location":"deployment/production/#next-steps","title":"Next Steps","text":"<ul> <li>Monitoring Setup: Detailed monitoring guide</li> <li>API Reference: Complete API documentation</li> <li>Scaling Guide: Advanced scaling strategies</li> </ul>"},{"location":"examples/advanced_usage/","title":"Advanced Usage &amp; Recipes","text":"<p>This guide covers scenarios beyond the basic \"Quickstart\", tailored for enterprise deployments and power users.</p>"},{"location":"examples/advanced_usage/#1-indexing-strategy","title":"1. Indexing Strategy","text":""},{"location":"examples/advanced_usage/#monorepo-filtering","title":"Monorepo Filtering","text":"<p>For large repositories, you generally want to exclude build artifacts, docs, and test fixtures to keep the graph clean and performance high.</p> <p>The indexer respects <code>GLOBAL_IGNORE_DIRS</code> by default, but you can customize this by modifying <code>src/code_graph_indexer/parsing/parsing_filters.py</code> or by passing custom logic if you extend the class.</p> <p>Default Exclusions: *   Technical Noise: <code>node_modules</code>, <code>.git</code>, <code>dist</code>, <code>__pycache__</code> *   Semantic Noise: <code>fixtures</code>, <code>migrations</code>, <code>locales</code></p>"},{"location":"examples/advanced_usage/#branch-specific-indexing","title":"Branch-Specific Indexing","text":"<p>You can maintain separate indices for different branches (e.g., <code>main</code> vs <code>develop</code>).</p> <p><pre><code>from code_graph_indexer import CodebaseIndexer\n\n# index main (Production)\nindexer_main = CodebaseIndexer(\n    repo_url=\"git@github.com:org/repo.git\",\n    branch=\"main\",\n    db_url=\"postgresql://user:pass@localhost/dbname\"\n)\nindexer_main.index()\n\n# index feature-branch (Development)\nindexer_dev = CodebaseIndexer(\n    repo_url=\"git@github.com:org/repo.git\",\n    branch=\"feature/new-search-api\",\n    db_url=\"postgresql://user:pass@localhost/dbname\"\n)\nindexer_dev.index()\n</code></pre> Note: The <code>Repository</code> entity in the DB is unique per (url, branch) pair.</p>"},{"location":"examples/advanced_usage/#2-advanced-search-retrieval","title":"2. Advanced Search &amp; Retrieval","text":""},{"location":"examples/advanced_usage/#using-metadata-filters","title":"Using Metadata Filters","text":"<p>The <code>filters</code> argument in <code>retrieve()</code> allows you to slice the graph by any metadata field extracted during parsing. This is pushed down to the database (SQL <code>WHERE</code> clause) for maximum speed.</p> <p>Filter by Language: <pre><code>results = retriever.retrieve(\n    query=\"authentication middleware\",\n    repo_id=repo_id,\n    filters={\"language\": \"python\"} \n)\n</code></pre></p> <p>Filter by Semantic Role: Find only Class Definitions related to \"User\": <pre><code>results = retriever.retrieve(\n    query=\"User\",\n    repo_id=repo_id,\n    filters={\"role\": \"class\"} # derived either from 'type' or 'category' in metadata\n)\n</code></pre></p> <p>Filter by File Path: Scope search to a specific module: <pre><code>results = retriever.retrieve(\n    query=\"calculate_tax\",\n    repo_id=repo_id,\n    filters={\"start_path\": \"src/billing/\"} # Implementation depends on storage backend\n)\n</code></pre></p>"},{"location":"examples/advanced_usage/#hybrid-search-tuning","title":"Hybrid Search Tuning","text":"<p>By default, <code>CodeRetriever</code> uses Reciprocal Rank Fusion (RRF) to combine Vector and Keyword results. You can force a specific strategy if you know what you are doing.</p> <ul> <li><code>strategy='vector'</code>: Best for concept search (\"how do I login?\").</li> <li><code>strategy='keyword'</code>: Best for exact error codes (\"Error 503\") or specific symbol names (\"UserFactory\").</li> <li><code>strategy='hybrid'</code>: (Default) Best of both worlds.</li> </ul> <pre><code># Force exact keyword match for an error code\nhits = retriever.retrieve(\n    query=\"ERR_CONNECTION_RESET\",\n    repo_id=repo_id,\n    strategy=\"keyword\"\n)\n</code></pre>"},{"location":"examples/advanced_usage/#3-custom-embedding-models","title":"3. Custom Embedding Models","text":"<p>The system uses the <code>EmbeddingProvider</code> interface. You can swap OpenAI with any other provider (Vertex AI, HuggingFace, Bedrock) by implementing a simple adapter.</p> <pre><code>from code_graph_indexer.providers.embedding import EmbeddingProvider\n\nclass VertexEmbeddingProvider(EmbeddingProvider):\n    def __init__(self, project_id, model=\"text-embedding-gecko\"):\n        self.client = ... # Initialize Vertex AI client\n\n    async def embed_async(self, texts: List[str]) -&gt; List[List[float]]:\n        # Implement call to Google Cloud\n        return await self.client.get_embeddings(texts)\n\n# Usage\nindexer = CodebaseIndexer(..., embedding_provider=VertexEmbeddingProvider(\"my-project\"))\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers the prerequisites and steps to install <code>sheep-codebase-indexer</code> in your environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the python library, ensure you have the following system components:</p> <ol> <li>Python 3.10+: The codebase uses modern Python features like <code>asyncio</code> and type hinting.</li> <li>PostgreSQL 15+: Required for robust data storage.</li> <li>pgvector: The PostgreSQL extension for vector similarity search.</li> <li>Git: Required for cloning and managing repositories.</li> <li>SCIP CLI (Required): For advanced semantic indexing (LSIF).<ul> <li>Install via npm: <code>npm install -g @sourcegraph/scip-typescript @sourcegraph/scip-python</code> (etc.)</li> </ul> </li> </ol>"},{"location":"getting-started/installation/#installation_1","title":"Installation","text":""},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>The recommended way to install is directly from the source code, as it is an internal library.</p> <pre><code>git clone https://github.com/your-org/sheep-codebase-indexer.git\ncd sheep-codebase-indexer\npip install .\n</code></pre>"},{"location":"getting-started/installation/#with-development-dependencies","title":"With Development Dependencies","text":"<p>If you plan to contribute or run tests:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>The library uses Environment Variables for configuration. You can set these in your shell or use a <code>.env</code> file.</p> Variable Description Default Required <code>DATABASE_URL</code> PostgreSQL Connection String. <code>postgresql://user:pass@localhost:5432/sheep</code> Yes <code>OPENAI_API_KEY</code> Key for generating embeddings (OpenAIProvider). None Yes <code>REPO_VOLUME</code> Local directory where repos are cloned/cached. <code>/var/tmp/sheep_volume</code> No <code>LOG_LEVEL</code> Python logging level (DEBUG, INFO). <code>INFO</code> No"},{"location":"getting-started/installation/#setting-up-postgresql-with-pgvector","title":"Setting up PostgreSQL with pgvector","text":"<p>Ensure your database has the <code>vector</code> extension enabled:</p> <pre><code>CREATE EXTENSION IF NOT EXISTS vector;\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will help you index a repository and perform your first semantic search in under 10 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker (for PostgreSQL)</li> <li>Python 3.11+</li> <li>OpenAI API Key (or another embedding provider)</li> </ul>"},{"location":"getting-started/quickstart/#1-start-infrastructure","title":"1. Start Infrastructure","text":"<p>Use the provided <code>docker-compose.yml</code> to spin up PostgreSQL with <code>pgvector</code>:</p> <pre><code>docker-compose up -d db\n</code></pre>"},{"location":"getting-started/quickstart/#2-install-library","title":"2. Install Library","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/quickstart/#3-creating-the-index-python-script","title":"3. Creating the Index (Python Script)","text":"<p>Create a file named <code>index_repo.py</code>. We will use the <code>CodebaseIndexer</code> to clone, parse, and embed the <code>flask</code> repository.</p> <pre><code>import os\nimport asyncio\nfrom code_graph_indexer.indexer import CodebaseIndexer\nfrom code_graph_indexer.providers.embedding import OpenAIEmbeddingProvider\n\n# 1. Configuration\nDB_URL = \"postgresql://sheep_user:sheep_password@localhost:6432/sheep_index\"\nREPO_URL = \"https://github.com/pallets/flask.git\"\nBRANCH = \"main\"\n\nasync def main():\n    # 2. Init Indexer\n    # The indexer manages cloning and DB connection pools automatically.\n    indexer = CodebaseIndexer(REPO_URL, BRANCH, db_url=DB_URL)\n\n    # 3. Init Provider (Async)\n    # Use a cost-effective model like text-embedding-3-small\n    provider = OpenAIEmbeddingProvider(\n        model=\"text-embedding-3-small\", \n        max_concurrency=10\n    )\n\n    try:\n        # 4. Phase 1: Indexing (Parsing &amp; Graph Construction)\n        print(\"Starting Indexing (Parsing, SCIP)...\")\n        snapshot_id = indexer.index(force=False)\n        print(f\"Active Snapshot: {snapshot_id}\")\n\n        # 5. Phase 2: Embedding (Async Pipeline)\n        print(\"Starting Embedding pipeline...\")\n        async for update in indexer.embed(provider, batch_size=200):\n            status = update['status']\n            if status == 'embedding_progress':\n                print(f\"   Processing... {update.get('total_embedded')} vectors\", end='\\r')\n            elif status == 'completed':\n                print(f\"\\nDone! New vectors: {update.get('newly_embedded')}\")\n\n    finally:\n        indexer.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run it: <pre><code>export OPENAI_API_KEY=\"sk-...\"\npython index_repo.py\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-semantic-search","title":"4. Semantic Search","text":"<p>Now that the data is indexed, let's search it. Create <code>search.py</code>:</p> <pre><code>import asyncio\nfrom code_graph_indexer.indexer import CodebaseIndexer\nfrom code_graph_indexer.retriever import CodeRetriever\nfrom code_graph_indexer.providers.embedding import OpenAIEmbeddingProvider\n\nDB_URL = \"postgresql://sheep_user:sheep_password@localhost:6432/sheep_index\"\nREPO_URL = \"https://github.com/pallets/flask.git\"\n\nasync def main():\n    # We reuse the indexer to get access to storage\n    indexer = CodebaseIndexer(REPO_URL, \"main\", db_url=DB_URL)\n    provider = OpenAIEmbeddingProvider(model=\"text-embedding-3-small\")\n\n    # Init Retriever\n    retriever = CodeRetriever(indexer.storage, provider)\n\n    # Resolve Repository ID\n    repo_id = indexer.storage.get_repository(indexer.storage.ensure_repository(REPO_URL, \"main\", \"flask\"))['id']\n\n    query = \"How does request routing work?\"\n    print(f\"Searching for: '{query}'...\")\n\n    results = retriever.retrieve(\n        query, \n        repo_id=repo_id, \n        limit=3, \n        strategy=\"hybrid\"\n    )\n\n    for r in results:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"File: {r.file_path} (Line {r.start_line})\")\n        print(f\"Score: {r.score:.4f}\")\n        print(f\"Labels: {r.semantic_labels}\")\n        print(\"-\" * 50)\n        print(r.content[:200] + \"...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/architecture/","title":"System Architecture","text":"<p>This document describes the high-level architecture of the <code>sheep-codebase-indexer</code>. The system is designed as a Code Property Graph (CPG) Builder that ingests source code repositories and transforms them into a queriable knowledge graph.</p>"},{"location":"guides/architecture/#high-level-overview","title":"High-Level Overview","text":"<p>The system operates in two main phases: Indexing (Write Path) and Retrieval (Read Path).</p>"},{"location":"guides/architecture/#write-path-from-git-to-graph","title":"Write Path: From Git to Graph","text":"<pre><code>graph TD\n    A[Git Repository] --&gt;|Clone| B(GitVolumeManager)\n    B --&gt;|Files| C{ParserOrchestrator}\n\n    subgraph \"Parallel Processing\"\n        direction TB\n        C --&gt;|Syntactic Analysis| D[TreeSitterRepoParser]\n        C --&gt;|Semantic Analysis| E[SCIPIndexer]\n    end\n\n    D --&gt;|Chunks| F[CodeEmbedder]\n    F --&gt;|Embeddings| G[PostgresGraphStorage]\n    E --&gt;|Edges| G</code></pre>"},{"location":"guides/architecture/#read-path-from-query-to-context","title":"Read Path: From Query to Context","text":"<pre><code>graph TD\n    H[User Query] --&gt;|Input| I[CodeRetriever]\n    I --&gt;|Vector/Keyword Search| J[SearchExecutor]\n\n    subgraph \"Enrichment\"\n        direction TB\n        J --&gt;|Node IDs| K[GraphWalker]\n        K --&gt;|Traverse Graph| L[Augmented Context]\n    end</code></pre>"},{"location":"guides/architecture/#indexing-pipeline","title":"Indexing Pipeline","text":"<p>The indexing process is orchestrated by <code>CodebaseIndexer</code>. It follows a Snapshot-based consistency model: every indexing run creates an immutable <code>Snapshot</code> of the repository.</p>"},{"location":"guides/architecture/#1-ingestion-gitvolumemanager","title":"1. Ingestion (<code>GitVolumeManager</code>)","text":"<ul> <li>Role: Manages the cloning and updating of Git repositories.</li> <li>Mechanism: Uses <code>git</code> CLI (via <code>GitPython</code>) to fetch data.</li> <li>Optimization: Maintains a persistent \"Bare Repository\" cache and creates lightweight <code>worktrees</code> for specific commits. This avoids re-downloading the entire history for every analysis.</li> </ul>"},{"location":"guides/architecture/#2-syntactic-parsing-treesitterrepoparser","title":"2. Syntactic Parsing (<code>TreeSitterRepoParser</code>)","text":"<ul> <li>Role: Breaks down source files into \"Chunks\" (Functions, Classes, Methods).</li> <li>Technology: Uses <code>tree-sitter</code>, a high-performance incremental parser.</li> <li>Logic:<ul> <li>Iterates over all files.</li> <li>Applies language-specific S-expression queries (<code>*.scm</code>) to extract definitions.</li> <li>Splits large files using a recursive \"scope-aware\" chunking strategy to preserve context.</li> </ul> </li> </ul>"},{"location":"guides/architecture/#3-semantic-analysis-scipindexer","title":"3. Semantic Analysis (<code>SCIPIndexer</code>)","text":"<ul> <li>Role: Extracts cross-file relationships (e.g., <code>calls</code>, <code>inherits</code>, <code>imports</code>).</li> <li>Technology: Wraps SCIP (Source Code Indexing Protocol) CLIs (e.g., <code>scip-python</code>, <code>scip-typescript</code>).</li> <li>Process:<ul> <li>Runs the language-specific indexer (often requiring a build environment).</li> <li>Streams the resulting Protobuf index.</li> <li>Resolves \"References\" to \"Definitions\" to create graph edges.</li> </ul> </li> </ul>"},{"location":"guides/architecture/#4-embedding-generation-codeembedder","title":"4. Embedding Generation (<code>CodeEmbedder</code>)","text":"<ul> <li>Role: Converts text code chunks into dense vectors.</li> <li>Pipeline:<ol> <li>Staging: Chunks are written to a temporary table.</li> <li>Deduplication: The system computes a hash of the content. If the same code block (same hash) was already embedded in a previous snapshot, the implementation reuses the existing vector.</li> <li>Delta Computing: Only new/changed chunks are sent to the LLM API.</li> <li>Batching: Requests are batched to respect API rate limits.</li> </ol> </li> </ul>"},{"location":"guides/architecture/#storage-layer-postgresgraphstorage","title":"Storage Layer (<code>PostgresGraphStorage</code>)","text":"<p>The persistence layer is valid PostgreSQL 15+ with the <code>pgvector</code> extension. It handles the Code Property Graph (CPG) schema, ensuring data integrity and high performance for both inserts and lookups.</p> <p>Key Responsibilities: *   Snapshot Isolation: Guarantees consistent reads during indexing updates (MVCC). *   Bulk Ingestion: Uses <code>COPY</code> protocol for millions of rows/sec. *   Hybrid Indexing: Manages IVFFlat/HNSW indexes for vectors and GIN indexes for keywords.</p> <p>Full Specification</p> <p>For a complete, exhaustive description of the Database Schema, Table structures, and Method behaviors, please consult the Storage API Reference.</p>"},{"location":"guides/architecture/#retrieval-architecture","title":"Retrieval Architecture","text":"<p>The <code>CodeRetriever</code> implements a \"Retrieval-Augmented Generation\" (RAG) specialized for code.</p>"},{"location":"guides/architecture/#1-hybrid-search","title":"1. Hybrid Search","text":"<p>Combines two search strategies:</p> <ul> <li>Dense Retrieval (Vector): Finds conceptually similar code (e.g. \"auth logic\" -&gt; <code>login()</code>).</li> <li>Sparse Retrieval (Keyword): Finds exact matches (e.g. <code>UserFactory</code>, <code>API_KEY</code>).</li> </ul>"},{"location":"guides/architecture/#2-result-fusion","title":"2. Result Fusion","text":"<p>Uses Reciprocal Rank Fusion (RRF) to combine the ranked lists from Vector and Keyword search into a single, high-quality result set.</p>"},{"location":"guides/architecture/#3-graph-expansion-graphwalker","title":"3. Graph Expansion (<code>GraphWalker</code>)","text":"<p>Once relevant \"Seed Nodes\" are found, the <code>GraphWalker</code> traverses the graph edges to fetch context that wasn't in the search results:</p> <ul> <li>Vertical Expansion: \"Who defines this function?\" (Parent Class).</li> <li>Horizontal Expansion: \"What does this function call?\" (Dependencies).</li> </ul> <p>This ensures the LLM receives a complete subgraph rather than a disconnected snippet.</p>"},{"location":"guides/data_model/","title":"Data Model","text":"<p>This document details the core entities and relationships in the <code>sheep-codebase-indexer</code>.</p>"},{"location":"guides/data_model/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":"<pre><code>erDiagram\n    REPOSITORY ||--o{ SNAPSHOT : has\n    SNAPSHOT ||--o{ FILE_RECORD : contains\n    FILE_RECORD ||--o{ CHUNK_NODE : defines\n    CHUNK_NODE ||--o{ CODE_RELATION : source\n    CHUNK_NODE ||--o{ CODE_RELATION : target\n\n    REPOSITORY {\n        string id PK\n        string url\n        string current_snapshot_id FK\n    }\n\n    SNAPSHOT {\n        string id PK\n        string commit_hash\n        string status\n    }\n\n    CHUNK_NODE {\n        string id PK\n        string chunk_hash\n        vector embedding\n        jsonb metadata\n    }\n\n    CODE_RELATION {\n        string source_id FK\n        string target_id FK\n        string type\n    }</code></pre>"},{"location":"guides/data_model/#core-entities","title":"Core Entities","text":""},{"location":"guides/data_model/#1-repository-modelsrepository","title":"1. Repository (<code>models.Repository</code>)","text":"<p>Represents a stable project identity. *   id: Unique hash of the remote URL. *   current_snapshot_id: Pointer to the currently active version of the code.</p>"},{"location":"guides/data_model/#2-snapshot-modelssnapshot","title":"2. Snapshot (<code>models.Snapshot</code>)","text":"<p>Represents an immutable version of the code at a specific commit. *   Status Lifecycle: <code>pending</code> -&gt; <code>indexing</code> -&gt; <code>completed</code> (or <code>failed</code>). *   Isolation: Allows indexing a new commit in the background while serving queries from the old commit.</p>"},{"location":"guides/data_model/#3-filerecord-modelsfilerecord","title":"3. FileRecord (<code>models.FileRecord</code>)","text":"<p>Represents a physical file on disk. *   file_hash: Content hash (SHA256) used for change detection. *   parsing_status: Tracks if <code>tree-sitter</code> successfully parsed the file.</p>"},{"location":"guides/data_model/#4-chunknode-modelschunknode","title":"4. ChunkNode (<code>models.ChunkNode</code>)","text":"<p>The fundamental unit of the graph (a \"Node\"). *   Granularity: Can be a Function, Class, Method, or a standalone specific block of code. *   Vector: Stores the 1536-dim embedding (if using OpenAI). *   Metadata: JSONB field containing \"semantic captures\" (e.g., <code>role: entry_point</code>, <code>access: public</code>).</p>"},{"location":"guides/data_model/#5-coderelation-modelscoderelation","title":"5. CodeRelation (<code>models.CodeRelation</code>)","text":"<p>A directed edge between two ChunkNodes. *   Types:     *   <code>child_of</code>: Structural containment (Method -&gt; Class).     *   <code>calls</code>: Function A invokes Function B.     *   <code>inherits</code>: Class A extends Class B.     *   <code>imports</code>: File A requires Module B.</p>"},{"location":"guides/embedding-strategy/","title":"Embedding Strategy","text":"<p>This guide explains how code chunks are converted into semantic vectors for similarity search.</p>"},{"location":"guides/embedding-strategy/#overview","title":"Overview","text":"<p>Embeddings transform code into high-dimensional vectors that capture semantic meaning. Similar code produces similar vectors, enabling semantic search beyond keyword matching.</p> <pre><code>Code Chunk \u2192 Tokenization \u2192 Embedding Model \u2192 Vector (1536 dims) \u2192 Storage\n</code></pre>"},{"location":"guides/embedding-strategy/#why-embeddings-for-code","title":"Why Embeddings for Code?","text":""},{"location":"guides/embedding-strategy/#traditional-search-limitations","title":"Traditional Search Limitations","text":"<p>Keyword search fails when: - Variable names differ (<code>process_data</code> vs <code>handle_request</code>) - Comments are missing - Code is refactored but semantically identical</p> <p>Example: <pre><code># Version 1\ndef calculate_total(items):\n    return sum(item.price for item in items)\n\n# Version 2 (semantically identical)\ndef compute_sum(products):\n    total = 0\n    for product in products:\n        total += product.cost\n    return total\n</code></pre></p> <p>Keyword search sees these as completely different. Embeddings recognize semantic similarity.</p>"},{"location":"guides/embedding-strategy/#embedding-models","title":"Embedding Models","text":""},{"location":"guides/embedding-strategy/#provider-options","title":"Provider Options","text":"<p>The library supports multiple embedding providers:</p> <p>Cloud Providers: - OpenAI: Industry-standard models with good performance - Cohere: Optimized for semantic search - Anthropic: High-quality embeddings - Google: Vertex AI embedding models</p> <p>Local Models: - Sentence Transformers: Free, runs locally - ONNX Runtime: Optimized inference - Custom Models: Bring your own embedding model</p>"},{"location":"guides/embedding-strategy/#choosing-a-model","title":"Choosing a Model","text":"<p>For production: - Cloud providers offer best accuracy and reliability - Consider API costs vs infrastructure costs - Evaluate based on your specific use case</p> <p>For development: - Local models avoid API costs - Good for testing and prototyping - Switch to cloud providers for production</p>"},{"location":"guides/embedding-strategy/#configuration","title":"Configuration","text":"<pre><code>from code_graph_indexer.providers.embedding import OpenAIEmbeddingProvider\n\nprovider = OpenAIEmbeddingProvider(\n    model=\"text-embedding-3-small\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    max_retries=3,\n    timeout=30\n)\n</code></pre> <p>Note: Model availability and capabilities change frequently. Check provider documentation for current offerings.</p>"},{"location":"guides/embedding-strategy/#what-gets-embedded","title":"What Gets Embedded","text":"<p>Understanding exactly what content is embedded is crucial for optimizing search quality and relevance.</p>"},{"location":"guides/embedding-strategy/#embedding-template","title":"Embedding Template","text":"<p>Each code chunk is embedded using a structured template that includes:</p> <pre><code>File: {file_path}\nLanguage: {language}\nType: {semantic_type}\n\n{code_content}\n\nContext:\n- Defined symbols: {symbols}\n- Calls: {function_calls}\n- Imports: {imports}\n</code></pre>"},{"location":"guides/embedding-strategy/#example-function-chunk","title":"Example: Function Chunk","text":"<p>Original Code: <pre><code># src/auth/validators.py\nimport re\nfrom typing import Optional\n\ndef validate_email(email: str) -&gt; bool:\n    \"\"\"Validate email format using regex.\"\"\"\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n</code></pre></p> <p>Embedded Content: <pre><code>File: src/auth/validators.py\nLanguage: python\nType: function\nName: validate_email\n\nimport re\nfrom typing import Optional\n\ndef validate_email(email: str) -&gt; bool:\n    \"\"\"Validate email format using regex.\"\"\"\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None\n\nContext:\n- Defined symbols: validate_email\n- Calls: re.match\n- Imports: re, typing.Optional\n- Parameters: email: str\n- Returns: bool\n</code></pre></p>"},{"location":"guides/embedding-strategy/#metadata-included","title":"Metadata Included","text":"<p>Each chunk stores comprehensive metadata:</p> <pre><code>{\n    # Identity\n    \"id\": \"chunk_uuid_123\",\n    \"file_path\": \"src/auth/validators.py\",\n    \"file_id\": \"file_uuid_456\",\n\n    # Location\n    \"start_line\": 5,\n    \"end_line\": 10,\n    \"start_byte\": 120,\n    \"end_byte\": 350,\n\n    # Content\n    \"content_hash\": \"sha256_hash\",\n    \"language\": \"python\",\n\n    # Semantic Information\n    \"semantic_matches\": [\n        {\n            \"type\": \"function\",\n            \"identifier\": \"validate_email\",\n            \"signature\": \"validate_email(email: str) -&gt; bool\",\n            \"docstring\": \"Validate email format using regex.\",\n            \"parameters\": [\n                {\"name\": \"email\", \"type\": \"str\"}\n            ],\n            \"return_type\": \"bool\"\n        }\n    ],\n\n    # Relationships (from SCIP)\n    \"definitions\": [\n        {\n            \"symbol\": \"validate_email\",\n            \"kind\": \"function\",\n            \"range\": [5, 0, 5, 14]\n        }\n    ],\n\n    \"references\": [\n        {\n            \"symbol\": \"re.match\",\n            \"kind\": \"function_call\",\n            \"range\": [8, 11, 8, 19]\n        }\n    ],\n\n    # Git Metadata\n    \"git_metadata\": {\n        \"last_modified\": \"2024-01-15T10:30:00Z\",\n        \"author\": \"john.doe@example.com\",\n        \"commit_hash\": \"abc123def456\",\n        \"blame_info\": {\n            \"line_5\": {\"author\": \"john.doe\", \"date\": \"2024-01-15\"}\n        }\n    },\n\n    # Tags (from Tree-sitter queries)\n    \"tags\": [\n        \"function_definition\",\n        \"has_docstring\",\n        \"has_type_hints\",\n        \"uses_regex\"\n    ]\n}\n</code></pre>"},{"location":"guides/embedding-strategy/#class-chunk-example","title":"Class Chunk Example","text":"<p>Original Code: <pre><code># src/auth/handlers.py\nfrom .validators import validate_email\n\nclass AuthHandler:\n    \"\"\"Handle authentication operations.\"\"\"\n\n    def __init__(self, db_connection):\n        self.db = db_connection\n\n    def register_user(self, email: str, password: str):\n        \"\"\"Register a new user.\"\"\"\n        if not validate_email(email):\n            raise ValueError(\"Invalid email\")\n        # ... registration logic\n</code></pre></p> <p>Embedded Content: <pre><code>File: src/auth/handlers.py\nLanguage: python\nType: class\nName: AuthHandler\n\nfrom .validators import validate_email\n\nclass AuthHandler:\n    \"\"\"Handle authentication operations.\"\"\"\n\n    def __init__(self, db_connection):\n        self.db = db_connection\n\n    def register_user(self, email: str, password: str):\n        \"\"\"Register a new user.\"\"\"\n        if not validate_email(email):\n            raise ValueError(\"Invalid email\")\n\nContext:\n- Defined symbols: AuthHandler, __init__, register_user\n- Calls: validate_email\n- Imports: .validators.validate_email\n- Methods: __init__, register_user\n- Inherits from: (none)\n</code></pre></p>"},{"location":"guides/embedding-strategy/#relationship-graph","title":"Relationship Graph","text":"<p>Chunks are connected via edges in the graph:</p> <pre><code># Edge types stored in database\n{\n    \"source_id\": \"chunk_auth_handler\",\n    \"target_id\": \"chunk_validate_email\",\n    \"relation_type\": \"calls\",\n    \"metadata\": {\n        \"call_site\": \"src/auth/handlers.py:12\",\n        \"context\": \"register_user method\"\n    }\n}\n\n{\n    \"source_id\": \"chunk_auth_handler\",\n    \"target_id\": \"file_validators\",\n    \"relation_type\": \"imports\",\n    \"metadata\": {\n        \"import_statement\": \"from .validators import validate_email\"\n    }\n}\n</code></pre>"},{"location":"guides/embedding-strategy/#content-enrichment","title":"Content Enrichment","text":"<p>The embedding content is enriched with:</p> <ol> <li>File Context: Path, language, module structure</li> <li>Semantic Type: Function, class, method, variable</li> <li>Symbol Information: Names, signatures, types</li> <li>Relationships: Calls, imports, inheritance</li> <li>Documentation: Docstrings, comments</li> <li>Git History: Author, last modified, blame info</li> <li>Code Patterns: Detected patterns (async, decorators, etc.)</li> </ol>"},{"location":"guides/embedding-strategy/#why-this-matters","title":"Why This Matters","text":"<p>Better Search Results: - Queries like \"email validation\" match both the function name and its purpose - Type information helps find functions with specific signatures - Relationship data enables \"find all callers\" queries</p> <p>Context-Aware Retrieval: - LLMs receive not just code, but understanding of what it does - Import statements help understand dependencies - Docstrings provide natural language descriptions</p> <p>Graph Traversal: - Edges enable finding related code - Call graphs show execution flow - Import graphs show dependencies</p>"},{"location":"guides/embedding-strategy/#customization","title":"Customization","text":"<p>You can customize what gets embedded:</p> <pre><code>from code_graph_indexer.providers.metadata import MetadataProvider\n\n# Custom metadata provider\nclass CustomMetadataProvider(MetadataProvider):\n    def enrich_chunk(self, chunk, file_content):\n        \"\"\"Add custom metadata to chunks.\"\"\"\n        metadata = super().enrich_chunk(chunk, file_content)\n\n        # Add custom fields\n        metadata[\"complexity\"] = self.calculate_complexity(chunk)\n        metadata[\"test_coverage\"] = self.get_coverage(chunk)\n        metadata[\"security_annotations\"] = self.extract_security_info(chunk)\n\n        return metadata\n\n# Use custom provider\nindexer = CodebaseIndexer(\n    repo_path=\"./repo\",\n    metadata_provider=CustomMetadataProvider()\n)\n</code></pre>"},{"location":"guides/embedding-strategy/#staging-pipeline","title":"Staging Pipeline","text":""},{"location":"guides/embedding-strategy/#architecture","title":"Architecture","text":"<p>Embeddings are generated through a staging pipeline to optimize performance and cost:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Chunks    \u2502\n\u2502 (unembedded)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Staging   \u2502  \u2190 Fetch chunks without embeddings\n\u2502   Table     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Hash Check  \u2502  \u2190 Detect duplicates\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500 Duplicate? \u2192 Reuse existing embedding\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Batch     \u2502  \u2190 Group for API efficiency\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Embed API   \u2502  \u2190 Call embedding provider\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Storage   \u2502  \u2190 Save vectors\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/embedding-strategy/#implementation","title":"Implementation","text":"<pre><code>class CodeEmbedder:\n    async def embed_all(self, snapshot_id: str):\n        \"\"\"Generate embeddings for all chunks in a snapshot.\"\"\"\n\n        while True:\n            # Fetch batch of unembedded chunks\n            batch = self.storage.fetch_staging_delta(\n                snapshot_id=snapshot_id,\n                limit=self.batch_size\n            )\n\n            if not batch:\n                break  # All chunks embedded\n\n            # Hash-based deduplication\n            unique_chunks = self._deduplicate(batch)\n\n            # Generate embeddings\n            vectors = await self.provider.embed_async(\n                texts=[chunk.content for chunk in unique_chunks]\n            )\n\n            # Save to database\n            self.storage.save_embeddings_direct(\n                chunks=unique_chunks,\n                vectors=vectors,\n                model_name=self.provider.model_name\n            )\n</code></pre>"},{"location":"guides/embedding-strategy/#deduplication-strategy","title":"Deduplication Strategy","text":""},{"location":"guides/embedding-strategy/#content-hashing","title":"Content Hashing","text":"<p>Identical code chunks share embeddings:</p> <pre><code>import hashlib\n\ndef compute_hash(content: str) -&gt; str:\n    \"\"\"Compute SHA-256 hash of content.\"\"\"\n    return hashlib.sha256(content.encode()).hexdigest()\n\n# Check if embedding exists\nchunk_hash = compute_hash(chunk.content)\nexisting = storage.get_embedding_by_hash(chunk_hash)\n\nif existing:\n    # Reuse existing embedding\n    storage.link_embedding(chunk.id, existing.vector_hash)\nelse:\n    # Generate new embedding\n    vector = await provider.embed(chunk.content)\n    storage.save_embedding(chunk.id, vector, chunk_hash)\n</code></pre>"},{"location":"guides/embedding-strategy/#benefits","title":"Benefits","text":"<ul> <li>Cost Reduction: 30-50% fewer API calls</li> <li>Faster Indexing: Skip redundant embeddings</li> <li>Consistency: Identical code always has identical vectors</li> </ul>"},{"location":"guides/embedding-strategy/#example","title":"Example","text":"<pre><code># File 1: utils.py\ndef validate_email(email):\n    return \"@\" in email\n\n# File 2: helpers.py (copied)\ndef validate_email(email):\n    return \"@\" in email\n</code></pre> <p>Both chunks get the same embedding (only computed once).</p>"},{"location":"guides/embedding-strategy/#batch-processing","title":"Batch Processing","text":""},{"location":"guides/embedding-strategy/#why-batching","title":"Why Batching?","text":"<p>Embedding APIs have: - Rate limits: Max requests/minute - Latency: Network overhead per request - Cost: Charged per token</p> <p>Batching reduces overhead: - 1 request for 100 chunks vs 100 requests - Lower latency (parallel processing) - Better throughput</p>"},{"location":"guides/embedding-strategy/#configuration_1","title":"Configuration","text":"<pre><code>embedder = CodeEmbedder(\n    storage=storage,\n    provider=provider,\n    batch_size=100,        # Chunks per API call\n    max_concurrency=4      # Parallel requests\n)\n</code></pre>"},{"location":"guides/embedding-strategy/#optimal-batch-size","title":"Optimal Batch Size","text":"Repository Size Batch Size Concurrency Throughput Small (&lt;1K chunks) 50 2 ~500 chunks/min Medium (1K-10K) 100 4 ~2K chunks/min Large (&gt;10K) 200 8 ~5K chunks/min <p>Rule of thumb: Larger batches = better throughput, but more memory</p>"},{"location":"guides/embedding-strategy/#async-processing","title":"Async Processing","text":""},{"location":"guides/embedding-strategy/#concurrent-embedding","title":"Concurrent Embedding","text":"<pre><code>import asyncio\n\nasync def embed_concurrent(chunks, provider, max_concurrency=4):\n    \"\"\"Embed chunks with controlled concurrency.\"\"\"\n\n    semaphore = asyncio.Semaphore(max_concurrency)\n\n    async def embed_with_limit(chunk):\n        async with semaphore:\n            return await provider.embed_async(chunk.content)\n\n    # Process all chunks concurrently\n    tasks = [embed_with_limit(chunk) for chunk in chunks]\n    vectors = await asyncio.gather(*tasks)\n\n    return vectors\n</code></pre>"},{"location":"guides/embedding-strategy/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from aiolimiter import AsyncLimiter\n\nclass RateLimitedProvider:\n    def __init__(self, provider, max_rate=100):\n        self.provider = provider\n        self.limiter = AsyncLimiter(max_rate, 60)  # max_rate per minute\n\n    async def embed_async(self, text):\n        async with self.limiter:\n            return await self.provider.embed_async(text)\n</code></pre>"},{"location":"guides/embedding-strategy/#vector-storage","title":"Vector Storage","text":""},{"location":"guides/embedding-strategy/#database-schema","title":"Database Schema","text":"<pre><code>CREATE TABLE node_embeddings (\n    node_id UUID PRIMARY KEY,\n    vector_hash TEXT NOT NULL,\n    embedding vector(1536),  -- pgvector type\n    model_name TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Index for vector similarity search\nCREATE INDEX idx_embeddings_vector \nON node_embeddings \nUSING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n</code></pre>"},{"location":"guides/embedding-strategy/#indexing-strategy","title":"Indexing Strategy","text":"<p>IVFFlat (Inverted File with Flat compression): - Partitions vectors into clusters - Fast approximate search - Good for 10K-1M vectors</p> <p>HNSW (Hierarchical Navigable Small World): - Graph-based index - Faster queries, slower inserts - Good for &gt;1M vectors</p> <pre><code>-- For large datasets (&gt;1M vectors)\nCREATE INDEX idx_embeddings_hnsw \nON node_embeddings \nUSING hnsw (embedding vector_cosine_ops)\nWITH (m = 16, ef_construction = 64);\n</code></pre>"},{"location":"guides/embedding-strategy/#incremental-updates","title":"Incremental Updates","text":""},{"location":"guides/embedding-strategy/#detecting-changes","title":"Detecting Changes","text":"<pre><code>def get_changed_chunks(old_snapshot, new_snapshot):\n    \"\"\"Find chunks that need re-embedding.\"\"\"\n\n    # Get all chunks in new snapshot\n    new_chunks = storage.get_chunks(new_snapshot)\n\n    # Check which ones lack embeddings\n    unembedded = []\n    for chunk in new_chunks:\n        if not storage.has_embedding(chunk.id):\n            unembedded.append(chunk)\n\n    return unembedded\n</code></pre>"},{"location":"guides/embedding-strategy/#reusing-embeddings","title":"Reusing Embeddings","text":"<pre><code>def reuse_embeddings(old_snapshot, new_snapshot):\n    \"\"\"Copy embeddings for unchanged chunks.\"\"\"\n\n    # Find chunks with identical content\n    for old_chunk in storage.get_chunks(old_snapshot):\n        new_chunk = storage.find_chunk_by_hash(\n            new_snapshot,\n            old_chunk.content_hash\n        )\n\n        if new_chunk:\n            # Copy embedding reference\n            storage.copy_embedding(\n                from_chunk=old_chunk.id,\n                to_chunk=new_chunk.id\n            )\n</code></pre> <p>Benefit: Only embed new/changed code (90%+ reuse for typical changes)</p>"},{"location":"guides/embedding-strategy/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/embedding-strategy/#memory-management","title":"Memory Management","text":"<pre><code># Process in chunks to avoid OOM\nasync def embed_large_dataset(chunks, batch_size=1000):\n    for i in range(0, len(chunks), batch_size):\n        batch = chunks[i:i+batch_size]\n        await embed_batch(batch)\n\n        # Free memory\n        del batch\n        gc.collect()\n</code></pre>"},{"location":"guides/embedding-strategy/#caching","title":"Caching","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=10000)\ndef get_embedding_cached(chunk_hash):\n    \"\"\"Cache frequently accessed embeddings.\"\"\"\n    return storage.get_embedding_by_hash(chunk_hash)\n</code></pre>"},{"location":"guides/embedding-strategy/#monitoring","title":"Monitoring","text":"<pre><code># Track embedding progress\nstats = {\n    \"total_chunks\": 0,\n    \"embedded\": 0,\n    \"reused\": 0,\n    \"api_calls\": 0,\n    \"tokens_used\": 0,\n    \"cost_usd\": 0.0\n}\n\nasync for update in embedder.embed_all(snapshot_id):\n    stats[\"embedded\"] = update.processed\n    stats[\"total_chunks\"] = update.total\n\n    print(f\"Progress: {update.processed}/{update.total}\")\n    print(f\"Reused: {stats['reused']} ({stats['reused']/update.total*100:.1f}%)\")\n    print(f\"Cost: ${stats['cost_usd']:.2f}\")\n</code></pre>"},{"location":"guides/embedding-strategy/#cost-optimization","title":"Cost Optimization","text":""},{"location":"guides/embedding-strategy/#strategies","title":"Strategies","text":"<ol> <li>Deduplication: Reuse embeddings for identical code (30-50% savings)</li> <li>Incremental Updates: Only embed changed chunks (90%+ savings on updates)</li> <li>Batch Processing: Reduce API overhead and improve throughput</li> <li>Local Models: Consider for development and testing environments</li> <li>Model Selection: Balance accuracy vs cost for your use case</li> </ol>"},{"location":"guides/embedding-strategy/#best-practices","title":"Best Practices","text":"<p>Minimize API Calls: - Enable content-based deduplication - Use incremental indexing for updates - Batch requests efficiently - Cache embeddings aggressively</p> <p>Optimize Infrastructure: - Use local models for non-production environments - Implement request caching - Monitor usage and set budgets - Consider reserved capacity for predictable workloads</p> <p>Monitor Costs: <pre><code># Track embedding usage\nstats = {\n    \"total_chunks\": 0,\n    \"embedded\": 0,\n    \"reused\": 0,\n    \"api_calls\": 0\n}\n\nasync for update in embedder.embed_all(snapshot_id):\n    stats[\"embedded\"] = update.processed\n    stats[\"total_chunks\"] = update.total\n\n    print(f\"Progress: {update.processed}/{update.total}\")\n    print(f\"Reused: {stats['reused']} ({stats['reused']/update.total*100:.1f}%)\")\n</code></pre></p>"},{"location":"guides/embedding-strategy/#error-handling","title":"Error Handling","text":""},{"location":"guides/embedding-strategy/#retry-logic","title":"Retry Logic","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10)\n)\nasync def embed_with_retry(text, provider):\n    \"\"\"Embed with automatic retries.\"\"\"\n    try:\n        return await provider.embed_async(text)\n    except Exception as e:\n        print(f\"Embedding failed: {e}\")\n        raise\n</code></pre>"},{"location":"guides/embedding-strategy/#partial-failures","title":"Partial Failures","text":"<pre><code>async def embed_batch_safe(chunks, provider):\n    \"\"\"Embed batch with individual error handling.\"\"\"\n\n    results = []\n    for chunk in chunks:\n        try:\n            vector = await provider.embed_async(chunk.content)\n            results.append((chunk.id, vector, None))\n        except Exception as e:\n            # Log error but continue\n            results.append((chunk.id, None, str(e)))\n\n    return results\n</code></pre>"},{"location":"guides/embedding-strategy/#testing","title":"Testing","text":""},{"location":"guides/embedding-strategy/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_deduplication():\n    \"\"\"Test that identical chunks share embeddings.\"\"\"\n\n    chunk1 = ChunkNode(content=\"def foo(): pass\")\n    chunk2 = ChunkNode(content=\"def foo(): pass\")\n\n    embedder.embed([chunk1, chunk2])\n\n    # Should only call API once\n    assert provider.embed_async.call_count == 1\n\n    # Should have same embedding\n    emb1 = storage.get_embedding(chunk1.id)\n    emb2 = storage.get_embedding(chunk2.id)\n    assert emb1.vector_hash == emb2.vector_hash\n</code></pre>"},{"location":"guides/embedding-strategy/#integration-tests","title":"Integration Tests","text":"<pre><code>async def test_full_embedding_pipeline():\n    \"\"\"Test complete embedding workflow.\"\"\"\n\n    # Index repository\n    indexer.index(repo_url, branch)\n\n    # Generate embeddings\n    await embedder.embed_all(snapshot_id)\n\n    # Verify all chunks have embeddings\n    chunks = storage.get_chunks(snapshot_id)\n    for chunk in chunks:\n        assert storage.has_embedding(chunk.id)\n</code></pre>"},{"location":"guides/embedding-strategy/#next-steps","title":"Next Steps","text":"<ul> <li>Retrieval Strategies: Using embeddings for search</li> <li>API Reference: Complete embedding API</li> <li>Production Deployment: Scaling embedding generation</li> </ul>"},{"location":"guides/indexing-pipeline/","title":"Indexing Pipeline","text":"<p>This guide explains the complete indexing workflow, from source code to queryable knowledge graph.</p>"},{"location":"guides/indexing-pipeline/#overview","title":"Overview","text":"<p>The indexing pipeline transforms raw source code into a structured, searchable knowledge graph through multiple stages:</p> <pre><code>Source Files \u2192 Parsing \u2192 Chunking \u2192 Relation Extraction \u2192 Embedding \u2192 Storage\n</code></pre> <p>Each stage is designed for performance, accuracy, and scalability.</p>"},{"location":"guides/indexing-pipeline/#stage-1-repository-preparation","title":"Stage 1: Repository Preparation","text":""},{"location":"guides/indexing-pipeline/#git-volume-management","title":"Git Volume Management","text":"<p>The <code>GitVolumeManager</code> handles repository cloning and updates:</p> <pre><code>from code_graph_indexer.volume_manager import GitVolumeManager\n\nmanager = GitVolumeManager(workspace_dir=\"./workspace\")\n\n# Clone or update repository\nmanager.ensure_repo_updated(\n    repo_url=\"https://github.com/org/repo\",\n    branch=\"main\"\n)\n\n# Get file list (respects .gitignore)\nfiles = manager.files(repo_path)\n</code></pre> <p>Key Features: - Shallow clones for faster downloads - Respects <code>.gitignore</code> patterns - Filters binary files automatically - Caches repositories for reuse</p> <p>Performance: ~5s for initial clone, ~1s for updates</p>"},{"location":"guides/indexing-pipeline/#stage-2-tree-sitter-parsing","title":"Stage 2: Tree-sitter Parsing","text":""},{"location":"guides/indexing-pipeline/#ast-generation","title":"AST Generation","text":"<p>Tree-sitter creates an Abstract Syntax Tree for each file:</p> <pre><code>from code_graph_indexer.parsing import TreeSitterRepoParser\n\nparser = TreeSitterRepoParser(repo_path=\"./repo\")\n\n# Parse a single file\nchunks, relations = parser.parse_file(\n    file_path=\"src/main.py\",\n    file_id=\"file_123\"\n)\n</code></pre>"},{"location":"guides/indexing-pipeline/#why-tree-sitter","title":"Why Tree-sitter?","text":"Feature Benefit Zero-copy No string allocations during parsing Incremental Reparse only changed sections Error-tolerant Handles syntax errors gracefully Multi-language Unified API for 40+ languages"},{"location":"guides/indexing-pipeline/#supported-languages","title":"Supported Languages","text":"<ul> <li>Python</li> <li>TypeScript/JavaScript</li> <li>Go</li> <li>Java</li> <li>Rust</li> <li>C/C++</li> <li>Ruby</li> <li>PHP</li> </ul>"},{"location":"guides/indexing-pipeline/#stage-3-semantic-chunking","title":"Stage 3: Semantic Chunking","text":""},{"location":"guides/indexing-pipeline/#chunking-strategy","title":"Chunking Strategy","text":"<p>Code is split into semantic chunks that respect code structure:</p> <pre><code># Configuration\nCHUNK_SIZE = 512        # Target tokens per chunk\nCHUNK_OVERLAP = 50      # Overlap for context\nMAX_CHUNK_SIZE = 2048   # Hard limit\n</code></pre>"},{"location":"guides/indexing-pipeline/#chunking-rules","title":"Chunking Rules","text":"<ol> <li>Respect Boundaries: Never split mid-function or mid-class</li> <li>Preserve Context: Include overlap from previous chunk</li> <li>Handle Large Nodes: Recursively split oversized functions</li> <li>Maintain Metadata: Track semantic type (function, class, etc.)</li> </ol>"},{"location":"guides/indexing-pipeline/#example","title":"Example","text":"<pre><code># Original code\ndef process_data(data):\n    \"\"\"Process incoming data.\"\"\"\n    validated = validate(data)\n    transformed = transform(validated)\n    return save(transformed)\n\ndef validate(data):\n    \"\"\"Validate data structure.\"\"\"\n    # ... implementation\n</code></pre> <p>Chunks Created:</p> <p>Chunk 1 (function: <code>process_data</code>): <pre><code>def process_data(data):\n    \"\"\"Process incoming data.\"\"\"\n    validated = validate(data)\n    transformed = transform(validated)\n    return save(transformed)\n</code></pre> Metadata: <code>{type: \"function\", name: \"process_data\", calls: [\"validate\", \"transform\", \"save\"]}</code></p> <p>Chunk 2 (function: <code>validate</code>, with overlap): <pre><code># Overlap from previous chunk\n    return save(transformed)\n\ndef validate(data):\n    \"\"\"Validate data structure.\"\"\"\n    # ... implementation\n</code></pre> Metadata: <code>{type: \"function\", name: \"validate\"}</code></p>"},{"location":"guides/indexing-pipeline/#chunking-algorithm","title":"Chunking Algorithm","text":"<pre><code>def _process_scope(node, content, file_path, parent_id):\n    \"\"\"Recursively process AST nodes into chunks.\"\"\"\n\n    # Extract semantic information\n    semantic_matches = _extract_tags(node, content)\n\n    # Check size\n    node_size = node.end_byte - node.start_byte\n\n    if node_size &lt;= MAX_CHUNK_SIZE:\n        # Create single chunk\n        chunk = ChunkNode(\n            content=content[node.start_byte:node.end_byte],\n            metadata={\"semantic_matches\": semantic_matches},\n            file_path=file_path,\n            parent_id=parent_id\n        )\n        return [chunk]\n    else:\n        # Recursively split children\n        chunks = []\n        for child in node.children:\n            chunks.extend(_process_scope(child, content, file_path, parent_id))\n        return chunks\n</code></pre>"},{"location":"guides/indexing-pipeline/#metadata-extraction","title":"Metadata Extraction","text":"<p>During parsing, rich metadata is extracted from each chunk:</p>"},{"location":"guides/indexing-pipeline/#tree-sitter-queries","title":"Tree-sitter Queries","text":"<p>Custom queries extract semantic information:</p> <pre><code>; Function definitions\n(function_definition\n  name: (identifier) @function.name\n  parameters: (parameters) @function.params\n  return_type: (type)? @function.return\n  body: (block) @function.body\n  (#set! \"type\" \"function\"))\n\n; Class definitions\n(class_definition\n  name: (identifier) @class.name\n  superclasses: (argument_list)? @class.bases\n  body: (block) @class.body\n  (#set! \"type\" \"class\"))\n\n; Method calls\n(call\n  function: (attribute\n    object: (identifier) @call.object\n    attribute: (identifier) @call.method))\n</code></pre>"},{"location":"guides/indexing-pipeline/#extracted-information","title":"Extracted Information","text":"<p>For each chunk, we extract:</p> <p>1. Semantic Type: <pre><code>{\n    \"type\": \"function\",  # or \"class\", \"method\", \"variable\"\n    \"identifier\": \"process_request\",\n    \"signature\": \"process_request(data: Dict) -&gt; Response\"\n}\n</code></pre></p> <p>2. Documentation: <pre><code>{\n    \"docstring\": \"Process incoming HTTP request and return response.\",\n    \"has_docstring\": True,\n    \"docstring_lines\": 3\n}\n</code></pre></p> <p>3. Type Information: <pre><code>{\n    \"parameters\": [\n        {\"name\": \"data\", \"type\": \"Dict\", \"has_default\": False},\n        {\"name\": \"timeout\", \"type\": \"int\", \"has_default\": True, \"default\": \"30\"}\n    ],\n    \"return_type\": \"Response\",\n    \"has_type_hints\": True\n}\n</code></pre></p> <p>4. Relationships: <pre><code>{\n    \"calls\": [\"validate_data\", \"transform_response\"],\n    \"imports\": [\"typing.Dict\", \"models.Response\"],\n    \"decorators\": [\"@app.route\", \"@require_auth\"],\n    \"raises\": [\"ValueError\", \"TimeoutError\"]\n}\n</code></pre></p> <p>5. Code Patterns: <pre><code>{\n    \"is_async\": False,\n    \"is_generator\": False,\n    \"is_property\": False,\n    \"is_static\": False,\n    \"is_classmethod\": False,\n    \"uses_context_manager\": True,\n    \"has_error_handling\": True\n}\n</code></pre></p> <p>6. Complexity Metrics: <pre><code>{\n    \"lines_of_code\": 25,\n    \"cyclomatic_complexity\": 4,\n    \"nesting_depth\": 2,\n    \"num_parameters\": 2,\n    \"num_return_statements\": 3\n}\n</code></pre></p>"},{"location":"guides/indexing-pipeline/#complete-metadata-example","title":"Complete Metadata Example","text":"<pre><code>{\n    # Identity\n    \"id\": \"node_abc123\",\n    \"chunk_id\": \"chunk_def456\",\n    \"file_id\": \"file_789\",\n\n    # Location\n    \"file_path\": \"src/api/handlers.py\",\n    \"start_line\": 45,\n    \"end_line\": 70,\n    \"start_byte\": 1200,\n    \"end_byte\": 1850,\n\n    # Content\n    \"content_hash\": \"sha256:abc...\",\n    \"language\": \"python\",\n    \"size_bytes\": 650,\n\n    # Semantic Information\n    \"semantic_matches\": [\n        {\n            \"type\": \"function\",\n            \"identifier\": \"process_request\",\n            \"signature\": \"process_request(data: Dict, timeout: int = 30) -&gt; Response\",\n            \"docstring\": \"Process incoming HTTP request and return response.\",\n            \"parameters\": [\n                {\"name\": \"data\", \"type\": \"Dict\"},\n                {\"name\": \"timeout\", \"type\": \"int\", \"default\": \"30\"}\n            ],\n            \"return_type\": \"Response\",\n            \"decorators\": [\"@app.route('/api/process')\", \"@require_auth\"],\n            \"is_async\": False\n        }\n    ],\n\n    # Relationships\n    \"calls\": [\n        {\"name\": \"validate_data\", \"line\": 48},\n        {\"name\": \"transform_response\", \"line\": 65}\n    ],\n    \"imports\": [\n        {\"module\": \"typing\", \"name\": \"Dict\"},\n        {\"module\": \"models\", \"name\": \"Response\"}\n    ],\n    \"raises\": [\"ValueError\", \"TimeoutError\"],\n\n    # Code Quality\n    \"has_docstring\": True,\n    \"has_type_hints\": True,\n    \"has_error_handling\": True,\n    \"cyclomatic_complexity\": 4,\n\n    # Git Metadata (from MetadataProvider)\n    \"git_metadata\": {\n        \"last_modified\": \"2024-01-15T10:30:00Z\",\n        \"author\": \"john.doe@example.com\",\n        \"commit_hash\": \"abc123\",\n        \"file_category\": \"code\"  # or \"test\", \"config\", \"docs\"\n    },\n\n    # Tags\n    \"tags\": [\n        \"function_definition\",\n        \"has_decorators\",\n        \"has_type_hints\",\n        \"api_endpoint\",\n        \"requires_auth\"\n    ]\n}\n</code></pre> <p>This rich metadata enables: - Precise Search: Find functions by signature, decorators, or patterns - Quality Analysis: Filter by code quality metrics - Impact Analysis: Understand relationships and dependencies - Context-Aware Retrieval: Provide LLMs with comprehensive information</p>"},{"location":"guides/indexing-pipeline/#stage-4-scip-relation-extraction","title":"Stage 4: SCIP Relation Extraction","text":""},{"location":"guides/indexing-pipeline/#what-is-scip","title":"What is SCIP?","text":"<p>SCIP (SCIP Code Intelligence Protocol) is an industry-standard format for representing code relationships.</p>"},{"location":"guides/indexing-pipeline/#relation-types","title":"Relation Types","text":"Relation Description Example Definition Where a symbol is defined <code>def foo():</code> Reference Where a symbol is used <code>result = foo()</code> Call Function invocation <code>foo()</code> Inheritance Class hierarchy <code>class Child(Parent):</code> Import Module dependencies <code>from x import y</code>"},{"location":"guides/indexing-pipeline/#scip-indexing-process","title":"SCIP Indexing Process","text":"<pre><code>from code_graph_indexer.graph.indexers.scip import SCIPRunner\n\nrunner = SCIPRunner(repo_path=\"./repo\")\n\n# Generate SCIP index\nindices = runner.prepare_indices()\n\n# Stream documents\nfor doc_wrapper in runner.stream_documents(indices):\n    # Process definitions\n    definitions = extract_definitions(doc_wrapper)\n\n    # Process references\n    references = extract_references(doc_wrapper)\n\n    # Store in graph\n    storage.ingest_scip_relations(definitions, references)\n</code></pre>"},{"location":"guides/indexing-pipeline/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Streaming: Process large indices without loading into memory</li> <li>Batching: Bulk insert relations using COPY protocol</li> <li>Parallel: Run SCIP indexing concurrently with Tree-sitter parsing</li> </ul> <p>Performance: ~100K relations/second on modern hardware</p>"},{"location":"guides/indexing-pipeline/#stage-5-embedding-generation","title":"Stage 5: Embedding Generation","text":""},{"location":"guides/indexing-pipeline/#embedding-pipeline","title":"Embedding Pipeline","text":"<pre><code>from code_graph_indexer.providers.embedding import CodeEmbedder\n\nembedder = CodeEmbedder(\n    storage=storage,\n    provider=embedding_provider,\n    batch_size=100,\n    max_concurrency=4\n)\n\n# Generate embeddings for new chunks\nasync for update in embedder.embed_all(snapshot_id):\n    print(f\"Embedded {update.processed}/{update.total} chunks\")\n</code></pre>"},{"location":"guides/indexing-pipeline/#staging-strategy","title":"Staging Strategy","text":"<p>Embeddings are generated in a staging pipeline:</p> <ol> <li>Fetch: Get chunks without embeddings</li> <li>Hash: Compute content hash to detect duplicates</li> <li>Batch: Group chunks for efficient API calls</li> <li>Embed: Call embedding provider (OpenAI, Cohere, local)</li> <li>Store: Save vectors to database</li> </ol>"},{"location":"guides/indexing-pipeline/#deduplication","title":"Deduplication","text":"<p>Chunks with identical content share embeddings:</p> <pre><code># Hash-based deduplication\nchunk_hash = hashlib.sha256(content.encode()).hexdigest()\n\n# Check if embedding exists\nexisting = storage.get_embedding_by_hash(chunk_hash)\n\nif existing:\n    # Reuse existing embedding\n    storage.link_embedding(chunk_id, existing.vector_hash)\nelse:\n    # Generate new embedding\n    vector = await provider.embed(content)\n    storage.save_embedding(chunk_id, vector, chunk_hash)\n</code></pre> <p>Benefit: 30-50% reduction in embedding API calls for typical codebases</p>"},{"location":"guides/indexing-pipeline/#stage-6-storage","title":"Stage 6: Storage","text":""},{"location":"guides/indexing-pipeline/#database-schema","title":"Database Schema","text":"<pre><code>-- Snapshots: Atomic versions\nCREATE TABLE snapshots (\n    id UUID PRIMARY KEY,\n    repository_id UUID,\n    commit_hash TEXT,\n    status TEXT,\n    created_at TIMESTAMP\n);\n\n-- Nodes: Code chunks\nCREATE TABLE nodes (\n    id UUID PRIMARY KEY,\n    snapshot_id UUID,\n    file_path TEXT,\n    content_hash TEXT,\n    start_line INT,\n    end_line INT,\n    metadata JSONB\n);\n\n-- Edges: Relationships\nCREATE TABLE edges (\n    source_id UUID,\n    target_id UUID,\n    relation_type TEXT,\n    metadata JSONB\n);\n\n-- Embeddings: Vectors\nCREATE TABLE node_embeddings (\n    node_id UUID,\n    vector_hash TEXT,\n    embedding vector(1536),\n    model_name TEXT\n);\n</code></pre>"},{"location":"guides/indexing-pipeline/#copy-protocol","title":"COPY Protocol","text":"<p>Bulk inserts use PostgreSQL's COPY protocol for maximum performance:</p> <pre><code>def add_nodes_fast(nodes: List[ChunkNode]):\n    \"\"\"Insert nodes using COPY protocol (10x-50x faster).\"\"\"\n\n    sql = \"\"\"\n        COPY nodes (id, file_path, content_hash, start_line, end_line, metadata)\n        FROM STDIN\n    \"\"\"\n\n    with conn.cursor() as cur:\n        with cur.copy(sql) as copy:\n            for node in nodes:\n                copy.write_row((\n                    node.id,\n                    node.file_path,\n                    node.content_hash,\n                    node.start_line,\n                    node.end_line,\n                    json.dumps(node.metadata)\n                ))\n</code></pre> <p>Performance: ~100K nodes/second vs ~2K nodes/second with INSERT</p>"},{"location":"guides/indexing-pipeline/#complete-workflow","title":"Complete Workflow","text":""},{"location":"guides/indexing-pipeline/#indexing-a-repository","title":"Indexing a Repository","text":"<pre><code>from code_graph_indexer import CodebaseIndexer\n\nindexer = CodebaseIndexer(\n    repo_path=\"./repo\",\n    storage_connector=connector,\n    max_workers=8  # Parallel processing\n)\n\n# Full indexing workflow\nindexer.index(\n    repo_url=\"https://github.com/org/repo\",\n    branch=\"main\"\n)\n</code></pre>"},{"location":"guides/indexing-pipeline/#internal-steps","title":"Internal Steps","text":"<ol> <li>Create Snapshot: Atomic version for this index</li> <li>Clone/Update: Ensure repository is current</li> <li>Parse Files: Tree-sitter + SCIP in parallel</li> <li>Store Graph: Bulk insert nodes and edges</li> <li>Generate Embeddings: Async batch processing</li> <li>Activate Snapshot: Atomic swap to new version</li> </ol>"},{"location":"guides/indexing-pipeline/#progress-tracking","title":"Progress Tracking","text":"<pre><code># Monitor indexing progress\nfor progress in indexer.index_with_progress(repo_url, branch):\n    print(f\"Stage: {progress.stage}\")\n    print(f\"Progress: {progress.current}/{progress.total}\")\n    print(f\"ETA: {progress.eta}\")\n</code></pre>"},{"location":"guides/indexing-pipeline/#performance-tuning","title":"Performance Tuning","text":""},{"location":"guides/indexing-pipeline/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Adjust based on CPU cores\nindexer = CodebaseIndexer(\n    max_workers=os.cpu_count()  # Default: 8\n)\n</code></pre>"},{"location":"guides/indexing-pipeline/#batch-sizes","title":"Batch Sizes","text":"<pre><code># Larger batches = fewer API calls, more memory\nembedder = CodeEmbedder(\n    batch_size=200,  # Default: 100\n    max_concurrency=8  # Default: 4\n)\n</code></pre>"},{"location":"guides/indexing-pipeline/#database-tuning","title":"Database Tuning","text":"<pre><code>-- Increase work memory for bulk operations\nSET work_mem = '256MB';\n\n-- Disable auto-vacuum during indexing\nALTER TABLE nodes SET (autovacuum_enabled = false);\n\n-- Re-enable after indexing\nALTER TABLE nodes SET (autovacuum_enabled = true);\nVACUUM ANALYZE nodes;\n</code></pre>"},{"location":"guides/indexing-pipeline/#incremental-updates","title":"Incremental Updates","text":""},{"location":"guides/indexing-pipeline/#detecting-changes","title":"Detecting Changes","text":"<pre><code># Only reindex changed files\nchanged_files = indexer.detect_changes(\n    old_commit=\"abc123\",\n    new_commit=\"def456\"\n)\n\n# Reindex only changed files\nindexer.reindex_files(changed_files, snapshot_id)\n</code></pre>"},{"location":"guides/indexing-pipeline/#snapshot-strategy","title":"Snapshot Strategy","text":"<ul> <li>Full Reindex: Create new snapshot, index all files</li> <li>Incremental: Reuse existing snapshot, update changed files</li> <li>Atomic Swap: Switch to new snapshot when complete</li> </ul> <p>Benefit: 10x-100x faster for small changes</p>"},{"location":"guides/indexing-pipeline/#monitoring","title":"Monitoring","text":""},{"location":"guides/indexing-pipeline/#metrics-to-track","title":"Metrics to Track","text":"<pre><code># Indexing stats\nstats = indexer.get_stats()\nprint(f\"Files indexed: {stats['files_indexed']}\")\nprint(f\"Nodes created: {stats['total_nodes']}\")\nprint(f\"Edges created: {stats['total_edges']}\")\nprint(f\"Embeddings generated: {stats['embeddings_count']}\")\nprint(f\"Duration: {stats['duration_seconds']}s\")\n</code></pre>"},{"location":"guides/indexing-pipeline/#common-issues","title":"Common Issues","text":"Issue Cause Solution Slow parsing Large files Increase <code>max_workers</code> OOM errors Large batches Reduce <code>batch_size</code> Slow embeddings API rate limits Reduce <code>max_concurrency</code> Slow inserts Not using COPY Enable <code>use_copy_protocol=True</code>"},{"location":"guides/indexing-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Embedding Strategy: Deep dive into vector generation</li> <li>Retrieval Strategies: Using the indexed graph</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"guides/retrieval-strategies/","title":"Retrieval Strategies","text":"<p>This guide explains how to query the indexed code graph using hybrid search, graph traversal, and context expansion.</p>"},{"location":"guides/retrieval-strategies/#overview","title":"Overview","text":"<p>The retrieval system combines three complementary approaches:</p> <ol> <li>Vector Search: Semantic similarity using embeddings</li> <li>Full-Text Search: Keyword matching with PostgreSQL FTS</li> <li>Graph Traversal: Navigate relationships (calls, references, inheritance)</li> </ol> <p>These are combined using Reciprocal Rank Fusion (RRF) for optimal results.</p>"},{"location":"guides/retrieval-strategies/#architecture","title":"Architecture","text":"<pre><code>Query \u2192 Multi-Stage Pipeline \u2192 Results\n\nStage 1: Resolution\n  \u251c\u2500 Parse query intent\n  \u251c\u2500 Extract keywords\n  \u2514\u2500 Identify target types (function, class, etc.)\n\nStage 2: Search\n  \u251c\u2500 Vector Search (semantic)\n  \u251c\u2500 Full-Text Search (keywords)\n  \u2514\u2500 RRF Fusion\n\nStage 3: Expansion\n  \u251c\u2500 Graph Traversal\n  \u251c\u2500 Context Gathering\n  \u2514\u2500 Ranking\n</code></pre>"},{"location":"guides/retrieval-strategies/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/retrieval-strategies/#simple-search","title":"Simple Search","text":"<pre><code>from code_graph_indexer import CodeRetriever\n\nretriever = CodeRetriever(connector)\n\n# Search with default settings\nresults = retriever.search(\n    query=\"authentication middleware implementation\",\n    limit=10\n)\n\nfor result in results:\n    print(f\"{result['file_path']}:{result['start_line']}\")\n    print(result['content'])\n    print(f\"Score: {result['score']}\\n\")\n</code></pre>"},{"location":"guides/retrieval-strategies/#hybrid-search","title":"Hybrid Search","text":"<pre><code># Combine vector + keyword search\nresults = retriever.search(\n    query=\"JWT token validation\",\n    limit=10,\n    search_mode=\"hybrid\",  # \"vector\", \"keyword\", or \"hybrid\"\n    rrf_k=60  # RRF parameter (default: 60)\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#vector-search","title":"Vector Search","text":""},{"location":"guides/retrieval-strategies/#how-it-works","title":"How It Works","text":"<ol> <li>Query Embedding: Convert query to vector</li> <li>Similarity Search: Find nearest neighbors using cosine similarity</li> <li>Ranking: Order by similarity score</li> </ol> <pre><code># Pure vector search\nresults = retriever.search(\n    query=\"handle user authentication\",\n    search_mode=\"vector\",\n    limit=20\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#similarity-metrics","title":"Similarity Metrics","text":"Metric Formula Use Case Cosine <code>1 - (A\u00b7B)/(\u2016A\u2016\u2016B\u2016)</code> Default, best for code L2 Distance <code>\u2016A-B\u2016\u00b2</code> Absolute distance Inner Product <code>A\u00b7B</code> Normalized vectors <p>Default: Cosine similarity (best for semantic code search)</p>"},{"location":"guides/retrieval-strategies/#performance","title":"Performance","text":"<pre><code>-- Vector index for fast search\nCREATE INDEX idx_embeddings_vector \nON node_embeddings \nUSING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- Query performance\nEXPLAIN ANALYZE\nSELECT node_id, embedding &lt;=&gt; query_vector AS distance\nFROM node_embeddings\nORDER BY distance\nLIMIT 10;\n</code></pre> <p>Typical performance: &lt;50ms for 100K vectors</p>"},{"location":"guides/retrieval-strategies/#full-text-search","title":"Full-Text Search","text":""},{"location":"guides/retrieval-strategies/#postgresql-fts","title":"PostgreSQL FTS","text":"<p>Uses PostgreSQL's built-in full-text search with custom configurations:</p> <pre><code>-- Create FTS index\nCREATE INDEX idx_contents_fts \nON contents \nUSING gin(to_tsvector('english', content));\n\n-- Search query\nSELECT node_id, ts_rank(to_tsvector('english', content), query) AS rank\nFROM contents\nWHERE to_tsvector('english', content) @@ plainto_tsquery('english', 'authentication JWT')\nORDER BY rank DESC;\n</code></pre>"},{"location":"guides/retrieval-strategies/#advanced-fts","title":"Advanced FTS","text":"<pre><code># Keyword search with filters\nresults = retriever.search(\n    query=\"validate email\",\n    search_mode=\"keyword\",\n    filters={\n        \"file_path\": \"src/auth/\",  # Path prefix\n        \"language\": \"python\",\n        \"type\": \"function\"\n    }\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":""},{"location":"guides/retrieval-strategies/#algorithm","title":"Algorithm","text":"<p>RRF combines rankings from multiple sources:</p> <pre><code>RRF_score(d) = \u03a3 1 / (k + rank_i(d))\n</code></pre> <p>Where: - <code>d</code> = document - <code>k</code> = constant (default: 60) - <code>rank_i(d)</code> = rank of document in source i</p>"},{"location":"guides/retrieval-strategies/#example","title":"Example","text":"<p>Vector Search Results: 1. doc_A (rank 1) 2. doc_B (rank 2) 3. doc_C (rank 3)</p> <p>Keyword Search Results: 1. doc_B (rank 1) 2. doc_C (rank 2) 3. doc_D (rank 3)</p> <p>RRF Scores (k=60): - doc_A: 1/(60+1) = 0.0164 - doc_B: 1/(60+1) + 1/(60+1) = 0.0328 \u2190 Best - doc_C: 1/(60+3) + 1/(60+2) = 0.0317 - doc_D: 1/(60+3) = 0.0159</p> <p>Final Ranking: B, C, A, D</p>"},{"location":"guides/retrieval-strategies/#implementation","title":"Implementation","text":"<pre><code>def reciprocal_rank_fusion(rankings: List[List[str]], k: int = 60) -&gt; List[str]:\n    \"\"\"Combine multiple rankings using RRF.\"\"\"\n    scores = {}\n\n    for ranking in rankings:\n        for rank, doc_id in enumerate(ranking, start=1):\n            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank)\n\n    # Sort by score descending\n    return sorted(scores.keys(), key=lambda d: scores[d], reverse=True)\n</code></pre>"},{"location":"guides/retrieval-strategies/#graph-traversal","title":"Graph Traversal","text":""},{"location":"guides/retrieval-strategies/#navigation-patterns","title":"Navigation Patterns","text":"<pre><code>from code_graph_indexer import CodeNavigator\n\nnavigator = CodeNavigator(connector)\n\n# Find all callers of a function\ncallers = navigator.get_incoming_calls(node_id=\"func_123\")\n\n# Find all functions called by this function\ncallees = navigator.get_outgoing_calls(node_id=\"func_123\")\n\n# Get class hierarchy\nhierarchy = navigator.get_class_hierarchy(class_name=\"BaseHandler\")\n\n# Find all implementations of an interface\nimplementations = navigator.find_implementations(\"IHandler\")\n</code></pre>"},{"location":"guides/retrieval-strategies/#context-expansion","title":"Context Expansion","text":"<p>Automatically include related code:</p> <pre><code># Search with context expansion\nresults = retriever.search(\n    query=\"authentication flow\",\n    limit=10,\n    include_context=True,\n    context_depth=2  # How many hops in the graph\n)\n\n# Each result includes:\n# - The matched chunk\n# - Callers (who calls this)\n# - Callees (what this calls)\n# - Definitions (symbols used)\n# - Related chunks (same file/class)\n</code></pre>"},{"location":"guides/retrieval-strategies/#graph-queries","title":"Graph Queries","text":"<pre><code># Custom graph traversal\ndef find_all_dependencies(node_id: str, max_depth: int = 3):\n    \"\"\"Find all transitive dependencies.\"\"\"\n    visited = set()\n    queue = [(node_id, 0)]\n    dependencies = []\n\n    while queue:\n        current, depth = queue.pop(0)\n        if current in visited or depth &gt; max_depth:\n            continue\n\n        visited.add(current)\n\n        # Get outgoing calls\n        callees = navigator.get_outgoing_calls(current)\n        dependencies.extend(callees)\n\n        # Add to queue\n        for callee in callees:\n            queue.append((callee['id'], depth + 1))\n\n    return dependencies\n</code></pre>"},{"location":"guides/retrieval-strategies/#multi-stage-retrieval","title":"Multi-Stage Retrieval","text":""},{"location":"guides/retrieval-strategies/#complete-pipeline","title":"Complete Pipeline","text":"<pre><code>class SearchExecutor:\n    def search(self, query: str, limit: int = 10):\n        \"\"\"Multi-stage retrieval pipeline.\"\"\"\n\n        # Stage 1: Resolution\n        intent = self._parse_query(query)\n\n        # Stage 2: Search\n        vector_results = self._vector_search(query, limit * 2)\n        keyword_results = self._keyword_search(query, limit * 2)\n\n        # Combine with RRF\n        combined = self._rrf_fusion([vector_results, keyword_results])\n\n        # Stage 3: Expansion\n        expanded = self._expand_context(combined[:limit])\n\n        # Stage 4: Ranking\n        final = self._rerank(expanded, query)\n\n        return final[:limit]\n</code></pre>"},{"location":"guides/retrieval-strategies/#query-intent-detection","title":"Query Intent Detection","text":"<pre><code>def _parse_query(self, query: str) -&gt; Dict:\n    \"\"\"Extract intent from query.\"\"\"\n\n    intent = {\n        \"type\": None,  # function, class, variable\n        \"action\": None,  # find, implement, debug\n        \"keywords\": [],\n        \"filters\": {}\n    }\n\n    # Detect type\n    if \"function\" in query.lower() or \"def \" in query:\n        intent[\"type\"] = \"function\"\n    elif \"class\" in query.lower():\n        intent[\"type\"] = \"class\"\n\n    # Detect action\n    if any(word in query.lower() for word in [\"implement\", \"create\", \"build\"]):\n        intent[\"action\"] = \"implement\"\n    elif any(word in query.lower() for word in [\"find\", \"search\", \"locate\"]):\n        intent[\"action\"] = \"find\"\n\n    # Extract keywords\n    intent[\"keywords\"] = self._extract_keywords(query)\n\n    return intent\n</code></pre>"},{"location":"guides/retrieval-strategies/#advanced-filtering","title":"Advanced Filtering","text":""},{"location":"guides/retrieval-strategies/#path-filters","title":"Path Filters","text":"<pre><code># Search within specific directories\nresults = retriever.search(\n    query=\"database connection\",\n    filters={\n        \"path_prefix\": \"src/db/\",\n        \"exclude_patterns\": [\"**/test_*.py\", \"**/__pycache__\"]\n    }\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#metadata-filters","title":"Metadata Filters","text":"<pre><code># Filter by semantic type\nresults = retriever.search(\n    query=\"validation logic\",\n    filters={\n        \"semantic_type\": \"function\",\n        \"has_docstring\": True,\n        \"min_lines\": 10,\n        \"max_lines\": 100\n    }\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#language-filters","title":"Language Filters","text":"<pre><code># Search only Python files\nresults = retriever.search(\n    query=\"async request handler\",\n    filters={\"language\": \"python\"}\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/retrieval-strategies/#caching","title":"Caching","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef search_cached(query: str, limit: int):\n    \"\"\"Cache frequent queries.\"\"\"\n    return retriever.search(query, limit)\n</code></pre>"},{"location":"guides/retrieval-strategies/#batch-queries","title":"Batch Queries","text":"<pre><code># Search multiple queries efficiently\nqueries = [\n    \"authentication flow\",\n    \"database connection\",\n    \"error handling\"\n]\n\nresults = retriever.search_batch(queries, limit=10)\n</code></pre>"},{"location":"guides/retrieval-strategies/#index-optimization","title":"Index Optimization","text":"<pre><code>-- Optimize vector index\nVACUUM ANALYZE node_embeddings;\n\n-- Rebuild FTS index\nREINDEX INDEX idx_contents_fts;\n\n-- Update statistics\nANALYZE contents;\n</code></pre>"},{"location":"guides/retrieval-strategies/#monitoring","title":"Monitoring","text":""},{"location":"guides/retrieval-strategies/#query-performance","title":"Query Performance","text":"<pre><code>import time\n\ndef search_with_metrics(query: str):\n    \"\"\"Track search performance.\"\"\"\n    start = time.time()\n\n    results = retriever.search(query, limit=10)\n\n    duration = time.time() - start\n\n    print(f\"Query: {query}\")\n    print(f\"Results: {len(results)}\")\n    print(f\"Duration: {duration*1000:.2f}ms\")\n\n    return results\n</code></pre>"},{"location":"guides/retrieval-strategies/#common-metrics","title":"Common Metrics","text":"Metric Target Description Latency &lt;100ms Time to return results Precision@10 &gt;0.8 Relevant results in top 10 Recall@10 &gt;0.6 Found relevant results Cache Hit Rate &gt;70% Cached query reuse"},{"location":"guides/retrieval-strategies/#best-practices","title":"Best Practices","text":""},{"location":"guides/retrieval-strategies/#1-use-hybrid-search","title":"1. Use Hybrid Search","text":"<p>Combine vector + keyword for best results: <pre><code>results = retriever.search(query, search_mode=\"hybrid\")\n</code></pre></p>"},{"location":"guides/retrieval-strategies/#2-enable-context-expansion","title":"2. Enable Context Expansion","text":"<p>Include related code for better understanding: <pre><code>results = retriever.search(query, include_context=True)\n</code></pre></p>"},{"location":"guides/retrieval-strategies/#3-filter-aggressively","title":"3. Filter Aggressively","text":"<p>Narrow search scope for faster results: <pre><code>results = retriever.search(\n    query,\n    filters={\"path_prefix\": \"src/\", \"language\": \"python\"}\n)\n</code></pre></p>"},{"location":"guides/retrieval-strategies/#4-tune-rrf-parameter","title":"4. Tune RRF Parameter","text":"<p>Adjust <code>k</code> based on your needs: - Lower k (30-40): Favor top-ranked results - Higher k (80-100): More balanced fusion</p>"},{"location":"guides/retrieval-strategies/#5-monitor-performance","title":"5. Monitor Performance","text":"<p>Track query latency and optimize indexes.</p>"},{"location":"guides/retrieval-strategies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/retrieval-strategies/#slow-queries","title":"Slow Queries","text":"<pre><code>-- Check index usage\nEXPLAIN ANALYZE\nSELECT * FROM node_embeddings\nWHERE embedding &lt;=&gt; query_vector\nLIMIT 10;\n\n-- Rebuild indexes if needed\nREINDEX INDEX idx_embeddings_vector;\n</code></pre>"},{"location":"guides/retrieval-strategies/#poor-results","title":"Poor Results","text":"<ol> <li>Check embeddings: Ensure all chunks are embedded</li> <li>Tune RRF k: Experiment with different values</li> <li>Add filters: Narrow search scope</li> <li>Expand context: Include related code</li> </ol>"},{"location":"guides/retrieval-strategies/#high-memory-usage","title":"High Memory Usage","text":"<pre><code># Reduce batch sizes\nretriever = CodeRetriever(\n    connector,\n    batch_size=50  # Default: 100\n)\n</code></pre>"},{"location":"guides/retrieval-strategies/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference: Complete retrieval API</li> <li>Indexing Pipeline: Understanding the indexed data</li> <li>Production Guide: Scaling retrieval</li> </ul>"},{"location":"reference/embedding/","title":"Embedding API","text":"<p>The embedding module manages the conversion of code chunks into high-dimensional vector representations. It is designed for massive throughput and cost efficiency.</p> <p>::: src.code_graph_indexer.embedding.embedder</p>"},{"location":"reference/embedding/#codeembedder","title":"CodeEmbedder","text":"<pre><code>class CodeEmbedder\n</code></pre> <p>The asynchronous engine responsible for vectorization.</p>"},{"location":"reference/embedding/#run_indexing","title":"<code>run_indexing</code>","text":"<pre><code>async def run_indexing(self, snapshot_id: str, ...) -&gt; AsyncGenerator\n</code></pre> <p>Executes the embedding pipeline.</p>"},{"location":"reference/embedding/#the-staging-pipeline","title":"The Staging Pipeline","text":"<p>The embedder implements a customized ETL (Extract, Transform, Load) process to minimize calls to the expensive Embedding API (OpenAI/Vertex).</p>"},{"location":"reference/embedding/#1-transform-hash-cpu-parallel","title":"1. Transform &amp; Hash (CPU Parallel)","text":"<p>Before touching the database, the system uses a <code>ProcessPoolExecutor</code> to prepare data in parallel. For each chunk, it computes a Semantic Fingerprint (<code>v_hash</code>). *   Formula: <code>SHA256(Filepath + Struct + Content + OutgoingDefs + IncomingDefs)</code> *   This hash represents the exact semantic state of the code.</p>"},{"location":"reference/embedding/#2-staging-load-bulk-io","title":"2. Staging Load (Bulk I/O)","text":"<p>The prepared data is loaded using <code>COPY</code> into a temporary table: <pre><code>CREATE UNLOGGED TABLE temp_embedding_staging ...\n</code></pre> Unlogged tables are faster as they bypass the WAL (Write Ahead Log).</p>"},{"location":"reference/embedding/#3-deduplication-sql-set-logic","title":"3. Deduplication (SQL Set Logic)","text":"<p>This is the core cost-saving mechanism. <pre><code>-- \"Backfill\": Find existing vectors for identical code\nUPDATE temp_embedding_staging t\nSET embedding = e.embedding, \n    is_cached = true\nFROM node_embeddings e\nWHERE e.vector_hash = t.vector_hash;\n</code></pre> If you move a file, or rename a folder, the <code>vector_hash</code> might change (due to path), but if the content is stable, we could potentially relax the hash strictness. Currently, it is strict.</p>"},{"location":"reference/embedding/#4-delta-processing-async-workers","title":"4. Delta Processing (Async Workers)","text":"<p>Only rows where <code>embedding IS NULL</code> are fetched. *   Producer: Pushes batches to an <code>asyncio.Queue</code>. *   Consumers: A pool of workers (size = <code>max_concurrency</code>) pull batches, call the API, and write results.</p>"},{"location":"reference/embedding/#_compute_prompt_and_hash","title":"<code>_compute_prompt_and_hash</code>","text":"<pre><code>def _compute_prompt_and_hash(node: Dict) -&gt; Tuple[str, str]\n</code></pre> <p>Constructs the \"Context Window\" for the LLM. It's not just the code!</p> <p>Prompt Structure: 1.  Header: File Path, Language, Category. 2.  Semantic Tags: Roles (e.g., <code>Role: API Endpoint</code>), Modifiers (e.g., <code>Tags: async, static</code>). 3.  Graph Context: Incoming Definitions (e.g., <code>Defines: UserFactory, AuthMiddleware</code>). 4.  Body: The actual source code.</p> <p>This rich context ensures that even small chunks (like a 5-line function) have enough semantic meaning for high-quality retrieval.</p>"},{"location":"reference/indexer/","title":"Indexer API Reference","text":"<p>The <code>indexer</code> module is the command center of the library. It acts as the coordinator between the Git filesystem, the Parsing workers, the Embedding provider, and the Storage layer.</p> <p>::: src.code_graph_indexer.indexer</p>"},{"location":"reference/indexer/#codebaseindexer","title":"CodebaseIndexer","text":"<pre><code>class CodebaseIndexer(repo_url: str, branch: str, db_url: Optional[str] = None, worker_telemetry_init: Optional[Callable] = None)\n</code></pre> <p>The persistent controller for a specific repository.</p>"},{"location":"reference/indexer/#constructor-arguments","title":"Constructor Arguments","text":"<ul> <li> <p><code>repo_url</code> (str):     The complete Git remote URL. Supports SSH (<code>git@...</code>) and HTTPS (<code>https://...</code>). This acts as part of the unique key for the repository.</p> </li> <li> <p><code>branch</code> (str):     The target branch to index (e.g., <code>main</code>, <code>master</code>, <code>develop</code>).</p> </li> <li> <p><code>db_url</code> (Optional[str]):     A standard PostgreSQL connection string (<code>postgresql://server:port/db</code>). If not provided, it looks for <code>DATABASE_URL</code> in environment variables.</p> </li> <li> <p><code>worker_telemetry_init</code> (Optional[Callable]):     A hook function called at the start of every worker process. Useful for initializing tools like Sentry, OpenTelemetry, or custom logging config in parallel processes.</p> </li> </ul>"},{"location":"reference/indexer/#methods","title":"Methods","text":""},{"location":"reference/indexer/#index","title":"<code>index</code>","text":"<pre><code>def index(self, force: bool = False, auto_prune: bool = False) -&gt; str\n</code></pre> <p>Description: Triggers the \"Structure Analysis\" phase. This involves cloning, parsing, and graph construction. This is a synchronous, blocking operation (though it uses internal parallelism).</p> <p>Arguments: *   <code>force</code> (bool): If <code>True</code>, ignores the \"Stale Check\". Even if the latest commit is already indexed, it will create a new Snapshot and re-parse everything. Useful for development or fixing corrupted indices. *   <code>auto_prune</code> (bool): If <code>True</code>, automatically sets old snapshots to <code>archived</code> or deletes them after successful indexing.</p> <p>Returns: *   <code>snapshot_id</code> (str): The UUID of the newly created (or existing reused) snapshot.</p> <p>Raises: *   <code>GitCommandError</code>: If cloning or fetching fails (e.g. auth error, network down). *   <code>DatabaseError</code>: If connection fails.</p>"},{"location":"reference/indexer/#embed","title":"<code>embed</code>","text":"<pre><code>async def embed(self, provider: EmbeddingProvider, batch_size: int = 1000, mock_api: bool = False, force_snapshot_id: str = None) -&gt; AsyncGenerator[Dict[str, Any], None]\n</code></pre> <p>Description: Triggers the \"Semantic Analysis\" phase. This is an Asynchronous Generator. You must iterate over it to drive the process forward. It is decoupled from <code>index()</code> to allow for different scheduling (e.g., Index now, Embed tonight).</p> <p>Arguments: *   <code>provider</code> (EmbeddingProvider): An instance of a provider wrapper (e.g. <code>OpenAIEmbeddingProvider</code>). *   <code>batch_size</code> (int): Number of vectors to flush to DB in one transaction. Default 1000. *   <code>mock_api</code> (bool): If <code>True</code>, generates random vectors. STRICTLY FOR TESTING. *   <code>force_snapshot_id</code> (str): target a specific historical snapshot instead of the current one.</p> <p>Yields: A stream of status dictionaries. *   <code>{'status': 'staging_progress', 'total': 100, 'staged': 50}</code> *   <code>{'status': 'embedding_progress', 'total_to_embed': 500, 'total_embedded': 100}</code> *   <code>{'status': 'completed', 'newly_embedded': 450, 'recovered_from_history': 50}</code></p>"},{"location":"reference/indexer/#_init_worker_process-internal","title":"<code>_init_worker_process</code> (Internal)","text":"<pre><code>def _init_worker_process(worktree_path: str, db_url: str)\n</code></pre> <p>Global State Initialization: This static method is the entry point for every <code>multiprocessing.Process</code> spawned by the indexer. 1.  disables Signals to avoid <code>KeyboardInterrupt</code> corruption. 2.  Initializes a thread-local <code>TreeSitterRepoParser</code>. 3.  Opens a new DB connection (as connections strictly cannot be shared across forks).</p>"},{"location":"reference/indexer/#utility-pruning","title":"Utility: Pruning","text":""},{"location":"reference/indexer/#prune_snapshots","title":"<code>prune_snapshots</code>","text":"<pre><code>def prune_snapshots(self, keep: int = 3)\n</code></pre> <p>Maintenance method to keep the database size in check. *   Keeps the last <code>keep</code> (default 3) snapshots for this repository. *   Hard deletes older snapshots and cascades deletes to <code>nodes</code>, <code>edges</code>, and <code>embeddings</code>. *   Warning: This is destructive and irreversible.</p>"},{"location":"reference/models/","title":"Data Models","text":"<p>The <code>models</code> module defines the data transfer objects (DTOs) used throughout the system. These reflect the database schema.</p> <p>::: src.code_graph_indexer.models</p>"},{"location":"reference/models/#core-entities","title":"Core Entities","text":""},{"location":"reference/models/#repository","title":"<code>Repository</code>","text":"<pre><code>@dataclass\nclass Repository\n</code></pre> <p>The persistent identity of a project. *   current_snapshot_id: Pointer to the \"LIVE\" version. *   reindex_requested_at: Flag used for distributed locking/coordination.</p>"},{"location":"reference/models/#snapshot","title":"<code>Snapshot</code>","text":"<pre><code>@dataclass\nclass Snapshot\n</code></pre> <p>An immutable point-in-time capture of the repository. *   status: <code>pending</code> -&gt; <code>indexing</code> -&gt; <code>completed</code>. *   file_manifest: A JSON tree structure caching the file list for O(1) directory navigation.</p>"},{"location":"reference/models/#chunknode","title":"<code>ChunkNode</code>","text":"<pre><code>@dataclass\nclass ChunkNode\n</code></pre> <p>The atom of the Knowledge Graph. *   chunk_hash: SHA-256 of the content (used for deduplication). *   byte_range: <code>[start_byte, end_byte]</code> for precise slicing. *   metadata: Flexible JSON store for semantic tags (<code>role</code>, <code>category</code>).</p>"},{"location":"reference/models/#coderelation","title":"<code>CodeRelation</code>","text":"<pre><code>@dataclass\nclass CodeRelation\n</code></pre> <p>A directed edge between nodes (or files). *   relation_type:     *   <code>child_of</code>: Structural hierarchy.     *   <code>calls</code> / <code>references</code>: Usage.     *   <code>inherits</code>: OOP Inheritance.     *   <code>imports</code>: Module Dependency.</p>"},{"location":"reference/models/#retrieval-objects","title":"Retrieval Objects","text":""},{"location":"reference/models/#retrievedcontext","title":"<code>RetrievedContext</code>","text":"<pre><code>@dataclass\nclass RetrievedContext\n</code></pre> <p>The rich object returned to the client/agent after a search. *   nav_hints: Pre-calculated IDs for \"Next Chunk\", \"Previous Chunk\", and \"Parent Container\" to enable UI navigation without extra queries. *   outgoing_definitions: List of symbols called by this chunk (e.g. <code>User.save()</code>) to provide immediate context to an LLM.</p>"},{"location":"reference/models/#render","title":"<code>render</code>","text":"<pre><code>def render(self) -&gt; str\n</code></pre> <p>Formats the context into a Markdown-friendly string optimized for LLM consumption. Includes specific headers <code>[CONTEXT]</code>, <code>[CODE]</code>, <code>[RELATIONS]</code> to help the model parse the input.</p>"},{"location":"reference/navigation/","title":"Navigation API Reference","text":"<p>The <code>navigator</code> module enables Structural Exploration of the code. While <code>Retriever</code> finds where things are, <code>Navigator</code> explains how they relate.</p>"},{"location":"reference/navigation/#codenavigator","title":"CodeNavigator","text":"<pre><code>class CodeNavigator(storage: GraphStorage)\n</code></pre> <p>Offers an IDE-like traversal API (\"Go to Definition\", \"Find Usages\").</p>"},{"location":"reference/navigation/#methods","title":"Methods","text":""},{"location":"reference/navigation/#read_neighbor_chunk","title":"<code>read_neighbor_chunk</code>","text":"<pre><code>def read_neighbor_chunk(self, node_id: str, direction: str = \"next\") -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Description: Simulates \"Scrolling\". Gets the code block immediately preceding or following the current one in the file.</p> <ul> <li>Use Case: An Agent is reading a function <code>foo()</code> and sees a call to <code>_helper()</code>. Use <code>next</code> to see if <code>_helper()</code> is defined right below.</li> </ul> <p>Arguments:</p> <ul> <li><code>direction</code> (str): <code>\"next\"</code> (down) or <code>\"prev\"</code> (up).</li> </ul>"},{"location":"reference/navigation/#read_parent_chunk","title":"<code>read_parent_chunk</code>","text":"<pre><code>def read_parent_chunk(self, node_id: str) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Description: Jumps up the scope hierarchy. *   <code>Method</code> -&gt; <code>Class</code> *   <code>Class</code> -&gt; <code>Module</code> (File)</p>"},{"location":"reference/navigation/#analyze_impact-incoming","title":"<code>analyze_impact</code> (Incoming)","text":"<pre><code>def analyze_impact(self, node_id: str, limit: int = 20) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: \"Who calls this?\" (Reverse Call Graph). Identifies all functions or classes that depend on <code>node_id</code>. Critical for refactoring agents to know what might break.</p> <p>Returns: <pre><code>[\n    {\n        \"file\": \"src/controllers.py\",\n        \"line\": 45,\n        \"relation\": \"calls\",\n        \"context_snippet\": \"user.login()\"\n    }\n]\n</code></pre></p>"},{"location":"reference/navigation/#analyze_dependencies-outgoing","title":"<code>analyze_dependencies</code> (Outgoing)","text":"<pre><code>def analyze_dependencies(self, node_id: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: \"What does this call?\" (Forward Call Graph). Lists all external symbols used by <code>node_id</code>.</p>"},{"location":"reference/navigation/#visualize_pipeline","title":"<code>visualize_pipeline</code>","text":"<pre><code>def visualize_pipeline(self, node_id: str, max_depth: int = 2) -&gt; Dict[str, Any]\n</code></pre> <p>Description: Constructs a recursive tree of the call graph starting from <code>node_id</code>. *   Use Case: Generating a UI visualization (Node-Link diagram) for the user to understand complex logic flows.</p>"},{"location":"reference/parsing/","title":"Parsing API","text":"<p>The parsing layer is responsible for transforming raw code into structured graph data. It uses two complementary engines: Tree-sitter for robust syntactic parsing and SCIP for precise semantic analysis.</p> <p>::: src.code_graph_indexer.parsing.parser</p>"},{"location":"reference/parsing/#treesitterrepoparser","title":"TreeSitterRepoParser","text":"<pre><code>class TreeSitterRepoParser\n</code></pre> <p>A high-performance parser that breaks down source files into \"Chunks\" (Functions, Classes).</p>"},{"location":"reference/parsing/#stream_semantic_chunks","title":"<code>stream_semantic_chunks</code>","text":"<pre><code>def stream_semantic_chunks(self, file_list: List[str] = None) -&gt; Generator\n</code></pre> <p>The core pipeline entry point. It yields a stream of graph elements to keep memory usage constant (O(1)).</p>"},{"location":"reference/parsing/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/parsing/#zero-copy-optimization","title":"Zero-Copy Optimization","text":"<p>The parser avoids string copying whenever possible to handle large files efficiently. *   MemoryViews: It uses Python's <code>memoryview</code> on the raw bytes of the file. *   ByteArrays: It uses mutable <code>bytearray</code> buffers to accumulate \"glue\" code (comments, whitespace) without creating intermediate immutable string objects.</p>"},{"location":"reference/parsing/#recursive-chunking-algorithm","title":"Recursive Chunking Algorithm","text":"<p>The <code>_process_scope</code> method implements a greedy, recursive strategy to balance context vs. token limits.</p> <ol> <li>Accumulation: It iterates over AST siblings, accumulating \"Glue\" (comments, imports).</li> <li>Barrier Detection: When it hits a container (e.g., <code>class</code>, <code>def</code>), it flushes the glue.</li> <li>Size Check:<ul> <li>If <code>Node + Glue &lt; MAX_CHUNK_SIZE</code> (800 lines/tokens): It creates a single chunk.</li> <li>If <code>Node &gt; MAX_CHUNK_SIZE</code>: It enters Breakdown Mode (<code>_handle_large_node</code>).<ul> <li>It extracts the Header (Signature + Decorators) as a standalone chunk.</li> <li>It recurses into the Body (Block), passing the Header ID as the <code>parent_chunk_id</code>.</li> <li>This ensures that a 2000-line class results in multiple chunks, but every method inside still links back to the class header.</li> </ul> </li> </ul> </li> </ol>"},{"location":"reference/parsing/#noise-filtering","title":"Noise Filtering","text":"<p>The parser applies multiple layers of filtering to ensure graph quality: 1.  Technical Noise (<code>GLOBAL_IGNORE_DIRS</code>): Fast O(1) checks for <code>.git</code>, <code>node_modules</code> in the path. 2.  Semantic Noise (<code>SEMANTIC_NOISE_DIRS</code>): Heuristics to identify <code>test/</code>, <code>fixtures/</code>. 3.  Generated Code Detection: Scans the first 500 bytes for headers like \"Auto-generated by\". 4.  Minification Detection: Rejects files with line lengths &gt; 1000 chars.</p> <p>::: src.code_graph_indexer.graph.indexers.scip</p>"},{"location":"reference/parsing/#scipindexer","title":"SCIPIndexer","text":"<p>A wrapper around the Source Code Indexing Protocol (SCIP) CLI tools (e.g., <code>scip-python</code>, <code>scip-typescript</code>). It extracts \"deep\" semantic relationships like <code>calls</code>, <code>inherits</code>, and <code>imports</code>.</p>"},{"location":"reference/parsing/#stream_relations","title":"<code>stream_relations</code>","text":"<pre><code>def stream_relations(self, ...) -&gt; Generator[CodeRelation]\n</code></pre> <p>Orchestrates the external indexing process.</p>"},{"location":"reference/parsing/#internal-works-the-two-pass-extraction","title":"Internal Works: The Two-Pass Extraction","text":"<ol> <li> <p>Pass 1: Definition Table Building</p> <ul> <li>Iterates over the SCIP index.</li> <li>Extracts all <code>Definition</code> roles.</li> <li>Builds a temporary SQLite DB (<code>DiskSymbolTable</code>) mapping <code>symbol_name -&gt; {file, line, col}</code>.</li> <li>Why SQLite? A large Monorepo can have millions of symbols. Keeping a Python Dict causes OOM.</li> </ul> </li> <li> <p>Pass 2: Occurrence Resolution</p> <ul> <li>Iterates again over the index.</li> <li>Finds <code>Reference</code> roles (calls).</li> <li>Queries SQLite to find the target definition.</li> <li>Yields a resolved <code>CodeRelation</code> (Edge).</li> </ul> </li> </ol>"},{"location":"reference/reading/","title":"Reading API Reference","text":"<p>The <code>reader</code> module provides a Virtual Filesystem interface over the indexed snapshots. It allows agents to \"mount\" and browse a repository as if it were on a local disk, but with Time-Travel capabilities.</p>"},{"location":"reference/reading/#codereader","title":"CodeReader","text":"<pre><code>class CodeReader(storage: GraphStorage)\n</code></pre> <p>Key Features:</p> <ul> <li>Virtual FS: Files are not stored as plain text on disk but as chunked nodes in Postgres. The Reader reconstructs them on-the-fly.</li> <li>O(1) Listings: Directory contents are served from a pre-computed JSON manifest, making <code>ls</code> operations instant even for huge monorepos.</li> <li>Time Travel: Every read operation is scoped to a <code>snapshot_id</code>, allowing you to read the filesystem exactly as it appeared in any past commit.</li> </ul>"},{"location":"reference/reading/#methods","title":"Methods","text":""},{"location":"reference/reading/#read_file","title":"<code>read_file</code>","text":"<pre><code>def read_file(self, snapshot_id: str, file_path: str, start_line: int = None, end_line: int = None) -&gt; Dict[str, Any]\n</code></pre> <p>Description: Reads file content. Supports partial reads (byte-range fetches) which is efficient for large files where you only need a specific function.</p> <p>Arguments:</p> <ul> <li><code>snapshot_id</code> (str): The version to read from.</li> <li><code>file_path</code> (str): Relative path (e.g. <code>src/utils.py</code>).</li> <li><code>start_line</code> (int, optional): 1-indexed start line.</li> <li><code>end_line</code> (int, optional): 1-indexed end line.</li> </ul> <p>Returns: <pre><code>{\n    \"file_path\": \"src/utils.py\",\n    \"content\": \"def foo():\\n    pass\",\n    \"start_line\": 10,\n    \"end_line\": 11\n}\n</code></pre></p>"},{"location":"reference/reading/#list_directory","title":"<code>list_directory</code>","text":"<pre><code>def list_directory(self, snapshot_id: str, path: str = \"\") -&gt; List[Dict[str, Any]]\n</code></pre> <p>Description: Lists the children of a directory.</p> <p>Returns: A list of entries sorted with directories first. <pre><code>[\n    {\"name\": \"components\", \"type\": \"dir\", \"path\": \"src/components\"},\n    {\"name\": \"App.js\", \"type\": \"file\", \"path\": \"src/App.js\"}\n]\n</code></pre></p>"},{"location":"reference/reading/#find_directories","title":"<code>find_directories</code>","text":"<pre><code>def find_directories(self, snapshot_id: str, name_pattern: str, limit: int = 10) -&gt; List[str]\n</code></pre> <p>Description: Performs a \"Fuzzy Find\" for directory names. Useful when the agent knows a folder exists (e.g., \"auth\") but not where (e.g., <code>src/domain/auth</code> vs <code>libs/auth</code>).</p> <p>Implementation Note: This executes In-Memory against the cached JSON manifest. It does NOT impact the database.</p>"},{"location":"reference/retrieval/","title":"Retrieval API Reference","text":"<p>The <code>retriever</code> module acts as the \"Brain\" interface. It translates user intent (queries) into actionable code context.</p> <p>::: src.code_graph_indexer.retriever</p>"},{"location":"reference/retrieval/#coderetriever","title":"CodeRetriever","text":"<pre><code>class CodeRetriever(storage: GraphStorage, embedding_provider: EmbeddingProvider)\n</code></pre> <p>The main facade class. It requires access to both the database (for keyword search/graph walk) and the embedding API (for converting queries to vectors).</p>"},{"location":"reference/retrieval/#methods","title":"Methods","text":""},{"location":"reference/retrieval/#retrieve","title":"<code>retrieve</code>","text":"<pre><code>def retrieve(self, query: str, repo_id: str, snapshot_id: Optional[str] = None, limit: int = 10, strategy: str = \"hybrid\", filters: Dict[str, Any] = None) -&gt; List[RetrievedContext]\n</code></pre> <p>Description: Performs the end-to-end search pipeline: <code>Vectorize Query -&gt; Search DB -&gt; Fusion -&gt; Graph Expansion</code>.</p> <p>Arguments:</p> <ul> <li> <p><code>query</code> (str):     The search string. Can be natural language (\"How to auth?\") or code snippets (\"def login()\").</p> </li> <li> <p><code>repo_id</code> (str):     The UUID of the repository to search.</p> </li> <li> <p><code>snapshot_id</code> (Optional[str]):     If <code>None</code> (default), automatically resolves to the Latest Completed Snapshot for the repo. Providing an ID allows \"Time-Travel\" search on older versions.</p> </li> <li> <p><code>limit</code> (int):     The maximum number of results to return. Default 10. Note: Internally, it fetches <code>limit * 2</code> capabilities to allow for RRF re-ranking.</p> </li> <li> <p><code>strategy</code> (str):</p> <ul> <li><code>\"hybrid\"</code>: (Recommended) Runs both Vector and FTS, merges with RRF.</li> <li><code>\"vector\"</code>: Semantic similarity only. Good for broad concepts.</li> <li><code>\"keyword\"</code>: Text match only. Good for exact identifiers.</li> </ul> </li> <li> <p><code>filters</code> (Dict):     Metadata filters pushed down to SQL.</p> <ul> <li><code>\"path_prefix\"</code>: <code>str</code> (e.g. <code>\"src/api\"</code>). Matches files starting with this string.</li> <li><code>\"language\"</code>: <code>List[str]</code> (e.g. <code>[\"python\", \"js\"]</code>). Matches file extensions.</li> <li><code>\"exclude_category\"</code>: <code>List[str]</code> (e.g. <code>[\"test\"]</code>). Hides test files.</li> </ul> </li> </ul> <p>Returns: *   <code>List[RetrievedContext]</code>: A list of rich context objects, sorted by relevance score.</p>"},{"location":"reference/retrieval/#data-objects","title":"Data Objects","text":""},{"location":"reference/retrieval/#retrievedcontext","title":"<code>RetrievedContext</code>","text":"<pre><code>@dataclass\nclass RetrievedContext\n</code></pre> <p>The output unit of the retrieval process.</p>"},{"location":"reference/retrieval/#attributes","title":"Attributes","text":"<ul> <li><code>file_path</code> (str): Source file path relative to repo root.</li> <li><code>content</code> (str): The actual code block.</li> <li><code>start_line</code>, <code>end_line</code> (int): Line numbers (1-indexed).</li> <li><code>score</code> (float): The computed relevance score (normalized 0-1).</li> <li><code>semantic_labels</code> (List[str]): Tags extracted during parsing (e.g. <code>[\"class_definition\", \"public_api\"]</code>).</li> </ul>"},{"location":"reference/retrieval/#enriched-attributes-graph-context","title":"Enriched Attributes (Graph Context)","text":"<p>Attributes populated by the <code>GraphWalker</code> step.</p> <ul> <li> <p><code>parent_context</code> (Dict): Information about the containing scope.</p> <ul> <li>Example: <code>{\"type\": \"class\", \"name\": \"PaymentProcessor\", \"line\": 45}</code>.</li> <li>Usage: Tells the LLM where this function lives.</li> </ul> </li> <li> <p><code>outgoing_definitions</code> (List[Dict]): Summaries of external symbols used by this code.</p> <ul> <li>Example: <code>[{\"name\": \"stripe.charge\", \"role\": \"call\"}, {\"name\": \"User\", \"role\": \"instantiation\"}]</code>.</li> <li>Usage: Tells the LLM what dependencies this function has, without needing to retrieve those files.</li> </ul> </li> </ul>"},{"location":"reference/retrieval/#methods_1","title":"Methods","text":"<ul> <li><code>render()</code> -&gt; <code>str</code>     Returns a Markdown-formatted representation optimized for LLM prompting.     <pre><code>### File: src/main.py (L10-20)\n[CONTEXT] Inside class App\n[CODE]\ndef run(): ...\n[RELATIONS] Calls: logger.info, db.connect\n</code></pre></li> </ul>"},{"location":"reference/storage/","title":"Storage API Reference","text":"<p>The <code>storage</code> module is the foundation of the state management for the entire indexing system. It provides a robust, transactional abstraction over the persistence layer.</p> <p>::: src.code_graph_indexer.storage.postgres</p>"},{"location":"reference/storage/#database-schema","title":"Database Schema","text":"<p>The system is designed to run on PostgreSQL 15+ with the <code>pgvector</code> extension (<code>v0.5+</code>).</p>"},{"location":"reference/storage/#key-concepts","title":"Key Concepts","text":"<ul> <li>Repository Isolation: All data is partitioned logically by <code>repository_id</code>. A single database cluster can host thousands of repositories.</li> <li>Snapshot Lifecycle: Data is versioned using \"Snapshots\".<ul> <li><code>pending</code>: Data is being written (Indexing phase).</li> <li><code>indexing</code>: Active processing (Embedding phase).</li> <li><code>completed</code>: Immutable, ready for search.</li> <li><code>failed</code>: Aborted run.</li> </ul> </li> </ul>"},{"location":"reference/storage/#table-structure","title":"Table Structure","text":""},{"location":"reference/storage/#repositories","title":"<code>repositories</code>","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the repository config. <code>url</code> TEXT Git remote URL (e.g. <code>git@github.com...</code>). <code>branch</code> TEXT Branch name (e.g. <code>main</code>). <code>current_snapshot_id</code> UUID (FK) Pointer to the currently serving snapshot. Critical for Readers."},{"location":"reference/storage/#snapshots","title":"<code>snapshots</code>","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the indexing run. <code>repository_id</code> UUID (FK) Parent repo. <code>commit_hash</code> CHAR(40) Git SHA being indexed. <code>status</code> ENUM State of the snapshot process. <code>file_manifest</code> JSONB Compressed file listing for O(1) existence checks."},{"location":"reference/storage/#nodes-the-graph-vertices","title":"<code>nodes</code> (The Graph Vertices)","text":"Column Type Description <code>id</code> UUID (PK) Unique ID of the code chunk. <code>snapshot_id</code> UUID (FK) Partition key. <code>file_path</code> TEXT Path relative to repo root (e.g. <code>src/main.py</code>). <code>chunk_hash</code> CHAR(64) SHA-256 of the content (for deduplication). <code>content</code> TEXT The actual source code segment. <code>metadata</code> JSONB Semantic tags (<code>role</code>, <code>category</code>, <code>language</code>). <code>ts_vector</code> TSVECTOR Full-Text Search index (english)."},{"location":"reference/storage/#edges-the-graph-relations","title":"<code>edges</code> (The Graph Relations)","text":"Column Type Description <code>source_id</code> UUID (FK) Origin Node. <code>target_id</code> UUID (FK) Destination Node. <code>relation_type</code> TEXT Discriminator: <code>calls</code>, <code>defines</code>, <code>imports</code>, <code>inherits</code>, <code>child_of</code>."},{"location":"reference/storage/#node_embeddings-vectors","title":"<code>node_embeddings</code> (Vectors)","text":"Column Type Description <code>chunk_id</code> UUID (FK) Link to <code>nodes</code>. <code>embedding</code> VECTOR(1536) The dense vector representation. <code>vector_hash</code> CHAR(64) Identity hash for vector reuse optimization."},{"location":"reference/storage/#postgresgraphstorage","title":"PostgresGraphStorage","text":"<pre><code>class PostgresGraphStorage(GraphStorage)\n</code></pre> <p>The concrete implementation of the storage interface. It manages connections, transactions, and optimized bulk operations.</p>"},{"location":"reference/storage/#initialization","title":"Initialization","text":""},{"location":"reference/storage/#__init__","title":"<code>__init__</code>","text":"<p><pre><code>def __init__(self, connector: DatabaseConnector, vector_dim: int = 1536)\n</code></pre> *   connector: An instance of <code>PooledConnector</code> or <code>SingleConnector</code> (for workers). *   vector_dim: The dimension of the embeddings (default 1536 for OpenAI <code>text-embedding-3-small</code> / <code>ada-002</code>).</p>"},{"location":"reference/storage/#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"reference/storage/#ensure_repository","title":"<code>ensure_repository</code>","text":"<p><pre><code>def ensure_repository(self, url: str, branch: str, name: str) -&gt; str\n</code></pre> Idempotent. Ensures a record exists for the given (URL, Branch) pair. *   Returns: The <code>repository_id</code> (UUID). *   Concurrency: Uses <code>ON CONFLICT DO UPDATE</code> to handle race conditions safely.</p>"},{"location":"reference/storage/#create_snapshot","title":"<code>create_snapshot</code>","text":"<p><pre><code>def create_snapshot(self, repository_id: str, commit_hash: str, force_new: bool = False) -&gt; Tuple[Optional[str], bool]\n</code></pre> Creates a new <code>pending</code> snapshot. *   force_new: If <code>True</code>, creates a new snapshot even if one exists for this commit hash. *   Returns: <code>(snapshot_id, created)</code>. If <code>created</code> is False, it means a reusable snapshot was found.</p>"},{"location":"reference/storage/#activate_snapshot","title":"<code>activate_snapshot</code>","text":"<p><pre><code>def activate_snapshot(self, repository_id: str, snapshot_id: str, stats: Dict = None, manifest: Dict = None)\n</code></pre> Critical Operation. Atomically promotes a snapshot to \"Current\". 1.  Updates <code>snapshots</code> table: Status -&gt; <code>completed</code>, sets <code>completed_at</code>. 2.  Updates <code>repositories</code> table: <code>current_snapshot_id</code> -&gt; <code>snapshot_id</code>. 3.  Because this happens in a single transaction, readers (Search) switch instantly to the new version with zero downtime.</p>"},{"location":"reference/storage/#write-operations-optimized","title":"Write Operations (Optimized)","text":""},{"location":"reference/storage/#add_nodes_raw","title":"<code>add_nodes_raw</code>","text":"<p><pre><code>def add_nodes_raw(self, nodes_tuples: List[Tuple])\n</code></pre> Bypasses ORM for speed. *   Input: List of tuples matching the <code>nodes</code> table layout (excluding generated cols). *   Mechanism: Uses <code>cursor.copy_expert()</code> with <code>COPY ... FROM STDIN (BINARY)</code>. *   Performance: Capable of inserting 50k-100k rows/second. *   Raises: <code>psycopg2.errors.UniqueViolation</code> if UUIDs collide (rare).</p>"},{"location":"reference/storage/#ingest_scip_relations","title":"<code>ingest_scip_relations</code>","text":"<p><pre><code>def ingest_scip_relations(self, relations_tuples: List[Tuple], snapshot_id: str)\n</code></pre> Performs a \"Spatial Join\" resolution. SCIP gives us <code>(path, start_line, end_line) -&gt; Target</code>. We need to map this to <code>source_node_id -&gt; target_node_id</code>. 1.  Loads raw data into <code>temp_scip_staging</code>. 2.  Executes a complex SQL <code>JOIN</code> between <code>temp_scip_staging</code> and <code>nodes</code> based on file path and byte ranges. 3.  Inserts resolved edges into <code>edges</code>.</p>"},{"location":"reference/storage/#read-operations-search","title":"Read Operations (Search)","text":""},{"location":"reference/storage/#search_vectors","title":"<code>search_vectors</code>","text":"<p><pre><code>def search_vectors(self, query_vector: List[float], limit: int, snapshot_id: str, filters: Dict[str, Any] = None) -&gt; List[Dict]\n</code></pre> Performs Approximate Nearest Neighbor (ANN) search. *   query_vector: The float list from the embedding provider. *   snapshot_id: Mandatory. Scopes search to the active version. *   filters: Optional metadata filters. Supported keys:     *   <code>language</code>: Exact match on file extension.     *   <code>role</code>: Semantic role (e.g. <code>class</code>, <code>function</code>).     *   <code>path_prefix</code>: Starts-with match on file path. *   SQL Strategy:     <pre><code>ORDER BY embedding &lt;=&gt; :query_vector LIMIT :limit\n</code></pre>     This triggers the <code>HNSW</code> index scan.</p>"},{"location":"reference/storage/#search_fts","title":"<code>search_fts</code>","text":"<p><pre><code>def search_fts(self, query: str, limit: int, snapshot_id: str, filters: Dict = None) -&gt; List[Dict]\n</code></pre> Performs Full-Text Search. *   Uses PostgreSQL's <code>websearch_to_tsquery</code> which supports Google-like syntax:     *   <code>\"exact phrase\"</code>     *   <code>login -test</code> (exclude word)     *   <code>auth or security</code> *   Ranking: Uses <code>ts_rank</code> for relevancy.</p>"},{"location":"reference/storage/#get_file_manifest","title":"<code>get_file_manifest</code>","text":"<p><pre><code>def get_file_manifest(self, snapshot_id: str) -&gt; Dict[str, Any]\n</code></pre> Retrieves the cached directory tree structure. *   Use Case: Used by the UI/Frontend to render the file explorer tree without querying the <code>files</code> table recursively. *   Return Format: A nested dictionary representing the folder structure.</p>"},{"location":"testing/unit-tests-guide/","title":"Unit Test Documentation","text":"<p>This guide explains the structure and purpose of the unit tests in the <code>sheep-codebase-indexer</code> project.</p>"},{"location":"testing/unit-tests-guide/#current-coverage","title":"\ud83d\udcca Current Coverage","text":"<p>Total Coverage: 73% (target: &gt;90%)</p> <p>All 100 tests passing \u2705</p>"},{"location":"testing/unit-tests-guide/#test-structure","title":"\ud83d\udcc1 Test Structure","text":""},{"location":"testing/unit-tests-guide/#testsunittest_scip_indexer_unitpy","title":"<code>tests/unit/test_scip_indexer_unit.py</code>","text":"<p>What it tests: SCIP (SCIP Code Intelligence Protocol) index processing</p> <p>Coverage areas: - <code>test_clean_symbol_logic</code>: SCIP symbol name cleaning (removing special characters) - <code>test_bytes_conversion</code>: Line/column coordinate \u2192 byte offset conversion - <code>test_extract_symbol_name</code>: Symbol name extraction from source code - <code>test_process_definitions</code>: Symbol definition processing - <code>test_scip_runner_prepare_indices</code>: Index preparation via external SCIP tool - <code>test_stream_documents</code>: JSON document streaming from SCIP indices</p> <p>Testing techniques: - Mock <code>subprocess</code> to simulate SCIP tool execution - Mock filesystem operations (<code>os.path.exists</code>, <code>os.path.getsize</code>) - Patch internal methods to isolate logic</p>"},{"location":"testing/unit-tests-guide/#testsunittest_parser_unitpy","title":"<code>tests/unit/test_parser_unit.py</code>","text":"<p>What it tests: Source code parsing with TreeSitter</p> <p>Coverage areas: - <code>test_chunking_with_overlap</code>: Code splitting into chunks with overlap - <code>test_recursive_chunking_small_file</code>: Small file chunking - <code>test_handle_large_node_breakdown</code>: Handling AST nodes exceeding size limits - <code>test_extract_tags</code>: Semantic tag extraction (functions, classes) - <code>test_should_process_file</code>: File filtering logic</p> <p>Key concepts: - Chunking: Splitting code into manageable pieces for embedding - Overlap: Maintaining context between adjacent chunks - Semantic captures: Metadata extracted from AST (function name, type, etc.) - AST (Abstract Syntax Tree): Structured representation of code</p> <p>Testing techniques: - Mock TreeSitter nodes with <code>MagicMock</code> - Custom <code>DummyNode</code> class to avoid comparison errors - Patch <code>_extract_tags</code> to isolate chunking logic</p>"},{"location":"testing/unit-tests-guide/#testsunittest_storage_postgrespy","title":"<code>tests/unit/test_storage_postgres.py</code>","text":"<p>What it tests: Code graph persistence in PostgreSQL</p> <p>Coverage areas: - Repository &amp; Snapshot:   - <code>test_ensure_repository_new</code>: New repository registration   - <code>test_create_snapshot_new</code>: Snapshot creation   - <code>test_create_snapshot_existing</code>: Existing snapshot reuse</p> <ul> <li>Data Insertion:</li> <li><code>test_add_files</code>: File insertion</li> <li><code>test_add_nodes_fast</code>: Bulk insert with COPY protocol</li> <li> <p><code>test_add_contents_raw</code>: Content insertion</p> </li> <li> <p>Search:</p> </li> <li><code>test_search_vectors</code>: Semantic vector search</li> <li><code>test_search_fts</code>: Full-text search</li> <li> <p><code>test_get_incoming_definitions_bulk</code>: Batch definition retrieval</p> </li> <li> <p>Graph:</p> </li> <li><code>test_get_context_neighbors</code>: Neighbor node navigation</li> <li><code>test_ingest_scip_relations</code>: SCIP relation ingestion</li> </ul> <p>Key concepts: - Snapshot: Repository state at a specific point in time - COPY protocol: High-performance bulk insert in PostgreSQL - Context manager: <code>with</code> pattern for transaction management - Bulk operations: Batch operations to reduce DB round-trips</p> <p>Testing techniques: - Mock <code>psycopg</code> (PostgreSQL driver) - Mock <code>uuid.uuid4</code> for deterministic results - <code>side_effect</code> to simulate call sequences - SQL verification with <code>assertIn</code> for flexibility</p>"},{"location":"testing/unit-tests-guide/#testsunittest_indexer_lifecyclepy","title":"<code>tests/unit/test_indexer_lifecycle.py</code>","text":"<p>What it tests: Complete indexing process orchestration</p> <p>Coverage areas: - <code>test_initialization</code>: Component initialization - <code>test_index_workflow_success</code>: Complete indexing workflow - <code>test_index_skip_existing</code>: Skip already-indexed snapshots - <code>test_embed_pipeline</code>: Embedding pipeline</p> <p>Key concepts: - Workflow: Repository \u2192 Snapshot \u2192 Parse \u2192 Embed \u2192 Swap - Concurrency: Using <code>ProcessPoolExecutor</code> for parallelization - Namespace injection: Manual mock injection to avoid import side-effects</p>"},{"location":"testing/unit-tests-guide/#testsunittest_git_volumepy","title":"<code>tests/unit/test_git_volume.py</code>","text":"<p>What it tests: Git repository management</p> <p>Coverage areas: - <code>test_ensure_repo_updated</code>: Repository clone/fetch - <code>test_get_head_commit</code>: HEAD commit retrieval</p>"},{"location":"testing/unit-tests-guide/#mocking-techniques","title":"\ud83d\udee0\ufe0f Mocking Techniques","text":""},{"location":"testing/unit-tests-guide/#1-basic-mock","title":"1. Basic Mock","text":"<pre><code>mock_obj = MagicMock()\nmock_obj.method.return_value = \"value\"\n</code></pre>"},{"location":"testing/unit-tests-guide/#2-context-manager","title":"2. Context Manager","text":"<pre><code>mock_copy_manager = MagicMock()\nmock_copy_obj = MagicMock()\nmock_copy_manager.__enter__.return_value = mock_copy_obj\n</code></pre>"},{"location":"testing/unit-tests-guide/#3-side-effects-sequences","title":"3. Side Effects (Sequences)","text":"<pre><code>mock_cursor.fetchone.side_effect = [\n    {\"id\": \"1\"},  # First call\n    None,         # Second call\n]\n</code></pre>"},{"location":"testing/unit-tests-guide/#4-patch-decorator","title":"4. Patch Decorator","text":"<pre><code>@patch(\"module.function\")\ndef test_something(self, mock_function):\n    mock_function.return_value = \"mocked\"\n</code></pre>"},{"location":"testing/unit-tests-guide/#best-practices","title":"\ud83c\udfaf Best Practices","text":"<ol> <li>Isolate dependencies: Mock DB, filesystem, subprocess</li> <li>Deterministic tests: Mock UUID, timestamps, random</li> <li>Flexible assertions: <code>assertIn</code> instead of <code>assertEqual</code> for SQL</li> <li>Descriptive comments: Explain WHAT and WHY, not just HOW</li> <li>Descriptive names: <code>test_create_snapshot_new</code> &gt; <code>test_snapshot_1</code></li> </ol>"},{"location":"testing/unit-tests-guide/#debugging-tests","title":"\ud83d\udc1b Debugging Tests","text":""},{"location":"testing/unit-tests-guide/#test-fails-with-typeerror-not-supported","title":"Test fails with <code>TypeError: '&lt;' not supported</code>","text":"<p>Cause: Comparison between <code>MagicMock</code> objects Solution: Use <code>DummyNode</code> class with concrete attributes</p>"},{"location":"testing/unit-tests-guide/#test-fails-with-attributeerror-nonetype-object","title":"Test fails with <code>AttributeError: 'NoneType' object</code>","text":"<p>Cause: Mock not configured correctly Solution: Verify <code>return_value</code> or <code>side_effect</code></p>"},{"location":"testing/unit-tests-guide/#test-fails-with-assertionerror-expected-execute-to-be-called","title":"Test fails with <code>AssertionError: Expected 'execute' to be called</code>","text":"<p>Cause: Assertion on wrong mock (<code>mock_cursor</code> vs <code>mock_conn</code>) Solution: Check which object actually executes the query</p>"},{"location":"testing/unit-tests-guide/#next-steps-for-90-coverage","title":"\ud83d\udcc8 Next Steps for 90% Coverage","text":"<ol> <li>Expand <code>scip.py</code> tests: Cover relation processing methods</li> <li>Test <code>schema.py</code>: Currently 0% coverage</li> <li>Test <code>embedding.py</code>: Increase from 68% to &gt;90%</li> <li>Test <code>git_volume_manager.py</code>: Cover <code>files()</code> method and filtering</li> </ol>"},{"location":"testing/unit-tests-guide/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>pytest Documentation</li> <li>unittest.mock Guide</li> <li>TreeSitter</li> <li>SCIP Protocol</li> </ul>"}]}